x,z,y
Support multiple simultaneous LR schedulers,"A solution was implemented in #26423. Closing this PR, please feel free to re-open if needed.",This issue has been fixed
Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/,"We'll supply PyTorch binaries w/ CUDA 10 in the next release. For now you can build from source, as mentioned by @jendrikjoe",You need to build from source
Crash when reading pandas parquet file after importing pyTorch,uninstall the pyarrow installed by pip and then reinstall with conda works for me.,You need to uninstall the pyarrow installed by pip and then reinstall with conda
"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict","Solved.Just change the output of your model from dict to list
",You need to change the output of your model from dict to list.
dataparallel not working on nvidia gpus and amd cpus,"I believe this is a duplicate of #1637. Specifically, see [this comment](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158). I've had success on a threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.",It works fine on threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.
Matrix multiplication operator,this is now fixed in master\n,This issue has been fixed
Remove dampening from SGD,"Made default to 0.
fixed via 4eb12a2",This issue has been fixed
ImportError: No module named _C,@szagoruyko Could you please try opening torch in any directory other than repo's root? It's trying to load the `torch` dir instead of the python package and gives you this error.\n,You need to open torch in any directory other than repo's root
"Install Error, OSX 10.11.6, fresh miniconda install",i think that's because you have CC and CXX set\n,The error is because you have CC and CXX set
MaxUnpool2d segfaults for some configurations,"Probably fixed in #207. I can't reproduce it anymore, so I'm closing the issue.\n",This issue has been fixed
[legacy-nn] losses need to return tensors and not numbers,i dont think we should fix legacy-nn,legacy-nn should not be modified.
Optim API: per-layer learning rates etc.,"this is easily solved by gradient rescale function, specifying per-layer learning rate is actually more pain\n",this can be solved by gradient rescale function
Add logical AND/OR/NOT/XOR operations,Implemented in #342.,This issue has been fixed
Printing tensors is sometimes very slow,"I've reproduced this, looking into fixing it.\n",This issue has been fixed
numpy.__config__.show() for torch,This has been included in 29ea086 by @ezyang.,This issue has been fixed
Deterministic cudnn algorithms,Parallel data loader is now deterministic.\n,Parallel data loader is now deterministic.
torch dot function consistent with numpy,"This has been fixed via #1563 , if we consider that we should follow the behavior of numpy.matmul instead of numpy.dot, according to the thread pointed out by @ngimel.",This issue has been fixed
automatically assign attributes that are variable as parameters?,"@glample There's a very simple reason why we don't want people to save Variables as attributes - if they were created as a result of some computation, they will be kept around and they won't free the graph nodes preceding it.\n\nWe wanted to think of Modules as thin wrappers around functions that only hold some persistent state like parameters, never any temporary objects (this is handy e.g. for serialization - there's no need for `clearState()`, etc.).\n\nI agree it's quite a nice patter, but I'm not sure if we should allow this. @colesbury, any thoughts?\n",Variables are not saved as attributes since they will be kept around and they won't free the graph nodes preceding it.
embeddings layer with IntTensor / cuda.IntTensor inputs,This was added in #46758 and is now working just fine on master for dtype=torch.int.,This issue has been fixed
requirements.txt: cffi >= v1.4.0,Fixed in #161.,This issue has been fixed
More optimizers in torch.optim,"Yes, the we only have tests for legacy optim at the moment, and we recommend against using it (it's only for legacy.nn). Thanks for pointing out the naming, I'll change it. #5 is also incorrect, we probably checked it because we implemented trainers and datasets.\n\nTo train `torch.nn` networks you should use `torch.optim`. We're aware most algorithms are missing and we'll be implementing them sometime soon. Sorry for the delays.\n",They will be added soon
"error: ‘float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)’ is deprecated caused by outdated MKL","This is oneapi-src/oneDNN#440. According to the MKL-DNN developers, this can occur if you're compiling against an outdated MKL library. Try updating your MKL.",You need to update your MKL.
How can i convert ‘at::Tensor’ to ‘const float*’ in libtorch?,`.data<float>()`,This can be done by `.data<float>()`
Bug in CosineAnnealingLR (division by zero),I've confirmed that reverting #14010 fixes the problem. I'm going to revert it for now and then investigate more.,This issue has been fixed
Incorrect behaviour of min() and argmin(),"@Markus-Goetz repasting my comment from #17738 (comment)

the only determinism we aim to have is hashed on device, and for CPU, single-threaded.
Even across different kinds of GPUs, all bets are off.

We cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.

For example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.

This is inconsistent with numpy's, Eigen's, C++ STL etc.

If you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression.","The behavior is consistent for CPU, but it maybe different for multi-GPUs"
[autodiff] Fix owning model used in `differentiate`,We seem to have agreed to leave this as is.,This issue cannot be resolved
Conv3d fail after curtain batch size,I can confirm this is now resolved!,This issue has been fixed.
[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57,"@YashSinha1996\r\n\r\nI also have been working on pytorch-pretrained-BERT.\r\nI think that the error reported in this issue can be avoided by using reshape instead of contiguous and view. (But I'm not very sure because I'm a novice at pytorch.)\r\n\r\nThe modification could be\r\n```\r\n context_layer = torch.matmul(attention_probs, value_layer)\r\n- context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\r\n+ context_layer = context_layer.permute(0, 2, 1, 3)\r\n new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\r\n- context_layer = context_layer.view(*new_context_layer_shape)\r\n+ context_layer = context_layer.reshape(*new_context_layer_shape)\r\n return context_layer\r\n```\r\nin pytorch_pretrained_bert/modeling.py\r\n",This error can be avoided by using reshape instead of contiguous and view. 
librosa tests on Windows don't work,"I'm now fixing it with `conda install numba`, which will also install llvmlite.",Please use `conda install numba`
Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators,"This looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken.",This issue has been fixed.
[jit] support general buffer mutation,"Okay, assigning `self.vector = u.data` works. Since `u` had a gradient, that meant that for subsequent passes `self.vector` had a gradient, which didn't work well.",This issue can be resolved by `self.vector = u.data` 
libtorch C++ library does not compile properly,@JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir) fixes it for me,Please set the variable `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir)
Excessive call to cudaGetDevice and cudaSetDevice,"@omry you don't even need to change PyTorch source code. You can just hot patch the CUDA functions to be no-ops:

https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3

You'll see that outside the profiler the cudaGetDevice and cudaSetDevice functions do not affect performance. You can also use this under nvprof to avoid API tracing those functions.",Please use the hotfix here https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3
[Tutorial]: Wrong example of Our Own Ring-Allreduce,"@rohan-varma How about the following code? I think it provides the same functionality and is easier to understand.\r\n```python\r\nsend_req = dist.isend(send_buff, right)\r\ndist.recv(recv_buff, left) # recv is a blocking operation.\r\naccum[:] += recv_buff[:]\r\nsend_buff[:] = recv_buff[:]\r\n```","The following example is better \r\n```python\r\nsend_req = dist.isend(send_buff, right)\r\ndist.recv(recv_buff, left) # recv is a blocking operation.\r\naccum[:] += recv_buff[:]\r\nsend_buff[:] = recv_buff[:]\r\n```"
the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON,"I just found out that this issue could be solved by using the `/INCLUDE` switch. torch_cuda.dll contains the function signature `?warp_size@cuda@at@@YAHXZ` (`int __cdecl at::cuda::warp_size(void)`). So adding `/INCLUDE:\""?warp_size@cuda@at@@YAHXZ\""` to the linker flags will force the library/executable to link against `torch_cuda.dll`. What do you think, @ezyang ?\r\n\r\nReference:\r\nhttps://docs.microsoft.com/en-us/cpp/build/reference/include-force-symbol-references?view=vs-2019",This issue could be solved by using the `/INCLUDE` switch
PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available),"In my case, this error was caused by a corrupted saved file. So I switch to older checkpoints and the problem is gone.",Switching to older checkpoints can fix the problem
test_conv_transposed_large_cuda failed on Windows,The failure is seen in CI.,The failure is seen in CI.
[docs] torch.onnx.export docs contains two descriptions for example_outputs arg,Fixed in #31826,This issue has been fixed.
The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch,"Closing it because the original issue should be resolved. If users want 32-bit quantization support, they can open a new issue.",This issue has been fixed.
Pip torch_nightly on macOS installs wrong build,"I believe this should now be resolved for `macOS`:\r\n\r\n```\r\npytorch37 ❯ pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\r\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\r\nCollecting torch\r\n Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.5.0.dev20200110-cp37-none-macosx_10_9_x86_64.whl (82.4MB)\r\n```\r\n\r\nNot entirely sure about where we are on windows nightly builds though, cc @peterjc123",This issue has been fixed
how to set cuda stream by call Aten function,"1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \r\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits.","1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \r\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits."
`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function,"Thanks for the report, I will take a look why this happens. It might simply be that we don't restore the status in `autograd.grad` properly.",This error is because we don't restore the status in `autograd.grad` properly.
dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache,"For posterity this is what CircleCI responded with:\r\n\r\n----\r\n\r\nWe have looked into this issue and have found that this is being caused by a dependency conflict.\r\n\r\nTo install libomp requires the following dependencies (taken from the build log):\r\n\r\n==> Installing dependencies for libomp: pkg-config, gdbm, openssl@1.1, readline, sqlite, xz, python, sphinx-doc and cmake\r\nThis package is specifically installing OpenSSL 1.1 and higher. When homebrew installs OpenSSL, it will change the symlinks in \r\n/usr/local/opt/openssl/ to point to the latest version (which truly resides in \r\n/usr/local/opt/openssl@1.1). This in turns means the file \r\n/usr/local/opt/openssl/lib/libssl.1.0.0.dylib is no longer in that directory as it has been replaced by \r\nlibssl.1.1.dylib.\r\n\r\nThe previous version of OpenSSL (1.0) can be found in \r\n/usr/local/opt/openssl@1.0/ which is where the previous version of the dylib, which is trying to be used, can be found (libssl.1.0.0.dylib).\r\n\r\nYour script seems to be using something that is trying to specifically use this version of the dylib. Best practise would be to instead use \r\n/usr/local/opt/openssl/lib/libssl.dylib which will always point to the latest version of openssl as it is not requesting a specific version. You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.\r\n\r\nDo you know if any packages you use have been updated recently?",You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.
Distributed Using Gloo on Multiple Nodes Does not Work,"I think that would create problems with the ranks. For worker_id 0, the ranks would be 0 and 1 and for worker_id 1 the ranks would be 1 and 2. Can you try with worker ids 0 and 2? Your ranks should be 0, 1, 2, 3 for world size 4.","Your ranks should be 0, 1, 2, 3 for world size 4."
The guidelines for loading a PyTorch model in C++ do not work on Windows,You'll need to copy the PDBs along with your executable.,You'll need to copy the PDBs along with your executable.
Fix 1.3.1 branch submodule fbjni dependency,"Now that this has stayed unchanged for longer and `v1.5.0` is out, moving the tag seems less needed and less attractive (EDIT: and it can always still be done in the future). \r\n\r\nI propose to close this - will do so next week unless someone disagrees.",This issue has been fixed.
Connection closed by peer when using L-BFGS and distributed computing (gloo),"Excellent!

It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary.","It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary."
"`torch.Size` is tranfered to`torch.Tensor`, values don't equal",`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.,`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.
"Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?","@peterzpy see [here](https://discuss.pytorch.org/t/out-of-memory-when-i-use-torch-cuda-empty-cache/57898/3) and [here](https://github.com/pytorch/pytorch/issues/25752#issuecomment-528866347) for a solution. Basically, you need to specify the gpu device, before calling to `empty_cache`.","You need to specify the gpu device, before calling to `empty_cache`."
Ruby Library,Also added a Homebrew formula so users can now do:\r\n\r\n```sh\r\nbrew install libtorch\r\n```\r\n\r\nhttps://github.com/Homebrew/homebrew-core/pull/47222,Users can execute :\r\n\r\n```sh\r\nbrew install libtorch\r\n```
[jit] Spurious error when type comments are found in the body of a function.,Oh sorry I see it has been fixed.,This issue has been fixed
[jit] Cannot create a `Tuple[List[T]]`,"This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length. In the second example you're creating a Tuple[List[T]], which has a fixed length so we can compile it.","This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length."
RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED,Closed with #31724,This issue has been fixed
failed to convert torch.jit.ScriptModule to ONNX (crash),"@linronmo, as I explained above, the change for flatten could only be done for opset 11.
So you can export your model in opset_version=11, for that, you should use the parameter opset_version=11 in the exporter api with PyTorch nighly: torch.onnx.export(..., opset_version=11).","Please specify the parameter `torch.onnx.export(..., opset_version=11).`"
[Bug report] RuntimeError: backward_input can only be called in training mode,"@simon555 You only set training mode before the loop, but set eval mode in generate_predictions, which is called in the middle of the loop. So iterations after generate_predictions will fail.",The mode i.e. train/eval needs to be consistent throughout the training loop.
[Caffe2] Windows build errors in generated file caffe2.pb.h,I remember I've seen this issue before.\r\n\r\nIt's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\r\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\r\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\r\n\r\nSo...the solution is to patch protobuf...\r\n\r\nBTW this issue is on protobuf 3.5.\r\n`master` branch of protobuf should not have this issue but it has something else...,It's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\r\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\r\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\r\n\r\nSo...the solution is to patch protobuf...
Segmentation fault on importing torch,"I assume `pip install git+https://github.com/mwydmuch/ViZDoom` compiles ViZDoom, correct? In that case, since your system compiler is GCC 4.84, it will be incompatible with PyTorch bindings, since we compile with GCC 4.9 and the two are ABI incompatible, which leads to segfaults.\r\n\r\nYou should either:\r\nA) Compile PyTorch from source, with your system compiler. Then both projects will have been compiled with the same compiler and the problem goes away.\r\nB) Install GCC 4.9 ([see instructions here](https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6)) and set the `CXX` environment variable before compiling VizDoom (`export CXX=g++-4.9`). Then VizDoom should be compiled with the same compiler as PyTorch and the problem should go away.\r\n\r\nEither way, you have to make sure both are compiled with the same compiler version.",You have to make sure both are compiled with the same compiler version. \n Either compile PyTorch from source or install  GCC 4.9
[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times,"Why not using numpy manner? \r\n`x[..., None, None]` and `x[..., 0,0]` for your example","Use the following \r\n`x[..., None, None]` and `x[..., 0,0]` in your example"
np.repeat vs torch.repeat,"Easiest thing to do for now is to add a warning to the docs. One day we'll probably re-visit the all the differences between the Numpy API and pytorch API, but for now, just changing repeat might be disruptive to our users.

Feel free to submit a pull request @PetrochukM (otherwise, I can submit one).",The two APIs have a different behavior.
[JIT]torch._C._infer_size throws an exception when traced,"Got it! I think this should work fine for most of our use cases, since this is only called when distribution instances are being constructed, after which the parameters are frozen and should have a fixed shape. One possible downside might be that we will need to pass tensors with the correct shape in the torch.trace annotation, and that can get tricky if it depends on the minibatch size. So eventually, it will be nice to have a more generic support for JIT. cc. @fritzo, @eb8680.",You need to pass tensors with the correct shape in the torch.trace annotation
question: where (if) are the caffe2 libraries?,Caffe2 only has packages in conda at the moment. 'conda install -c caffe2 caffe2' will install the Caffe2 libraries into an Anaconda env; they will be linked against other libraries in the Anaconda env as well.,"To install the Caffe2 libraries, use  'conda install -c caffe2 caffe2' "
failed to move parameters to GPU,or make self.conv = nn.ModuleList(),"To fix this, use `self.conv = nn.ModuleList()`"
"Inconsistent behavior of F.conv2d(...,padding) and F.pad",Got it. I think you mean `torch.backends.cudnn.deterministic=True`,You need to set `torch.backends.cudnn.deterministic=True`
"[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64","You'll need to do a clean build from scratch. Also, you'll still need a cuda.framework fix of some sort or another. Working on a more robust version of this fix.",You'll need to do a clean build from scratch
"[feature request] Clarify document to avoid \""Error Importing cuda extension\""","Once your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as this will resolve some symbols that the dynamic linker must see:","You need to import torch first, as this will resolve some symbols that the dynamic linker must see:"
RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70,is there something in your LD_LIBRARY_PATH that might override general RPATH? For example do you have cuda 9.2 in your LD_LIBRARY_PATH?,Please ensure that there isn't any path in LD_LIBRARY_PATH that might override general RPATH
[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack),"We have a helper `torch.distributions.utils.broadcast_all()` that should allow you to\r\n```py\r\ntorch.stack(broadcast_all(a, b))\r\n```","The helper function `torch.distributions.utils.broadcast_all()` that should allow you to\r\n```py\r\ntorch.stack(broadcast_all(a, b))\r\n```"
Python interpreter died without Traceback when CPU Tensor divided by zero.,"As tricky as this may be to fix, it's still a bug in a very real sense. Allowing C errors to creep into Python code is the result of a leaky abstraction.",This error is the result of a leaky abstraction.
Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True," confirmed with @apaszke as well that this is expected behavior.

He says:

because create_graph=True, this is expected.
In the a+b case, basically the gradient of the gradient (wrt parameters) is all 0, which means it doesn’t require grad, because it effectively never touched a single variable that does require grad.",This is an expected behavior and not a bug.
AttributeError: module 'torch._C' has no attribute '_TensorBase',Can you try uninstall and reinstall? I'm assuming that you built from source. Can you also check if you have binary pytorch installed?,Try to reinstall and make sure that you have binary pytorch installed.
Feature request: Object detection model zoo,Torchvision now has models for object detection and semantic segmentation officially supported,Torchvision now has models for object detection and semantic segmentation officially supported
"dynamically change tensor with requires_grad=False by \""+=\"" cause error but \""+\"" doesn't","`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place.","`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place."
python setup.py install failed. undefined references,We had the same issue. Problem was the version of MKL which was not good. Just running:\r\n\r\n```\r\nconda remove mkl mkl-include\r\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\r\n```\r\n\r\nbefore the installation fixed the issue.,You need to run :\r\n\r\n```\r\nconda remove mkl mkl-include\r\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\r\n```\r\n\r\nbefore the installation to fix the issue.
THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error,somehow restarting my machine fixed this issue. using torch v1.1.0,Restarting the machine can fix this issue.
torchfile.T7ReaderException: unknown object type / typeidx: -1112529805,"Please try appending the parameter long_size like this `load_lua(....., long_size=8)`.","You need to append the parameter long_size like this `load_lua(....., long_size=8)`."
RuntimeError: CUDA error: unknown error,I had exactly the same issue. i got it fixed by installing the right pytorch version for my 10.1 CUDA with:\r\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\r\ninstead of installing it with the installtion snippet generated for me by pytorch website:\r\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n,You need to install the right pytorch version for 10.1 CUDA with:\r\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\r\ninstead of installing it with the installtion snippet generated by pytorch website:\r\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n
optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum,"I think you are correct, if you indent the second and third to last lines in the init it works.",It can be fixed by indenting the second and third to last lines in the init
Different deterministic behavior between CPU and CUDA for orthogonal initialization,"This is expected. We do not guarantee that the random sequences generated on different devices will look the same. For performance reasons they are likely using different pseudo random generators, so it's almost impossible to make them equal.",This is expected. It is not guaranteed that the random sequences generated on different devices will look the same.
C++ torch::Tensor serialization,Cool!! I just checked that I can both save and load torch::Tensors and std::vector<torch::Tensor>s with the latest macOS libtorch! Closing this now :),"Using  the latest macOS libtorch, one can both save and load torch::Tensor and std::vector<torch::Tensor>"
Compiling from master yields std::runtime_error,"@pietern Thanks for tracking this.

Yet, I've been able ""overcome"" this somehow by compiling PyTorch again with a git clean -fdx in between, thus I don't have the logs for the failed compilation.

I just tried again with the same command line I ran this morning and the error doesn't occur. I'll investigate a little bit more on my side, to see if I can reproduce this, and will follow up on this topic.","You need to compile PyTorch again with a git clean -fdx in between, "
Improved documentation of distributed launch utility,Thanks for reporting! We are aware of this confusing doc issue and are actively working on remediating it. Please track our progress in https://github.com/pytorch/pytorch/issues/60754. Closing in favor of 60754.,We are working on this.
test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS,"Created a conda env with Python=3.7, followed the same steps to install Pytorch and tried to reproduce the issue.\r\n\r\ntest_dataloader.py passes the test.\r\n\r\nSeems that it is an issue related to Python 3.8 on Mac",This is an issue related to Python 3.8 on Mac
[package] error in colab tutorial #60189,This error is intentional—you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.,This error is intentional—you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.
USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory,"I got the same error, and I fixed it by updating the submodules:\r\n```\r\ngit submodule sync\r\ngit submodule update --init --recursive\r\n```",You need to update the submodules:\r\n```\r\ngit submodule sync\r\ngit submodule update --init --recursive\r\n```
Can I train AI If AI model is located in the another model’s forward?,You may want to add model2 to an attribute of model1 in \\_\\_init\\_\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\r\n\r\nfyi questions are supposed to be posted to https://discuss.pytorch.org/ as it's more likely you will get help there.,You may want to add model2 to an attribute of model1 in \\_\\_init\\_\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\r\n\r\n
M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`,Will be fixed in next nightly build,This will be fixed in the next nightly build.
it seems n_heads is not handled correctly in nn.MultiheadAttention,"As a simple workaround, couldn't you pass embed_dim * num_heads as the embed_dim? When it is divided amongst the heads, it should result in what you want.",You can pass embed_dim * num_heads as the embed_dim
`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky,@imaginary-person This test was failing consistently on master yesterday so I disabled them for XLA first. @bdhirsh and I will look into the root cause later today. Thanks for reporting!,You need to diable them for XLA.
Does the NCCL operation use the default stream as other computations?,"The nccl implementation under the hood uses a custom stream pool and each time you launch collective operations, appropriate synchronizations are done automatically. So before the allreduce is launched, the nccl stream synchronizes with the default stream to capture computations appropriately. Also, after nccl allreduce is done, the default stream synchronizes with the nccl stream. As a result, you don't need to perform any synchronizations.\r\n\r\nIf you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option.","If you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option."
Add container for recurrent nets,"No, definitely not to `Module`. We can consider adding a base class for RNNs","No, definitely not to `Module`. We can consider adding a base class for RNNs"
Support for einsum notation,"Yeah, Pytorch lacks this. I can implement the feature",Support for einsum notation has been added
GPU usage extremely in-balance for segmentation task,"The code hasn't been released yet. You can leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs","Leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs"
How to select GPU programmatically in code,"```\r\nimport os\r\nos.environ[\""CUDA_DEVICE_ORDER\""]=\""PCI_BUS_ID\"" \r\nos.environ[\""CUDA_VISIBLE_DEVICES\""]=\""1\""\r\n```\r\n\r\nAre you looking for this?","```\r\nimport os\r\nos.environ[\""CUDA_DEVICE_ORDER\""]=\""PCI_BUS_ID\"" \r\nos.environ[\""CUDA_VISIBLE_DEVICES\""]=\""1\""\r\n```\r\n"
Pooling throws an exception in Tegra TX1,"this is a weird error. Are you sure CuDNN is being detected at compile time?

You might have to set CUDNN_INCLUDE_DIR and CUDNN_LIB_DIR environment variables.
You can check cudnn's status with: print(torch.backends.cudnn.enabled)","You might have to set `CUDNN_INCLUDE_DIR` and `CUDNN_LIB_DIR` environment variables.
"
[Feature Request] Cyclical Learning Rates,"Hi. I am the puller of LR Scheduler. IMO, you should be able to easily implement this using class LambdaLR. @ajbrock

",This can be easily implemented using class `LambdaLR`
Get a single batch from DataLoader without iterating,`next(iter(data_loader))` ?,Use `next(iter(data_loader))` 
Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid,"Thanks for reporting this @martinarjovsky.

@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?

Because of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)

According to the docstrings of binary_cross_entropy and binary_cross_entropy_with_logits the input and target should have the same shape. Shall we just add a check and raise a ValueError if they are not? Happy to send a PR for this",The behavior is same if `binary_cross_entropy` and `binary_cross_entropy_with_logits` the input and target should have the same shape
[pylint] E1101:Module 'torch' has no 'squeeze' member,"On VS code:
Add ""python.linting.enabled"": false to the settings file. Worked for me.","You need to change the settings for your editor. On VS code, you need to add ""python.linting.enabled"": false to the settings file."
[Feature Request] Layer Normalization,"I use this:\r\n\r\n```python\r\nclass LayerNorm(nn.Module):\r\n\r\n def __init__(self, features, eps=1e-6):\r\n super().__init__()\r\n self.gamma = nn.Parameter(torch.ones(features))\r\n self.beta = nn.Parameter(torch.zeros(features))\r\n self.eps = eps\r\n\r\n def forward(self, x):\r\n mean = x.mean(-1, keepdim=True)\r\n std = x.std(-1, keepdim=True)\r\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n```","This code segment can acheive layer normalization:\r\n\r\n```python\r\nclass LayerNorm(nn.Module):\r\n\r\n def __init__(self, features, eps=1e-6):\r\n super().__init__()\r\n self.gamma = nn.Parameter(torch.ones(features))\r\n self.beta = nn.Parameter(torch.zeros(features))\r\n self.eps = eps\r\n\r\n def forward(self, x):\r\n mean = x.mean(-1, keepdim=True)\r\n std = x.std(-1, keepdim=True)\r\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n```"
Import fails after Conda install,"your `cudatoolkit` from anaconda is build 2, which had a bug. Anaconda team fixed this in build 3. Do this: `conda install cudatoolkit`, and that should install build 3.",You need to install the latest build via `conda install cudatoolkit`
ReduceOps are breaking Pyro test,"I found the bug, I'll send a patch soon.",This issue has been fixed.
Linux CPU build script fails as MKL header files not found,"If you are not using conda, pip install mkl-devel gets you the headers. I haven't looked at your CI script. @SsnL can we add this to the error message?

","To install the header files, use `pip install mkl-devel`"
RuntimeError: value cannot be converted to type uint8_t without overflow: 10000,"yeah the tutorials needs to be updated. for now, you can try this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```",You can use this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```
CUDNN_STATUS_NOT_INITIALIZED when built from source,After these updates:\r\n- Nvidia driver: 384.81-> 387.26 \r\n- CUDA: V9.0.176 -> V9.1.85\r\n- CuDNN: 7005 -> 7102\r\n\r\nThe installation completes without error. Thanks!,"You need to install the latest versions of Nvidia driver, CUDA, and CuDNN"
I cannot initialize Tensor. (They will become torch.autograd.variable.Variable),See https://github.com/pytorch/pytorch/pull/5225,This issue has been fixed.
[feature request] Add underscore to nn.init functions,"That makes sense, but we need to keep backward compatibility, so we need the non-`_` aliases too.",Non-underscore functions have been retained for backward compatibility. 
How can I access the model's attribution created during forward pass when using dataparallel?,"Returning what you want from the top-level forward function should defiantly work. You could do this pretty easily with an additional wrapper. As such:\r\n\r\n```python\r\n\r\nclass NormalModel(nn.Module):\r\n pass\r\n\r\nclass ExtraOutputWrapper(nn.Module):\r\n def __init__(self, *args, **kw):\r\n self.wrapped = NormalModel(*args, **kw)\r\n\r\n def forward(self, input):\r\n normal_output = self.wrapped(input)\r\n extra_output = self.wrapped.layer_of_interest.property\r\n return normal_output, extra_output\r\n```\r\n\r\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest. This seems fairly clean to me. Is there an issue with this?","You can use the following code:\r\n\r\n```python\r\n\r\nclass NormalModel(nn.Module):\r\n pass\r\n\r\nclass ExtraOutputWrapper(nn.Module):\r\n def __init__(self, *args, **kw):\r\n self.wrapped = NormalModel(*args, **kw)\r\n\r\n def forward(self, input):\r\n normal_output = self.wrapped(input)\r\n extra_output = self.wrapped.layer_of_interest.property\r\n return normal_output, extra_output\r\n```\r\n\r\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest."
Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object,The most recent version of cpuinfo should fix this. We have to update the submodule.,Install the latest version of cpuinfo to fix this.
fatal error: torch/torch.h: No such file or directory,"Right now you have to `build install` pytorch for that header to be installed correctly. There's three options:\r\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\r\n2. `python setup.py build install`\r\n3. `python run_test.py --exclude cpp_extensions`\r\n\r\n(1) is probably best","There are three options:\r\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\r\n2. `python setup.py build install`\r\n3. `python run_test.py --exclude cpp_extensions`\r\n\r\n(1) is probably best"
pytorch installation error on macOS 10.13.3,^ resolved by reinstalling tbb,This error can be resolved by reinstalling TBB.
Pretrained Model Loading Error,"If you were to load the file using pickle alone, I'd suggest trying to load it using `encoding='latin1'` for example.\r\n```python\r\nwith open(myfile, 'rb') as f:\r\n data = pickle.load(f, encoding='latin1')\r\n```\r\nThis has solved issues for me when deserializing some pickle files saved on python2 and loaded on python3. But to make it work out-of-the-box on `torch.load` might require extending the API.",Add the `encoding=latin1` argument to `pickle.load`
Bazel - Pybind - Pytorch - Undefined Symbol,Resolved. Turns out you need to add 'linkshared=True' and compile it as a binary,You need to add 'linkshared=True' and compile it as a binary
"TypeError: __init__() should return None, not 'int' in validation Dataset","Closing, as this does not look like a bug in PyTorch, but rather a mis-use of  Python data model.\r\nFor example, following code raises the same runtime error:\r\n```\r\nclass Bar:\r\n    def __init__(self, bar):\r\n        self.bar = bar\r\n        return bar\r\n\r\nif __name__ == \""__main__\"":\r\n    x = Bar(5)\r\n```\r\n```\r\n% python3 bar.py\r\nTraceback (most recent call last):\r\n  File \""bar.py\"", line 7, in <module>\r\n    x = Bar(5)\r\nTypeError: __init__() should return None, not 'int'\r\n```\r\nPlease do not hesitate to comment/update an example, if you believe that PyTorch behavior is incorrect in this case.",This is an expected behavior.
torch.linalg.cholesky fails for some PSD matrices,"Thanks for your response. Makes total sense - the context of the problem is trying to add a small amount of (tikhonov) regularization to make cholesky stable so aware of the stability problem just my fix was mixing precisions as you say. With double precision numpy==pytorch. Feel free to close.

",Using double precision will not give any errors.
Scribe stats reporting is broken in GHA due to secrets access from fork PRs,Verified that scribe proxy is working now,This issue has been fixed.
HTTP Error 403 for torch.hub ResNet,"As a workaround, can you please adding the following line before making any \""torch.hub\"" calls:\r\n```\r\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n```","Please add the following line before making any \""torch.hub\"" calls:\r\n```\r\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n```"
torch.permute missing in docs,"This has been fixed in master.\r\n\r\nThat being said, the docs are not well formatted, so I'll fix that.",This issue has been fixed.
"Conv2d triggers assertion in mkl-dnn when padding=(n, 3)","@sakaia , I verify that MKLDNN v0.21.1 can solve this problem, we have upgrade mkldnn to 0.21.1, you can test it your side by pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html or build the pytorch source code according to README.md. Thanks!",Installing MKLDNN v0.21.1 can fix this issue.
PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES,More comment:\r\n\r\nUsing the following command instead\r\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`\r\nwould fix the problem.\r\n\r\nBut I am not sure if this is the expected behavior...,Use the following command \r\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`
Allow parallel sending to device in DataLoader,"No the push occurs inside the transform. It would be possible to design a custom collate_fn but that would be inefficient. The whole point of doing this is because pushing to GPU is so time consuming. Therefore, the best way is to send to GPU in a data transform.",The best way is to send to GPU in a data transform.
distributed all_reduce deadlocks in v1.1,"Pytorch 1.1 uses nccl 2.4.2 which has a known issue of hanging with long running jobs that was fixed for 2.4.6. https://github.com/NVIDIA/nccl/commit/f40ce73e8987d2990e4b9ef6c75f4b3423acce78\r\nWorkaround is to export NCCL_LL_THRESHOLD=0. \r\ncc @pietern, @mrshenli to bump nccl submodule.",Installing nccl 2.4.6 can fix this issue.
Build error with MSVC (aten\\src\\ATen\\native\\quantized\\Copy.cpp),"move the sentence \""float* src_data = src.data<float>();\"" into the function of \""AT_DISPATCH_QINT_TYPES ...\"" solves my problem. It looks like:\r\n\r\n AT_DISPATCH_QINT_TYPES(self.scalar_type(), \""Copy\"", [&]() {\r\n float* src_data = src.data<float>();\r\n\tscalar_t* self_data = self.data<scalar_t>();\r\n for (int i = 0; i < self.numel(); ++i) {\r\n self_data[i] = quantize_val<scalar_t>(\r\n self.q_scale().to<float>(),\r\n self.q_zero_point().to<int32_t>(),\r\n src_data[i]);\r\n }\r\n });\r\n\r\n Hope that will help you.","Moving the sentence \""float* src_data = src.data<float>();\"" into the function of \""AT_DISPATCH_QINT_TYPES ...\"" can solve this problem."
RuntimeError: ONNX export failed: Couldn't export operator aten::softmax,I just checked the nightly version! There it works fine! Thanks! :),This works fine for the nightly version.
MultiheadAttention is not scriptable,"Thanks for bringing up the issue and we have fixed it. We also added two JIT unit tests to cover the applications of torch.nn.MultiheadAttention module (i.e. ""test_torchscript_multi_head_attn"" and ""test_scriptmodule_multi_head_attn_cuda"" in test_jit.py). Those two could possibly be good examples for JIT scriptable.",This issue has been fixed.
Building libtorch-dependent project with CMake,"I don't know the answer to this, but I suggest using new cmake 'targets' feature, more like\r\n\r\n```\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_library(a)\r\ntarget_link_libraries(a PRIVATE Torch)\r\n```\r\ninstead of messing with TORCH_INCLUDE_DIRS and TORCH_LIBRARIES themselves.","You can use new cmake 'targets' feature, e.g.\r\n\r\n```\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_library(a)\r\ntarget_link_libraries(a PRIVATE Torch)\r\n```"
Parameter not registering if .to(device) is used,"this is totally expected.\r\n\r\nParameters can only be leaf Tensors, not Tensors that are a result of an operation on another Tensor.\r\n\r\nWhat you want is:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```","You need to use:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```"
Windows CPU debug build fails at linking stage,"Hi,\r\nYes. With #17494 I can build without set BUILD_TEST=OFF\r\n\r\nThanks!",Build will succeed without setting BUILD_TEST=OFF\r\n\
Conda did not install cudnn for pytorch,"Hi,\r\neven if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`.","Even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`."
Tensor unfold backward is slow,"This was fixed by gh-36612, closing.",This issue has been fixed.
cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer,CUDA 9 and RTX 2080 Ti simply aren't compatible and dont play well togethere.\r\nAn older CuDNN version working is likely a side-effect rather than expectation.\r\nUse CUDA10 and CUDA10 versions of CuDNN etc. for RTX 2080 which is Turing architecture,Use the latest versions of CUDA10 and CuDNN
PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`,"Sorry I think I misunderstood your point above then, I though you were still seeing a problem even when .item() is used.
In that case it is (unfortunately) the expected behavior here.

It is easy to free any buffers encountered during the backward pass and raise a proper error if the user try to use them again.
But it is much harder to delete the graph structure itself while we're traversing it to compute the backward. Also the memory usage due to a single graph is small compared to the other objects in general and so should not be a problem (unless of course the graph keeps growing as in your case).

Another approach to solve this problem would be to use the `with torch.no_grad():` block (or the functions decorator equivalent) around any operations for which you won't need to compute backward. This way, it won't even build the graph and so will be faster and less memory hungry.",Enclose operations that do not require the backward computations within the  `with torch.no_grad():` block
Is it possible to integrate jax into pytorch ?,"1.1, we already have with torch.jit\r\n1.2 seems interesting\r\n2. probably not relevant\r\n\r\n1.2 is tracked in https://github.com/pytorch/pytorch/issues/1642\r\n\r\nOverall, there isn't really the concept of \""jax in pytorch\"" or \""pytorch in jax\"" in the same sense that they're both frontends",Jax and PyTorch are both frontends and cannot be integrated.
"ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5","Looks like it's not an issue with latest TRT6 and parser, so we can close it.",This issue has been fixed.
Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing,"@subho406 seems to work fine for me (with 1.1.0 and the latest `examples` repo), just fyi",This issue has been fixed.
Why torch wheel is so huge (582MB)?,"The number of GPU architectures targeted by the binary is a large contributor to binary size. If you'd like a smaller GPU binary, you can change TORCH_CUDA_ARCH_LIST accordingly. For example, if you want to only support compute 5.0 w/forward compatibility, you can set TORCH_CUDA_ARCH_LIST=5.0+PTX as opposed to the more comprehensive list built by default by the PyTorch devs.\r\n\r\n(I'm not a PyTorch developer, just someone who has built smaller versions of the library.)",You can specify the GPU architectures during installation to reduce the size.
"We should not mark non-floating point Tensors as requirering gradients, ever","Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense.","Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense."
Pickling of _VariableFunctions no longer works in 1.5,If any newcomers come here and wonder how to fix this. Upgrading to 1.5.1 fixed the issue. Downgrading to 1.4.1 may too.\r\n\r\nThat is\r\n`conda install pytorch==1.5.1 torchvision==0.6.1 cudatoolkit=10.1 -c pytorch` if you use vision\r\n`conda install pytorch==1.5.1 cudatoolkit=10.1 -c pytorch` if you use NLP.\r\n\r\ndepending on your cuda version.,Installing the latest version of PyTorch can fix the issue.
Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions,include_dirs now supports both absolute and relative paths.,include_dirs now supports both absolute and relative paths.
Empty GPU memory cache after Jupyter notebook interrupted,"jupyter notebook holds reference to the exception when interrupted (for things like %debug), which holds reference to the stack frames, which hold reference to the variables. so empty_cache won't work. this is not really a pytorch issue per se.",This issue is not related to PyTorch.
torch.utils.checkpoint.checkpoint + torch.cuda.amp fails,"Thanks for raising this issue!\r\nMy best guess is that `CheckpointFunction.backward` uses the stored mixed-precision tensors from its `forward`, but breaks the autocasting contract for the backward.\r\nIf you run `scaler.scale(loss).backward()` inside the `autocast` block, it should work for now as a workaround.\r\n\r\nCC @mcarilli","Running `scaler.scale(loss).backward()` inside the `autocast` block, should work"
Windows 10 Libtorch installation issue.,"Okay. I resolved it myself. 👍 \r\nThe tutorial needs to add this line in CMakeLists.txt file\r\n``` set(CMAKE_PREFIX_PATH \""libtorch/share/cmake/Torch\"") ```\r\n\r\nwhich should point to where Torch is unzipped appropriately.\r\nPlease update the docs. This issue is quite prelevant\r\n","You need to add this line in CMakeLists.txt file\r\n``` set(CMAKE_PREFIX_PATH \""libtorch/share/cmake/Torch\"") ```"
torch.remainder gives a remainder larger than the divisor,"Thanks for the report!\r\n\r\nIt turns out that this problem is due to numerical precision issue when you subtract two larger numbers with only small difference, such as 100000000002 - 100000000001. Even the numpy single precision result of 1.024195 is not accurate (from c function `fmodf`). The accurate double precision result is 0.577396.\r\n\r\nThe current `torch.remainder` impl for both cpu and gpu have this problem, e.g. https://github.com/pytorch/pytorch/blob/253943d5a7bb90a420e4d94366101915823c7929/aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu#L80-L85\r\n\r\nI also checked the `torch.fmod` impl which are correct for both cpu and gpu. You can temporarily use `torch.fmod` as a substitute for `torch.remainder`. I will provide a fix to `torch.remainder` soon, and let it use the native c/cuda function of `fmod`.",This is due to numerical precision issues. Please use `torch.fmod` instead of `torch.remainder`
torch.cdist [Cuda out of memory Tried to allocate 108GB of memory],"I'm closing it as after building my env from scratch it now works. (I was already on torch 1.5, so I can't directly point on the problem but ...)",Building environment from scracth will fix the issue.
C++ API for Transformer model in libtorch 1.5.0,This is available since 1.7,This is available in the recent versions of PyTorch.
RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py),@AugF `LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.\r\nThere should be a `/lib` after `site-packages/torch/`,`LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.
Better testing on CPUs without AVX capabilities,"In that case we can run cpp tests under qemu, which can be configured not to support any vecorized instructions.",You can use a machine emulator such as QEMU.
[BatchNorm] Unexpected behaviour with track_running_stats,Closed by #38084,This issue has been fixed.
`torch.log10` with float32 produces different results on different CPU,"@mruberry \r\nYes, that makes sense. I wanted to leave a record of this finding and double check if there is something that we can work on.\r\n\r\nWhile filling out this issue I also found that `float64` produces consistent result, so we are going to use `float64` in torchaudio to resolve the issue we are having.\r\n\r\nThanks!",Using `float64` can resolve this issue.
Falling to turn shape into Tensor,"Do\r\n```python\r\na = torch.ones(10, 10)\r\nb = torch.tensor(a.shape)\r\nprint(b)\r\n# tensor([10, 10])\r\n```\r\nAlso, questions like this are better suited for https://discuss.pytorch.org/","This can be done via \r\n```python\r\na = torch.ones(10, 10)\r\nb = torch.tensor(a.shape)\r\nprint(b)\r\n# tensor([10, 10])\r\n```"
[pytorch] [feature request] Entropy function,you can calculate this via `distributions.Categorical(probs=p).entropy()`?,This can ve calculated via `distributions.Categorical(probs=p).entropy()`
mse_loss reduction='none' is ignored when required_grads is True,"Many thanks @li-roy, with 1.0.0.dev20181018 it works smoothly.",This issue has been fixed.
3x regression in JIT LSTM speeds,Yes the slowdown was due to memory leak,This issue arises because of memory leaks
windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device,"The new binaries are updated for Windows.\r\n\r\nIf you have previously installed via anaconda, you have to do:\r\n\r\n```\r\nconda uninstall -y pytorch\r\nconda clean -t -y\r\n```\r\n\r\nand then reinstall pytorch.\r\n\r\n\r\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",You need to uninstall and reinstall PyTorch.
Quadro m2000m not able to get pytorch working with gpu,"The new binaries are updated for Windows.\r\n\r\nIf you have previously installed via anaconda, you have to do:\r\n\r\n```\r\nconda uninstall -y pytorch\r\nconda clean -t -y\r\n```\r\n\r\nand then reinstall pytorch.\r\n\r\n\r\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",You need to uninstall and reinstall PyTorch.
"Wrong Warning \""compiler (c++) may be ABI-incompatible with PyTorch!\""","@johnmarkwayve no, the warning is only raised if you're using a binary build of Pytorch, i.e. from pip. It's never raised if you compile from source.",The error is not encountered if compiled from source.
[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.,"Yes @vadimkantorov , that was the solution!!
I enabled ""set_flush_denorm"" in case of CPU and now different checkpoints have the same inference time. Many thanks!",Inference time is the same if `set_flush_denorm` is used with CPU. 
CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++),"I just found out that it works, if I add \""filter\"" as external input for the \""train_net\"" like so:\r\n\r\n train_net.add_external_input (\""filter\"");\r\n\r\nbefore executing CreateNet(train_net).",Adding `filter` as external input for the model works.
MSELoss wrongly sums instead of averages when reduction='elementwise_mean',"Yes but it’s fixed on master and will be included in the next release.\n\nOn Wed, Sep 19, 2018 at 03:52 simama <notifications@github.com> wrote:\n\n> I am on 0.4.1 version and it seems this bug is still not fixed.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/10148#issuecomment-422696370>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZfuFXXaUvgqzfKfZO7_kPUMt0gsQks5ucfe-gaJpZM4VrZIT>\n> .\n>\n",This issue has been fixed.
Backward engine computes unnecessary dependencies,Fixed in #752.,This issue has been fixed.
F.relu(inplace) followed by F.dropout(inplace) breaks backward pass,"Yeah, that is expected. You can't apply in-place operations to leaf Variables. Just remove the `inplace` flag and it should be good.",Removing the `inplace` flag should make it work.
"torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",I think we should make a `torch.arange` that is equivalent to `numpy.arange` and depreceate `torch.range` properly. It is used quite extensively all over the place.\r\nwdyt @colesbury @apaszke ?,`torch.range` needs to be deprecated
LSTM forget gate bias initialization,"Yes, the ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value.","The ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value."
Error while saving my network,this is a syntax error. `torch.save` does not have the order of arguments as you've used.,Using the correct arguments will not give this error.
Build fails on Ubuntu 14.04 + conda latest,CUDA Version 8.0.27 has this issue. it is a pre-release version. The stable CUDA8 version is 8.0.44,Installing the latest stable release fixes the issue.
view() after transpose() raises non contiguous error,"Yes, this is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue","This is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue"
Errors import torch installed form source on macOS,Run python from a different directory than the repository root.,You need to open torch in any directory other than repo's root
Add support for variable length sequences in cuDNN RNNs,@glample CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality,CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality
Maxout Layer,"For ones who need Maxout, I changed the above code to make it work. \r\n\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n def __init__(self, d_in, d_out, pool_size):\r\n super().__init__()\r\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n self.lin = nn.Linear(d_in, d_out * pool_size)\r\n\r\n\r\n def forward(self, inputs):\r\n shape = list(inputs.size())\r\n shape[-1] = self.d_out\r\n shape.append(self.pool_size)\r\n max_dim = len(shape) - 1\r\n out = self.lin(inputs)\r\n m, i = out.view(*shape).max(max_dim)\r\n return m\r\n```\r\n","The Maxout layer can be implemented as follows \r\n\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n def __init__(self, d_in, d_out, pool_size):\r\n super().__init__()\r\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n self.lin = nn.Linear(d_in, d_out * pool_size)\r\n\r\n\r\n def forward(self, inputs):\r\n shape = list(inputs.size())\r\n shape[-1] = self.d_out\r\n shape.append(self.pool_size)\r\n max_dim = len(shape) - 1\r\n out = self.lin(inputs)\r\n m, i = out.view(*shape).max(max_dim)\r\n return m\r\n```\r\n"
Unefined reference to C10::Error::Error when linking against libTorch,"I think you're missing `\""-D_GLIBCXX_USE_CXX11_ABI=0\""` in your build flags. We provide this in our TorchConfig.cmake, which is why we recommend cmake as the easiest way to build LibTorch. That said, you can add that to your QtCreator config. See https://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in for the relevant file\r\n\r\nPlease let me know if that fixes the issue.",You need to use the build flags `QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0`
can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.,"`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor.","`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor."
GPU Memery leak,i find a solution yesterday\r\n\r\nbatch_y_predlabel has to be **detach** so the GPU memory can be free.\r\n\r\nI don't know if it's normal that a GPU tensor transfer to the CPU in the compute graph still take GPU memory?\r\nfor gradient compute I imagine?,"To free up GPU memory, you need to detach the predictions."
[Build error] libnccl.so: error adding symbols: File in wrong format,"it's very likely that you installed x64 libnccl, instead of ppc64 version",You need to install the correct version for your platform
Can cuda10 use pytorch-0.4.1?,no,"Not, it cannot."
cosine_similarity function produces results more than 1.0,"Probably better to re-order the computations to improve numerical precision. Might want to look at SciPy:\r\n\r\nhttps://github.com/scipy/scipy/blob/453932337f4a67170e4e7fda3f808b273a787a41/scipy/spatial/distance.py#L717-L721\r\n\r\nI think the issue is that we're doing:\r\n\r\n```\r\nx / (sqrt(x) * sqrt(x)) # bad\r\n```\r\n\r\nvs.\r\n\r\n```\r\nx / sqrt(x * x) # good\r\n```\r\n\r\nI don't know enough about floating point arithmetic to argue why the second is more accurate, but it seems to be. (You can look at the min/max over a large random tensor)",This error can be fixed by reordering the computations to improve numerical precisions.
Is mkl-dnn enabled in the latest binary distribution v1.0.1?,Yes.,Yes.
nn.LSTM gives nondeterministic results with dropout and multiple layers,Closed and fixed in cudnn_7.6.1 @ngimel,This issue has been fixed.
Add support for mixture models in torch.distributions,"`MixtureSameFamily` should be easy to implement. We don't use this in Pyro since we usually keep the mixture component id as an explicit variable and enumerate over that variable:\r\n```py\r\ncomponent = pyro.sample(\""component\"", dist.Categorical(probs),\r\n                        infer={\""enumerate\"": \""parallel\""})\r\nassert component.reshape(-1).shape == probs.shape[-1:]\r\nvalue = pyro.sample(\""mixture\"", MyDistribution(my_params[component]))\r\n```\r\ncc @martinjankowiak","`MixtureSameFamily` can be implemented as:\r\n```py\r\ncomponent = pyro.sample(\""component\"", dist.Categorical(probs),\r\n                        infer={\""enumerate\"": \""parallel\""})\r\nassert component.reshape(-1).shape == probs.shape[-1:]\r\nvalue = pyro.sample(\""mixture\"", MyDistribution(my_params[component]))\r\n```"
IndexError while trying to save torchscript,"Closing this since I can't reproduce and we fixed some similar errors recently, but feel free to re-open if you're still getting this error",This issue has been fixed.
[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED,Closing as this has been fixed (the repro in the initial post now works as intended),This issue has been fixed.
[CPU] several inplace functions fail since 1.0.1 on certain hw,I can reproduce this with OMP_NUM_THREADS=2. I haven't seen it with OMP_NUM_THREADS=1.,Setting `OMP_NUM_THREADS=1` will not raise an error.
module' object has no attribute '_dl',"it's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\r\n\r\nTry:\r\n\r\n```\r\npip uninstall torch\r\npip uninstall torch\r\npip uninstall torch\r\npip install torch\r\n```",You need to uninstall and reinstall PyTorch.
IndyLSTM & IndyGRU in PyTorch,"it's not widely used enough yet, to be pushed into core (feel free to reopen once it becomes more of a standard).\r\n\r\nAdditionally, we are working on a user handwritten RNNs being fast, rather than adding more fundamental multi-layer RNNs into core.\r\nSo, I'm closing the feature request.",This feature will not be implemented.
Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight',your input is on gpu 1 but your net work is on gpu 0.,Model and data need to be on the same GPU.
CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1,"I got the same issue with Pytorch 1.3.1 + CUDA 10.0. Finally, I got it resolved with:\r\n```\r\npip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\r\n```",You need to downgrade your version of CUDA.
error executing torch_shm_manager in cifar10_tutorial.py,"Experienced the same issue on OSX. Setting `num_workers=0` on the DataLoaders solved it, even though the tutorial only recommends it for Windows.\r\nIt should probably be better clarified/fixed though.",Setting `num_workers=0` will not raise an error.
"\""module has no attribute 'downsample'\"" when scripting torchvision's resnet",I believe you need to use a nightly version of torchvision (and thus a nightly version of PyTorch) for resnet to be scriptable.,You need to install the latest version of PyTorch.
"Failed to load model on mobile for device type \""c10::DeviceType::CUDA\""",@ljk53 sorry late response. \r\nI confirmed the model converted to cpu() is able to load on the phone.,You need to move the model to `.cpu()`
Didn't find kernel to dispatch to for operator 'quantized::conv2d',"Hi @thiyagu145, the error message you've encountered says that you're trying to run the quantized Conv2d operator, but you're passing in an unquantized tensor.

Please take a look for where we use QuantStub in the tutorial and workflow documentation. Once this is present and the values are passed through them, the issue should be solved",Passing a quantized vector to the operator will not raise an error.
"Why t.arange(0,3) create an int type Tensor?","torch.arange returns an int64 tensor by defaut. It mimics numpy behavior. if you want different dtype, give dtype as argument, such as `t.arange(0, 3, dtype=torch.float32)`",You need to specify the datatype as an argument.
Loading opencv image to pytorch tensor,"I found it at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#exhale-class-classat-1-1-tensor.\r\nHere is a demo:\r\n```C++\r\n tensor_image = tensor_image.permute({0, 3, 1, 2});\r\n```",The order of dimensions is different in OpenCV and PyTorch.
Bug in transferring model from pytorch --> caffe2,"I modified the tutorial and removed the `pixel_shuffle` part and composed the network only with ReLU and conv layers (just to see if `pytorch` and `caffe2` give the same output), and **now it works fine**.\r\n\r\nSince `pixel_shuffle` is only on the master branch, I assume that it's still buggy?\r\nAnyway, I'll just test if all the layers I personally need output the same thing between the two frameworks and proceed.",Remove the `pixel_shuffle` part and composed the network only with ReLU and conv layers will not give an error.
Using net.cuda crashes the kernel,"If you look into the list of types in the `got (...)` part, you'll find a mix of CPU and CUDA tensors, with `input` and `output` being on CPU, while `weight` and `bias` is on the GPU.\r\n\r\nYou probably forgot to send the input to the GPU. Alternatively, keep in mind that `.cuda()` is an out of place operation i.e.\r\n```python\r\ninput.cuda()\r\nmodel(Variable(input))\r\n```\r\nwill fail. You need to overwrite the reference with a new CUDA tensor:\r\n```python\r\ninput = input.cuda()\r\nmodel(Variable(input))\r\n```",You need to overwrite the reference with a new CUDA tensor:\r\n```python\r\ninput = input.cuda()\r\nmodel(Variable(input))\r\n```
Gumbel noise,"Here you go. Much more readable and no modules required:\r\n```python\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef sampler(input, tau, temperature):\r\n noise = torch.rand(input.size())\r\n noise.add_(1e-9).log_().neg_()\r\n noise.add_(1e-9).log_().neg_()\r\n noise = Variable(noise)\r\n x = (input + noise) / tau + temperature\r\n x = F.softmax(x.view(input.size(0), -1))\r\n return x.view_as(input)\r\n```\r\nWe're using GitHub for bug reports only, if you have questions please post the on [our forums](https://discuss.pytorch.org).","The code is as follows:\r\n```python\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef sampler(input, tau, temperature):\r\n noise = torch.rand(input.size())\r\n noise.add_(1e-9).log_().neg_()\r\n noise.add_(1e-9).log_().neg_()\r\n noise = Variable(noise)\r\n x = (input + noise) / tau + temperature\r\n x = F.softmax(x.view(input.size(0), -1))\r\n return x.view_as(input)\r\n```"
GOMP_4.0 not found,I fixed the problem by importing opencv before torch.,Importing OpenCV before PyTorch fixes the issue.
Cannot install `torchvision 0.1.7` by using conda,"i see. in this case, just remove torchvision (conda uninstall torchvision) and then install it via pip.\r\n\r\npip install torchvision",Installing torchvision through pip fixes this issue.
nn.Module not importing parameters contained in lists,This behaviour is expected.\r\nThere is a detailed discussion in https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219,This behaviour is expected.
libTH doesn't recognize Intel MKL in its default location,"I encounter the same error, and I solve it by running command 'conda install mkl' in my activated conda env.\r\n\r\n> File \""/usr/local/lib/python3.5/site-packages/torch/__init__.py\"", line 45, in <module>\r\n> from torch._C import *\r\n> ImportError: dlopen(/usr/local/lib/python3.5/site-packages/torch/_C.cpython-35m-darwin.so, 10): Library not loaded: @rpath/libmkl_intel_lp64.dylib\r\n> Referenced from: /usr/local/lib/python3.5/site-packages/torch/lib/libTH.1.dylib\r\n> Reason: image not found",Running the command `conda install mkl` fixes this issue.
[build/nccl] failed to build libnccl on Debian unstable,"@apaszke Thanks, the fix is to to export the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```",You need to exprt the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```
Allow optimizers to skip nn.Parameters that have requires_grad=False,"So I don't really think that it makes sense to allow such parameters. If you don't want to optimize some tensors, they're not parameters - they're fixed. You probably don't want to count them in. And if you really need to then\r\n```pytorch\r\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\r\n```\r\nshould do the trick.","This can be done via ```pytorch\r\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\r\n```"
cuda runtime error (8) : invalid device function - adding cuda tensors,"@adelsalehali1982 Your error is unrelated to this issue, and happens because the code you are running supposes that 4 GPUs are used (if gpus are enabled), and you probabmy have less than 4 GPUs in your machine. You can fix that by changing the line with DataParallel to only use 1 or 2 GPUs",You need to use a more recent GPU for this to work.
"\""Symbol not found\"" when \""import torch\"" on Mac OS","As a workaround, \r\n\r\n\r\n`pip3 install torchtext==0.4`\r\n\r\nsolved the issue.",Install torchtext from pip instead of source to fix this issue.
How to use cudnn in pytorch？,To verify that pytorch uses cudnn:\r\n```\r\n>>> torch.backends.cudnn.version()\r\n6021\r\n```,To verify that pytorch uses cudnn:\r\n```\r\n>>> torch.backends.cudnn.version()```
[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member,"On VS code: \r\nAdding `\""python.linting.enabled\"": false` also worked in this case.","You need to set the following flag `""python.linting.enabled\"": false` "
ImportError: No module named 'tools.setup_helpers',"@soumith I expect people will just keep doing this until there's a working package on PyPI. That's just the first thing people are going to try for a Python project.\r\n\r\nMaybe you could upload an `0.2` wheel that just prints a more helpful error like:\r\n\r\n> Installation from PyPI not supported yet (see status at `https://github.com/pytorch/pytorch/issues/566`). For now, please uninstall this package (`pip uninstall pytorch`) and follow the instructions at `http://pytorch.org/` to install with miniconda.",Install using instructions from PyTorch's website.
"from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",It is pydot.\r\n,You need to install pydot.
ImportError: No module named future.utils,sudo pip install future\r\n or\r\neasy_install future,You need to install the package 'future'
Raise correct error type when passing invalid covariance matrix to MultivariateNormal,"As mentioned in #12102, the issue is that arg checking [happens](https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L30) in `Distribution.__init__()`, but `MultivariateNormal.__init__()` performs some linear algebra before calling `super(...).__init__()`. I think the solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.",The solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.
Install only caffe2,it is no longer possible to only install CAFFE2.,It is no longer possible to only install CAFFE2.
Illegal instruction (core dumped) on Debug CPU build,Could you try `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code`?,Setting `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code` will fix this issue.
OMP: Warning #190 because of fork not waiting for parallel region to end,"I am also having this issue. It persists across data (MNIST, CIFAR10) and various architectures. I do not get the warning if I set `pin_memory=False`. I think it might be due to having three Dataloaders in my script like @FunkyKoki (i.e. train, test, validation) and iterating over two of them (test, val) while inside the loop of the other (train).",The warning does not appear if the argument `pin_memory=False` is given.
torch.jit.trace returns unwrapped C type,"@NeilWangziyu You could also consider trying building PyTorch from source, if nightly doesn't work for you. Saving should definitely work now. @driazati added it 2 months ago in this PR: #20386",Installing PyTorch from source fixes the error.
"I can't import PyTorch, libomp.dylib can't be loaded.",`brew install libomp` solves the problem.,"To solve the problem, run `brew install libomp`."
Slow distributed training,"After using OMP_NUM_THREADS=1, the speed is back to normal. Thanks, @VitalyFedyunin.",Setting the flag `OMP_NUM_THREADS=1` makes it faster.
Unable to compile an older version of PyTorch,Try `git submodule sync` and then `git submodule update --init`?,Run the following: `git submodule sync` and then `git submodule update --init`
torch.arange always generate constant result in tracing,"Hi @lara-hdr, \r\n\r\nDo you mind linking to where you set `traceable=true` exactly? I'm not too familiar with that part of the code base.\r\n\r\n@zou3519 do you know why this has to be implemented in the python arg parser? it's also a schematized aten op. \r\n\r\n\r\n\r\n",This issue has been fixed.
Unable to import 1.1 when installing with pip,Probably duplicate of #20030\r\ntry `brew install libomp `,"To solve the problem, run `brew install libomp`."
Is the `device=` parameter required in torch.FloatTensor and similar ones,"`torch.FloatTensor` is a legacy constructor and doesn't support all types.\r\nTo answer your question, i think `torch.cuda.FloatTensor` might be what you are looking for -- but please dont use either. Just use `torch.empty(..., device='cuda')`","Please use  `torch.empty(..., device='cuda')` instead of `torch.cuda.FloatTensor` or `torch.FloatTensor`. "
NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training,"I ran with n_procs=1 and suddenly a magic new error message appeared telling me to try adding `find_unused_parameters=True` to `DistributedDataParallel`. When I did that, it worked! Thank you for good error messages!!!",You need to set `n_proc=1` and `find_unused_parameters=True` to `DistributedDataParallel`.
[FR] make IncompatibleKeys print nicer when there is no error,@designnner This is not an error. It's just the `repr` that isn't ideal. Feel free to ignore.,This error can be ignored.
GRUcell has a wrong formula,"either is fine, because it's all about semantics of z. that is, if z is considered an update gate, z * n makes sense, while (1-z)* n makes sense if we think of z as a leaky coefficient. in the end, they are equivalent.",The formula is correct.
TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above,I think it's a version problem.\r\njust run this:\r\n`pip install tensorboard==1.14.0`\r\n(not pip install tensorboard==1.14),You need to install the latest version of Tensorboard.
Building from source error: command 'gcc' failed with exit status 1,I have the same problem (`error: expected ')' before 'PRId64'` etc.) with gcc 4.8.5 on Linux with the current head ( 02450fff3)\r\n\r\nWhat works for me is to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\r\n* torch/csrc/Storage.cpp\r\n* torch/csrc/Tensor.cpp\r\n* torch/csrc/cuda/Storage.cpp\r\n* torch/csrc/cuda/Tensor.cpp\r\n\r\n(see also https://github.com/pytorch/pytorch/compare/master...andreh7:2017-11-10-prid64-fix -- this can be turned into a pull request very easily)\r\n\r\nThis fix is the same as #3574 but for different files. \r\n\r\n ,You need to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\r\n* torch/csrc/Storage.cpp\r\n* torch/csrc/Tensor.cpp\r\n* torch/csrc/cuda/Storage.cpp\r\n* torch/csrc/cuda/Tensor.cpp
RuntimeError when using DistributedDataParallel,"I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with ``CUDA_VISIBLE_DEVICES=1`` on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\r\n\r\nI solved the problem by deleting a ``nn.Linear`` that I assigned to an attribute during the ``__init__`` of a custom ``nn.Module`` but never used during ``forward``.\r\n\r\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\r\n\r\nThanks!",The problem can be solved by deleting a ``nn.Linear`` that is assigned to an attribute during the ``__init__`` of a custom ``nn.Module``
GPU Memory Leak at Master Branch,"Closing since this issue is stale, please reopen with a repro script if you still see the memory leak.",This issue has been fixed.
Support view() on batch dimensions for non-contiguous tensors?,"There's a good reason for the `view` invariant - most of such reshapes are impossible to pull off using stride tricks if then tensor isn't contiguous. On the other hand, making it contiguous inside `view` would mean that sometimes the returned tensor shares storage with input, and sometimes doesn't. This is important for cases like these: `x.view(-1)[::x.size(1) + 1] += c` (add `c` to diagonal of matrix `x`). If you know/suspect that tensors might sometimes be non-contiguous just add `.contiguous()` before `.view()` it's a no-op if the tensor already is contiguous",This feature will not be implemented.
Slight memory leak for LSTM,"Thanks @ngimel !

I'll close this issue for now. If you see this issue, please upgrade to cudnn 7.1+, driver 384.69+.

@Evpok @jiesutd @bangbangjim See above.",This issue has been fixed.
Error in nll_loss - multi-target not supported,"CrossEntropyLoss takes a 1D tensor. If your `target` has size `(32, 1)`, you need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.",You need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.
softmax doesn't support negative dimensions,if it didn't support negative dims in 0.2.0 then i wont mark it as a release blocker for 0.3,This feature will not be implemented.
torch.load() requires model module in the same folder,PyTorch internally uses pickle and it's a limitation of pickle. You can try meddling with `sys.path` to include the directory where `module.py` is. This is exactly why we recommend saving only the state dicts and not whole model objects.,You need to add the directory with your module to `sys.path`.
Pytorch AssertionError: Torch not compiled with CUDA enabled,"you are running this on OSX. The OSX binary of PyTorch does not come with GPU support.\r\n\r\nThe code you linked to still has some GPU stuff remaining (as you see from the stack-trace.\r\n\r\nChange these two lines to get_iterator:\r\nhttps://github.com/eladhoffer/captionGen/blob/48552694775ef11f5fa68c100584f40d98e4b690/main.py#L96\r\n\r\n`get_iterator(..., pin_memory=False)`\r\n",PyTorch does not support GPU on OSX
Variable methods which need to change before we combine Variable and Tensor,"We have additional work to do before we can combined Variable and Tensor, but all these methods are implemented.",These methods are already implemented.
"using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__","This confirms my worst fears, python allows for pathologic iterables that have a length but never end... The fact that the `reversed` trick works is even weirder from a logical point of view: Reversing an infinite iterable results in a finite iterable... :woozy_face:\r\n\r\nAfter digging a bit through the [python reference](https://docs.python.org/3/reference/datamodel.html#object.__getitem__), the intended way of handling this is by raising an `IndexError` from `__getitem__` for invalid indices. Upon encountering this, the for loop will stop automagically. Changing my above example like below makes the iteration work as expected\r\n\r\n```python\r\nfrom torch.utils.data import Dataset\r\n\r\nclass TestDataset(Dataset):\r\n def __init__(self):\r\n super().__init__()\r\n\r\n def __getitem__(self, i):\r\n if i < 0 or i >= len(self): # These two lines\r\n raise IndexError() # are new\r\n return 0\r\n\r\n def __len__(self):\r\n return 10\r\n\r\n\r\ndataset = TestDataset()\r\nfor i, data in enumerate(dataset):\r\n print(i)\r\n assert i < len(dataset)\r\n```\r\n\r\nI believe this issue can be closed, as it's not pytorch specific. (Of course, @minghuisvn you're free to object, as I kind of highjacked your issue thread :wink:)","This issue is not related to PyTorch, but to Python."
Windows source build fails with 'error LNK2019' at linking stage,"I found what causing the problem, and wanted to inform you since it might be useful. `BUILD_TEST=0` variable is causing this problem at linking stage. If that not being set it builds successfully.",Unset the flag `BUILD_TEST=0` to fix the error.
Too few arguments to vulkanOptimizeForMobile(),The issue was fixed in https://github.com/pytorch/pytorch/pull/45052,This issue has been fixed.
CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0,"Can you share a few more details how you've built gcc-5.4.0?\r\nBy default, GCC is coupled with a version of `libstdc++`, but there is a way to configure the build to use version already available in the system path. \r\nOtherwise, it sort of expected behaviour: shared library/executable compiled against newer version of libstdc++ is not compatible with an old one.\r\n\r\nPyTorch binary is build using `devtoolset-7`, which can be installed using something like the following \r\nhttps://github.com/pytorch/builder/blob/589a615fc8a8ee24690a1037ba583d32f22bc3a3/manywheel/Dockerfile#L12-L14\r\n\r\nFollowing article describes process of installing newer toolchain on CentOS in a bit more detail: https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/",Building PyTorch binary using `devtoolset-7` can fix this issue.
nn.Module.script,PyText is replacing `ScriptVocab` with TorchText's `Vocab`. However we have to add this `to_ivalue` to our transforms in order to scriptify them because of this issue. It would be great if users like us don't need to add this additional function to all layers to keep our interface clean. Thanks! CC @hudeven,Please use  TorchText's `Vocab` instead of `ScriptVocab`.
Add RMSE loss function,"Actually, there is probably no advantage in having RMSE as a loss function. It's more computationally expensive than MSE and I don't see how it can be helpful to compensate for that. What I actually wanted is ready-made RMSE metric, not a loss function. Not sure if having a library of metrics is in scope for pytorch, so feel free to close this if the issue is not useful.",This feature will not be implemented.
Is there a typo in ReLU6 declaration ?,nope if you look at the documentation of `relu6` and `hardtanh` it's reasonable to do subclass. Closing since it's resolved.,"No, there isn't."
Ellipsis support for view and reshape functions,"I'm afraid this could be an ill-defined operation in general (specially if mixed with `-1` reshaping).\r\n\r\nFor the most common cases of flattening only a subset of dimensions, we have `tensor.flatten(start, end)`, which IMO is a good compromise in veneral.",This feature will not be implemented.
Static Quantized model accuracy varies greatly with Calibration data,"Using numeric suite I was able to debug and fix the issue.
Appreciate all your help and time.",This issue can be fixed using numeric suite.
test_nn.py returns inconsistent result in different test setup,"Here's a reliable reproduction\r\n```\r\nimport torch\r\n\r\ninput_channels = 3\r\noutput_channels = 3\r\nbatch_size = 2\r\ndepth=3\r\nheight = 5\r\nwidth = 5\r\nkernel = 1\r\nstride = 1\r\nwith torch.backends.mkldnn.flags(enabled=False):\r\n conv_op = torch.nn.Conv3d(\r\n input_channels,\r\n output_channels,\r\n kernel,\r\n bias=False, # No bias\r\n ).to(dtype=torch.double)\r\n input = torch.randn(batch_size, input_channels, depth, height, width, dtype=torch.double, requires_grad=True)\r\n out = conv_op(input)\r\n gO = torch.rand_like(out)\r\n out.backward(gO)\r\n print(conv_op.weight.grad)\r\n```\r\nThe issue is on cpu, not on cuda.",This issue does not arise while using GPU.
Nightly builds for cp38 missing for non-windows targets,"Looks like we neglected to add them to `master`, submitted #34732 to remedy that",This issue has been fixed.
Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor,"This is expected, as it's a float32 limitation.\r\nFrom the [Wikipedia article](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\r\n> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between −16777216 and 0)\r\nIntegers between `2**24=16777216` and `2**25=33554432` round to a multiple of 2 (even number)\r\nIntegers between `2**25` and `2**26` round to a multiple of 4\r\n...",This is an expected behavior.
Building wheel for torch (setup.py) ... error - While running pip install torch,"You could not do `pip install torch` because the Windows packages are not hosted on PyPI. Instead, please enter the commands in https://pytorch.org.",Install using instructions from PyTorch's website.
load_state_dict_from_url error with weights downloaded from Google Drive ?,"That URL is not a PyTorch file (.pth). It's a webpage with a link to a PyTorch file.\r\n\r\nYou need a direct download URL. Google drive doesn't support that directly, but you can use something like https://sites.google.com/site/gdocs2direct/","Please use a direct download URL, something like `https://sites.google.com/site/gdocs2direct/`"
Support arbitrary types in jit,"We have support for classes in TorchScript, is there something in particular that's missing that you're looking for? NamedTuples are also supported. We also have other things like enum.Enum and @dataclass on our roadmap but no concrete timeline yet. We're also introducing a way to bind C++ classes into TorchScript in our next release.",These methods are already implemented.
[C++ API] Support for CIFAR10 and CIFAR100 Datasets,"Not yet, will try to add one by this weekend as an example",They will be added soon
Cannot use setup.py install,> ModuleNotFoundError: No module named 'past'\r\n\r\nModule `past` is part of `python3-future` package on Ubuntu (or just `pip3 install future`),You need to install the package 'future'
torch.cat is moving Tensors across devices silently,"One case (hopefully supported in future) where moving between devices makes sense:
`x = torch.cat([torch.rand(3, device = 'cuda'), 1])` for appending python scalars to a tensor",This is an expected behavior.
Compilation error on aarch64,"I ran into the same issue on aarch64. I am able to work around it by downloading sleef 3.5.1, install it to system and set the USE_SYSTEM_SLEEF=ON flag.

Hope it helps",You need to set the flag `USE_SYSTEM_SLEEF=ON`.
[feature request] Allow `torch.unsqueeze` to insert multiple new dims,"@tshadley \r\n\r\n> ```\r\n> import torch\r\n> t_b = t[...,(None,)*3]\r\n> print(t_b.shape)\r\n> ```\r\n\r\nTo unsqueeze at the end, you could try to use `t_b = t[(..., ) + (None, ) * 3]` (works for me on version 1.1.0).","You can use `t_b = t[(..., ) + (None, ) * 3]`"
[feature request] add `torch.find` to find the indices of values,"A not very optimized version of this function can be obtained with a one-liner I believe (for 1d `values`)\r\n```python\r\ndef find(tensor, values):\r\n return torch.nonzero(tensor[..., None] == values)\r\n```","You can use \r\n```python\r\ndef find(tensor, values):\r\n return torch.nonzero(tensor[..., None] == values)\r\n```"
Segmentation Fault using dist.broadcast() with openmpi,"Actually I tested a simple cuda program doing MPI_Bcast/MPI_allreduce and confirmed that it also segfaults there with openmpi 1.10, while it works fine on openmpi 2.1+. I'm more inclined to a openmpi issue in this case. I will make a PR to add this warning to the doc I think.",Installing the latest version of openmpi fixes this issue.
.topk() returns incorrect values + indeces on non-contiguous tensors (CUDA),The reason this fails is `topk` needs contiguous inputs and the outputs of Beta.sample aren't.\r\nCan you retitle the bug report to that?\r\nI'll submit a PR to check contiguous in topk.,You need to pass contiguous inputs to `topk`
OOM when using Adam optimizer compared to SGD when using same batch size.,"Adam is more stateful than SGD, so it is expected that it uses more memory (proportional to the total size of the optimized parameters).","Adam is more stateful than SGD, so it is expected that it uses more memory."
log_prob returns positive values for small cov,"The values of the `pdf` can be arbitrarily large but never negative. This is completely acceptable, and I am sure this is not a bug.\r\n\r\nSimple example: normal distribution with standard deviation = 0.000001 and mean = 0. The `pdf` at `x = 0` is `1000 / (2 * pi)`, and the natural log of this is clearly greater than 0.",The values of the `pdf` can be arbitrarily large but never negative. 
"\""ImportError: No module named tools.setup_helpers.env\"" when \""python setup.py egg_info\""","Does it work if you do ""FULL_CAFFE2=1 python setup.py install"" instead?",Please install using the flag `FULL_CAFFE2=1`
[bug] Multiplication of tensor with numpy scalar does not always work,"The problem is that the left operand is the default operand to execute the `__mul__`.\r\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\r\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.\r\nStandard numpy scalars have a strongly negative priority, standard ndarrays have one of 0.\r\nSo in case 2:\r\n```\r\ntensor.__array_priority__ = 1000\r\nprint (scalar * tensor)\r\n```\r\nworks!\r\n\r\nI'm happy to send a PR.\r\n","The problem is that the left operand is the default operand to execute the `__mul__`.\r\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\r\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead."
git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.,"Sorry, I did not realize you were trying to install detectron. It looks like detectron expects to find Caffe2 through find_package, which right now requires Caffe2 to be built from source. What's causing that error message is that a source build (through Caffe2's cmake) populates a ""Caffe2 target"" that tells other cmake projects (Detectron) where to find Caffe2. These extra cmake files aren't included in the pre-built packages because you shouldn't need cmake to install and use Caffe2.

When installing from source, do still read through https://caffe2.ai/docs/faq.html#why-do-i-get-import-errors-in-python-when-i-try-to-use-caffe2 and follow it's recommendations; it can save you from a lot of frustrating errors.",Installing Caffe2 from source fixes the error.
[jit] support at::optional,cc @wanchaol \r\n\r\n@SsnL the nan is a temporary stopgap. I believe the end state is indeed to support `at::optional` or something similar.,This is an expected behavior.
[distributions] dirichlet pathwise gradient does not work well with .expand,I think this should be closed now.,This issue has been fixed.
[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`,"After a lot of discussion, we decided not to move on with this proposal.",This feature will not be implemented.
"RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True","maybe \""sudo python\"" can solve it",Use `sudo python` to fix this.
Add python 3.7 to binary install page,"python 3.7 binaries are live on PyPI, conda and on https://pytorch.org","python 3.7 binaries are live on PyPI, conda and on https://pytorch.org"
Caffe2 Train your own image,"@aswin1980 Check out the \""CIFAR10_Part1\"" and \""CIFAR10_Part2\"" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. Part 1 specifically shows how to take a custom image dataset (in this case .png mirror of the CIFAR-10 dataset), format it, create image LMDBs, and train a model on them.","Please refer to the \""CIFAR10_Part1\"" and \""CIFAR10_Part2\"" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. "
Conda Install: PackageNotFoundError,@pjh5 \r\n\r\nUpdating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda` fixed it! I'm able to properly install pytorch now by running `conda install pytorch torchvision -c pytorch`. \r\n\r\nThanks for the help.,This can be fixed by updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda`.
"Always get error \""ConnectionResetError: [Errno 104] Connection reset by peer\""","for me, I found set thread number equal 0 will solve this problem, namely:\r\nnum_workers=0\r\n",Setting `num_workers=0` will fix this problem
[Bug] Segmentation fault when importing fastText (with v0.4.0),"For the record, the problem was:\r\n - in a conda environment, I installed pytorch with `conda install`(as described on pytorch web site) and fastText with `pip install .` from their git clone.\r\n - that resulted in a segfault when doing `import fastText` and `import torch` \r\n\r\nReason:\r\n - pytorch is compiled with gcc 4.9.2\r\n - conda's default gcc is 4.8.5\r\n\r\nFix:\r\n - install gcc-4.9 in conda (e.g. `conda install -c serge-sans-paille gcc_49`)\r\n - install pytorch with `conda install` (in my case, `conda install pytorch torchvision cuda90 -c pytorch`)\r\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone\r\n\r\nThat's it! Thanks a lot @weiyangfb and @SsnL for your help!",To fix:\r\n - install gcc-4.9 in conda\r\n - install pytorch with `conda install` r\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone
How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source,@Tianji95 this problem is outdated. The FULL_CAFFE2 flag no longer exists and is no longer needed.,This issue has been fixed.
"[feature request] torch.isinf, torch.isfinite","`torch.isinf` merged in, `torch.isfinite` not implemented yet",`torch.isinf` has been merged; `torch.isfinite` has not been implemented yet
cannot reload on CPU model saved on GPU,"When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`. If you don't, since you are loading GPU tensors, PyTorch will try to reconstruct GPU tensors, and fail.","When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`."
setting CUDA_VISIBLE_DEVICES just has no effect,You need to do that before import pytorch.,You need to do that before import pytorch.
[feature request] nn.Identity,You can use nn.Sequential() to simulate identity,You can use nn.Sequential() to simulate identity
Big drop in performance for larger batch size for otherwise same training script,"You should tune different hyparams (e.g., lr) to accommodate different batch size. Some say that larger batch size requires larger lr too.","You should tune different hyparams (e.g., lr) to accommodate different batch size. Larger batch size requires larger lr too."
torch.save() and nn.DataParallel(),I usually do:\r\n```\r\ntry:\r\n state_dict = model.module.state_dict()\r\nexcept AttributeError:\r\n state_dict = model.state_dict()\r\n```,Use the following :\r\n```\r\ntry:\r\n state_dict = model.module.state_dict()\r\nexcept AttributeError:\r\n state_dict = model.state_dict()\r\n```
torch.Tensor.__repr__ is slow,This has been fixed in master. Here are my timings: ...,This issue has been fixed.
"[feature request] Convert \""indices\"" variable in \""torch.utils.data.dataset.random_split\"" to list",I think we can just add a .tolist() after the code in here. Could you send a PR?,You can add a `.tolist()` after the code.
problem building with ROCm,"There's a \""hipify\"" step to replace all the CUDA references with HIP/ROCm references in-place: \""python tools/amd_build/build_pytorch_amd.py\"". Also, don't forget to set env var USE_ROCM to 1. After that, you should be able to build using the normal \""python setup.py\"" step.",Please set the environment variable `USE_ROCM=1`
RNN weights are not Xavier-initialized,"Also, adding to @Kaixhin 's point, if you really want Xavier initialization, you could try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.",You cant try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.
"Sequential does not allow muti-output modules such as RNN, LSTM","As I said in forum, this is not a bug.",This is an expected behavior.
Weights won't update during backpropogation,"your learning rate simply has to be increased, and as Issam pointed out SGD has quite a bit of variance in the gradients.",Your learning rate has to be increased.
"error: identifier \""__half_as_ushort\"" is undefined","It tends out to be my own installation problem. cuda_fp16.h is not inside /usr/local/cuda/include but I've found it in /usr/local/cuda/targets/x86_64-linux/include. Maybe because I used to have an older version of cuda installed.\r\n\r\nAnyway, I completely removed cuda and reinstalled it. All the header files are now in place and built with no problem.",This error can be resolved by reinstalling CUDA.
"When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.","I meet the same error on pytorch 1.0.0.dev20181115 with inner ctc loss, but did not encounter this situation at pytorch 0.4 with warpctc loss",Install the latest version of PyTorch to fix this.
Memory leak from Function.save_for_backward() when looping over batch,the `@staticmethod` stuff is the right / official way. We'll change the tutorials right away.,Use static methods to fix this issue.
"Segfault in neg, introduced in 3e6e81d",Fixed in #3433,This issue has been fixed.
`torch.utils.data.DataLoader`,this is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe: \r\nhttps://github.com/pandas-dev/pandas/issues/12236\r\nhttps://github.com/pandas-dev/pandas/issues/14692\r\n\r\nTo actually allow concurrent reads for a file you have to use SWMR feature of HDF5: https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html\r\n\r\n\r\n,This is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe. To actually allow concurrent reads for a file you have to use SWMR feature of HDF5.
inconsistent behavior of max,this was fixed in master. will be part of the next release.,This issue has been fixed.
Kaiming/Xavier initializer cannot deal with bias term,"Correct me if I am wrong. I think those methods are not supposed to deal with bias terms. In the paper, bias terms are initialized to zeros, which you can achieve by `layer.bias.data.zero_()`",This can be achieved by `layer.bias.data.zero_()`
Implementation Discussion: Native CTC,"You would have to use 1.0rc, I wasn't quite finished in time for 0.4.1.",This feature has been implemented.
Can I plz has determinant function?,fixed on master via #3816,This feature has been implemented.
Recent bug in torch.cat() on Variables?,"Great, this is also fixed on the `v0.3.0` branch as of 9a67882",This issue has been fixed.
cuda out of memory error when GPU0 memory is fully utilized,@TomHeaven did you set CUDA_VISIBLE_DEVICES outside the python process? if that's the case pytorch should not even have driver-level access to your GPU0.\nIdeally: `CUDA_VISIBLE_DEVICES=1 python foo.py`,You need to set `CUDA_VISIBLE_DEVICES` outside the python process
RuntimeError: CUDA error (3): initialization error,"as mentioned in http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\r\n\r\ninsert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(\""spawn\"")\r\n```","Insert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(\""spawn\"")\r\n```"
RuntimeError: context has already been set(multiprocessing),"Hi @pancho111203 ,\r\n\r\nYou might have other files in your project which also have a `if __name__ == '__main__':`. One workaround is to call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`. This solved the issue for me.\r\n\r\nRegards\r\nNabarun","You can call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`."
Gradient Ascent Cross Entropy Loss,"@cdjhz `(-loss).backward(); optimizer.step()`
",Use this: `(-loss).backward(); optimizer.step()`
ParameterList and ModuleList with named modules or parameters,fixed via #3505,This issue has been fixed.
PyTorch Implementation of Michael Jordan’s lab's Perturbed SGD?,"I think adding noise to gradients is simple. A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.\r\n\r\nI initially proposed adding Noisy SGD, but the proposal was rejected considering its triviality. Ref: https://github.com/pytorch/pytorch/pull/4332","A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called."
NameError: name 'logging' is not defined,"I think this has been fixed on master, because the point of error is non-existent on master.",This issue has been fixed.
Builing for a specific SM number,"Yes. By example:\r\n```\r\nTORCH_CUDA_ARCH_LIST=\""5.2;6.1;7.0\"" python setup.py install\r\n```","You can do this via:\r\n```\r\nTORCH_CUDA_ARCH_LIST=\""5.2;6.1;7.0\"" python setup.py install\r\n```"
[jit] torch.empty_like is different from eager mode,"I think I can fix this - it's probably not an issue with `empty_like` in particular, but with all optional arguments. https://github.com/pytorch/pytorch/pull/22055 fixes one case of this.","This is not an issue with `empty_like` in particular, but with all optional arguments."
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0,"Try to use this command from the [website](https://pytorch.org/get-started/locally/):\r\n```\r\npip3 install torch torchvision\r\n```\r\n\r\nThis should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\r\nJust select whatever config matches your current setup and use the shown install command.",Use this command:\r\n```\r\npip3 install torch torchvision\r\n```
CI failure points to nonexistent code...,fixed via #23304 by @zou3519 :D,This issue has been fixed.
torch.onnx._export does not support tensor sum with multiple dims,"@yil8 - yes, you can install the nightly build of PyTorch to test this.",Install the latest version of PyTorch to fix this.
issue with ONNX and PyTorch,"@arijit17 - It is hard to say where exactly the issue is without looking at your model and export code. Could you please share the repro? \r\nFYI - if your model has input-dependent control flow, then ideally you will have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation. But once you do that the export process itself should not change.\r\nIf you can share a repro, we might be able to help better.",You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation.
Wrong distribution sampled by torch.multinomial on CUDA,Closed by #22183,This issue has been fixed.
Inplace error if DistributedDataParallel module that contains a buffer is called twice,"The cause here is that by default DDP modules broadcast the contents of the root process module's buffers to all processes at every forward pass, and this broadcasting counts as an inplace operation for the purposes versioning. The fix is to disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor. Thanks @pietern for the help.","To fix this , disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor."
LR scheduler design bug !,It's in master: you need to compile from source for now.,This issue has been fixed.
Segmentation fault Autograd,"@shoukang by any chance you can try installing new Pytorch, we have done a lot in terms of threading recently. And your problem might be related.",This issue has been fixed.
Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?,"Actually, in the current code, sigmoid is also deprecated in nn https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L1390. But not relu. Are you now saying that sigmoid is a general purpose mathematical function but relu isn't ?","In the current code, sigmoid is also deprecated in nn, but not relu."
Building from source failed. Multiple errors in the printout,"You can find good answer here: https://github.com/torch/torch7/issues/1190#issuecomment-498934400 (credits to @talkenig)\r\n\r\nCopy pasting for future reference:\r\n\r\nHad the same problem and got to the bottom of it. My configuration is a Tesla T4 with 410.92 driver and CUDA 9.2, Ubuntu 18.10.\r\nThe thing is, the torch installer tries for some reason to use the highest compute capability supported by the device (or the driver - not sure which), but ignores the compute capability supported by the CUDA toolkit.\r\nSo, in my case the device supports compute capability 7.5, but CUDA 9.2 supports only 7.0 or 7.2 (not sure which one). You can guess I got the same nvcc fatal : Unsupported gpu architecture 'compute_75'\r\nThe solution is to force the nvcc compile options to use a lower compute capability. This can be achieved by setting the following environment variable:\r\n\r\n`export TORCH_CUDA_ARCH_LIST=\""7.0\""`\r\n\r\nJust before running ./install.sh from the torch directory.\r\nNote that the list may contain more than one compute capability, e.g. it can be \""6.0 6.2 7.0 7.2\"". This will be reflected in the CUDA_NVCC_FLAGS -gencode arch=compute_70,code=sm_70 and those will be outputted to the terminal during the build.\r\n","To fix this, export the following environment variable:\r\n\r\n`export TORCH_CUDA_ARCH_LIST=\""7.0\""`"
[dataloader] Add a context= argument for multiprocessing,"This is expected, because thed spawned workers does not see the dataset def.\n I think the proper way to solve this is to add a `context=` argument to data loader, so that a global start method needs not be set.","This is expected, because thed spawned workers does not see the dataset def.\n The proper way to solve this is to add a `context=` argument to data loader."
"Torch crashes when calling torch.rand(2,3)",your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. We do not support processors that dont have these features in binaries.\r\nYour only choice is to install from source: https://github.com/pytorch/pytorch#from-source,Your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. You have to install from source.
No method to set the timeout for distributed Gloo backend,"@ejoebstl this would helps a lot, i used to solve this problem by changing the default timeout and recompile the whole pytorch o(╥﹏╥)o",This can be fixed by changing the default timeout and recompiling PyTorch
[PyTorch] Build error (NCCL) on Ubuntu 16.04,"A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/nccl.py#L74\r\n\r\nAfter modifying this, I can compile PyTorch without error. \r\n",A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. 
"Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue","""It looks like you installed with setup_caffe2.py, as setuptools (the Python package behind setup.py scripts) is what installs .egg files.\r\n\r\nCould you try `pip uninstall caffe2` and `conda install -c caffe2 caffe2-cuda9.0-cudnn7` ? This way should be much faster too.\r\n\r\nIf the pip uninstall caffe2 doesn't seem to work, then you can manually uninstall by deleting every file and folder under /home/sam/anaconda3/envs/caffe36/ that has 'caffe' or 'caffe2' in the name.",You need to uninstall and reinstall Caffe2.
"BatchNorm2d when batch size 1 works, what is it doing?",That is normalizing `[B x C x *]` over the dimensions `[*]`,It is normalizing `[B x C x *]` over the dimensions `[*]`
can't rebuild with NO_CUDA=1 after clean,Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\r\n`\r\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\r\n`,Please run :\r\n`\r\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\r\n`
Where is the Caffe2 website?,https://github.com/caffe2/caffe2.github.io,It is at `https://github.com/caffe2/caffe2.github.io`.
Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed,"I had the same problem and renamed `THGeneral.h` from `conda/envs/<ENV_NAME>/` to something similar to what @rgreenblatt  said, and it was installed normally. Good luck to friends who are having the same problem.\r\n\r\n```\r\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\r\n```",Please run \r\n```\r\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\r\n```
Add `torch.pi` like `numpy.pi` and possibly other constants,Aren't all of those constants in math?,They exist in `math`
"torch.irfft produces \""cuFFT error: CUFFT_ALLOC_FAILED\"" when called after torch.rfft","Me either, it seems to work now. Maybe you fixed something along the way :) Maybe this can be closed, having fewer temp tensors along the way also helps. Might be even better if complex product gets implemented some day.\nIt would be nice if the OOM exception was some standard PyTorch exception, not a CUDA one.",Having fewer temp tensors along the way can help.
[feature request] Complex multiplication,One verbose way may be a kwarg `complex = True` to `torch.mul`. Some alternative ideas: `mul_complex` / `mulc`,Add a kwarg `complex = True` argument to `torch.mul`.
cuda runtime error (48): no kernel image is available for execution on the device,"Hi, your problem is stated in this warning here\r\n```\r\n/home/azat/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py:97: UserWarning:\r\n Found GPU0 GeForce 820M which is of cuda capability 2.1.\r\n PyTorch no longer supports this GPU because it is too old.\r\n```\r\n\r\nYou can build from source to use some functionality, but there are many operations that require compute capability greater than SM_21 to perform.",You can build from source to use some functionality. Some operations require a more recent GPU.
Stop using undefined tensors to represent zero gradients in engine,going to close this because I believe the issues with undefined tensors in the engine have been addressed.,This issue has been fixed.
Proposal: rename upsample to resample,This function has been deprecated in favor of interpolate so I think this issue has been addressed.,Use the `interpolate` function instead.
RuntimeError for indexing with high dimensional tensor only when using cuda,"As for pytorch 0.4 (nightly build 2018.04.20, installed via conda) this problem does not exist anymore. And the following code works:\r\n```python\r\nimport torch\r\nx = torch.randn(2, 2, 2, 2, 2, 2).cuda()\r\ni = torch.cuda.LongTensor([0, 0, 0, 0, 0, 0])\r\nx[i,:]\r\n```\r\nClosing the issue.",This issue has been fixed.
RuntimeError: reduce failed to synchronize: unspecified launch failure,"I was having similar error. Make sure your layer has values that make sense to the BCELoss. If you for example output negative values and pass them to the logarithm, the training will fail.\r\n\r\nIn my case I was missing the last sigmoid activation to shrink the numbers between 0 and 1.",Make sure your layer has values that make sense to the BCELoss.
Zombie process when use GPU,"For anyone who still suffer from this issue, try the following command:\r\n`fuser -k /dev/nvidia*`\r\nor\r\n`kill $(lsof -t /dev/nvidia*)`",Use the following command:\r\n`fuser -k /dev/nvidia*`\r\nor\r\n`kill $(lsof -t /dev/nvidia*)`
torch.HalfTensor' object has no attribute 'mean',"we don't have math for CPU Half type (it would be very slow), convert it to cuda or CPU Float.",Please convert tensor to cuda or CPU Float.
import torch; libcublas.so.9.0 error,"I had the exact same linking problem when trying to compile pytorch 0.3 with CUDA 9.1. I couldn't figure out how it manages to find and link to cublas.8. I gave up and installed the `pip` wheel package which contains CUDA, CuDNN everything inside.",Installing from pip fixes this error.