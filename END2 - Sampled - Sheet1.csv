x,z
Support multiple simultaneous LR schedulers,"A solution was implemented in #26423. Closing this PR, please feel free to re-open if needed."
Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/,"We'll supply PyTorch binaries w/ CUDA 10 in the next release. For now you can build from source, as mentioned by @jendrikjoe"
Crash when reading pandas parquet file after importing pyTorch,uninstall the pyarrow installed by pip and then reinstall with conda works for me.
"RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict","Solved.Just change the output of your model from dict to list
"
dataparallel not working on nvidia gpus and amd cpus,"I believe this is a duplicate of #1637. Specifically, see [this comment](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158). I've had success on a threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'."
[perf] Reduce tensor & aten overhead,9 days later the timings for JIT lstms as well as aten lstms [look better now](https://gist.github.com/zou3519/83a2169731b3f1ec4b9bd9d259b010b2) and the difference is stable. Good to see that we're moving in the right direction
Matrix multiplication operator,this is now fixed in master\n
PEP8,"Can we get rid of \""import *\"" statements as well? Not PEP8 but lots of people seem annoyed at it.\n\nhttp://stackoverflow.com/questions/2386714/why-is-import-bad\n"
Remove dampening from SGD,"Made default to 0.
fixed via 4eb12a2"
ImportError: No module named _C,@szagoruyko Could you please try opening torch in any directory other than repo's root? It's trying to load the `torch` dir instead of the python package and gives you this error.\n
"Install Error, OSX 10.11.6, fresh miniconda install",i think that's because you have CC and CXX set\n
MaxUnpool2d segfaults for some configurations,"Probably fixed in #207. I can't reproduce it anymore, so I'm closing the issue.\n"
"Add an keyword argument \""device\"" to torch.cuda.XXXTensor","I think `y = x.type('torch.cuda.FloatTensor', device=3)` doesn't make sense, since the type conversions are now separate from device changes. But I agree that we should accept a device argument inside a tensor constructor ðŸ‘ We can implement this in Python.\n"
[legacy-nn] losses need to return tensors and not numbers,i dont think we should fix legacy-nn
Optim API: per-layer learning rates etc.,"this is easily solved by gradient rescale function, specifying per-layer learning rate is actually more pain\n"
Containers should allow module assignments,"We may want to prefer this syntax (in examples and docs) instead of the current keyword arguments in the constructor. \n\nFor example:\n\n``` python\nclass MyNetwork(nn.Container):\n def __init__(self):\n super(nn.Container, self).__init__()\n self.l1 = nn.Linear(5, 10)\n self.l2 = nn.Linear(10, 20)\n```\n\ninstead of:\n\n``` python\nclass MyNetwork(nn.Container):\n def __init__(self):\n super(nn.Container, self).__init__(\n l1=nn.Linear(5, 10),\n l2=nn.Linear(10, 20),\n )\n```\n\nOne advantage is that the order of module iteration would match the order of assignment, which is not true for the constructor syntax (except in the upcoming Python 3.6)\n"
Add logical AND/OR/NOT/XOR operations,Implemented in #342.
Printing tensors is sometimes very slow,"I've reproduced this, looking into fixing it.\n"
numpy.__config__.show() for torch,This has been included in 29ea086 by @ezyang.
Deterministic cudnn algorithms,Parallel data loader is now deterministic.\n
torch dot function consistent with numpy,"This has been fixed via #1563 , if we consider that we should follow the behavior of numpy.matmul instead of numpy.dot, according to the thread pointed out by @ngimel."
automatically assign attributes that are variable as parameters?,"@glample There's a very simple reason why we don't want people to save Variables as attributes - if they were created as a result of some computation, they will be kept around and they won't free the graph nodes preceding it.\n\nWe wanted to think of Modules as thin wrappers around functions that only hold some persistent state like parameters, never any temporary objects (this is handy e.g. for serialization - there's no need for `clearState()`, etc.).\n\nI agree it's quite a nice patter, but I'm not sure if we should allow this. @colesbury, any thoughts?\n"
embeddings layer with IntTensor / cuda.IntTensor inputs,This was added in #46758 and is now working just fine on master for dtype=torch.int.
requirements.txt: cffi >= v1.4.0,Fixed in #161.
More optimizers in torch.optim,"Yes, the we only have tests for legacy optim at the moment, and we recommend against using it (it's only for legacy.nn). Thanks for pointing out the naming, I'll change it. #5 is also incorrect, we probably checked it because we implemented trainers and datasets.\n\nTo train `torch.nn` networks you should use `torch.optim`. We're aware most algorithms are missing and we'll be implementing them sometime soon. Sorry for the delays.\n"
"error: â€˜float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)â€™ is deprecated caused by outdated MKL","This is oneapi-src/oneDNN#440. According to the MKL-DNN developers, this can occur if you're compiling against an outdated MKL library. Try updating your MKL."
Check that outputs and grad_outputs have same shape in torch.autograd.grad(),"This looks like a good idea, as it will help narrow down the problem in the case of unmatched shapes. The documentation for _torch.autograd.grad_ says, \r\n\r\n> grad_outputs should be a sequence of length matching output containing the pre-computed gradients w.r.t. each of the outputs.\r\n\r\nSo, you're right, they should be the same shape (Except when grad_outputs = None). I'd be happy to work on implementing this feature."
How can i convert â€˜at::Tensorâ€™ to â€˜const float*â€™ in libtorch?,`.data<float>()`
Bug in CosineAnnealingLR (division by zero),I've confirmed that reverting #14010 fixes the problem. I'm going to revert it for now and then investigate more.
Incorrect behaviour of min() and argmin(),"@Markus-Goetz repasting my comment from #17738 (comment)

the only determinism we aim to have is hashed on device, and for CPU, single-threaded.
Even across different kinds of GPUs, all bets are off.

We cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.

For example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.

This is inconsistent with numpy's, Eigen's, C++ STL etc.

If you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression."
Allow ONNX export of Softmax with dim != -1 (including Softmax2d),This is very useful for sequence models in the BCT format. Currently export of log_softmax with dim=1 fails.
[autodiff] Fix owning model used in `differentiate`,We seem to have agreed to leave this as is.
Conv3d fail after curtain batch size,I can confirm this is now resolved!
[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57,"@YashSinha1996\r\n\r\nI also have been working on pytorch-pretrained-BERT.\r\nI think that the error reported in this issue can be avoided by using reshape instead of contiguous and view. (But I'm not very sure because I'm a novice at pytorch.)\r\n\r\nThe modification could be\r\n```\r\n context_layer = torch.matmul(attention_probs, value_layer)\r\n- context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\r\n+ context_layer = context_layer.permute(0, 2, 1, 3)\r\n new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\r\n- context_layer = context_layer.view(*new_context_layer_shape)\r\n+ context_layer = context_layer.reshape(*new_context_layer_shape)\r\n return context_layer\r\n```\r\nin pytorch_pretrained_bert/modeling.py\r\n"
librosa tests on Windows don't work,"I'm now fixing it with `conda install numba`, which will also install llvmlite."
Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators,"This looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken."
[jit] support general buffer mutation,"Okay, assigning `self.vector = u.data` works. Since `u` had a gradient, that meant that for subsequent passes `self.vector` had a gradient, which didn't work well."
libtorch C++ library does not compile properly,@JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir) fixes it for me
Excessive call to cudaGetDevice and cudaSetDevice,"@omry you don't even need to change PyTorch source code. You can just hot patch the CUDA functions to be no-ops:

https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3

You'll see that outside the profiler the cudaGetDevice and cudaSetDevice functions do not affect performance. You can also use this under nvprof to avoid API tracing those functions."
[Tutorial]: Wrong example of Our Own Ring-Allreduce,"@rohan-varma How about the following code? I think it provides the same functionality and is easier to understand.\r\n```python\r\nsend_req = dist.isend(send_buff, right)\r\ndist.recv(recv_buff, left) # recv is a blocking operation.\r\naccum[:] += recv_buff[:]\r\nsend_buff[:] = recv_buff[:]\r\n```"
the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON,"I just found out that this issue could be solved by using the `/INCLUDE` switch. torch_cuda.dll contains the function signature `?warp_size@cuda@at@@YAHXZ` (`int __cdecl at::cuda::warp_size(void)`). So adding `/INCLUDE:\""?warp_size@cuda@at@@YAHXZ\""` to the linker flags will force the library/executable to link against `torch_cuda.dll`. What do you think, @ezyang ?\r\n\r\nReference:\r\nhttps://docs.microsoft.com/en-us/cpp/build/reference/include-force-symbol-references?view=vs-2019"
PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available),"In my case, this error was caused by a corrupted saved file. So I switch to older checkpoints and the problem is gone."
nelements is not jitable,"Thanks for issue! This is more generally covered by https://github.com/pytorch/pytorch/issues/30774, so I'm going to close this one. FWIW, full list of unsupported ops is [here](https://pytorch.org/docs/master/jit_unsupported.html), and each section has an outstanding issue to cover it."
About torch.poisson,"Thank you for reporting this bug. This function is not documented, most likely it returns a value sampled according to poisson distribution with the parameter given by the input. \r\n@vishwakftw can you please submit a PR to document this function, if you know what it's doing?"
test_conv_transposed_large_cuda failed on Windows,The failure is seen in CI.
[docs] torch.onnx.export docs contains two descriptions for example_outputs arg,Fixed in #31826
The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch,"Closing it because the original issue should be resolved. If users want 32-bit quantization support, they can open a new issue."
Pip torch_nightly on macOS installs wrong build,"I believe this should now be resolved for `macOS`:\r\n\r\n```\r\npytorch37 â¯ pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\r\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\r\nCollecting torch\r\n Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.5.0.dev20200110-cp37-none-macosx_10_9_x86_64.whl (82.4MB)\r\n```\r\n\r\nNot entirely sure about where we are on windows nightly builds though, cc @peterjc123"
how to set cuda stream by call Aten function,"1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \r\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits."
`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function,"Thanks for the report, I will take a look why this happens. It might simply be that we don't restore the status in `autograd.grad` properly."
dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache,"For posterity this is what CircleCI responded with:\r\n\r\n----\r\n\r\nWe have looked into this issue and have found that this is being caused by a dependency conflict.\r\n\r\nTo install libomp requires the following dependencies (taken from the build log):\r\n\r\n==> Installing dependencies for libomp: pkg-config, gdbm, openssl@1.1, readline, sqlite, xz, python, sphinx-doc and cmake\r\nThis package is specifically installing OpenSSL 1.1 and higher. When homebrew installs OpenSSL, it will change the symlinks in \r\n/usr/local/opt/openssl/ to point to the latest version (which truly resides in \r\n/usr/local/opt/openssl@1.1). This in turns means the file \r\n/usr/local/opt/openssl/lib/libssl.1.0.0.dylib is no longer in that directory as it has been replaced by \r\nlibssl.1.1.dylib.\r\n\r\nThe previous version of OpenSSL (1.0) can be found in \r\n/usr/local/opt/openssl@1.0/ which is where the previous version of the dylib, which is trying to be used, can be found (libssl.1.0.0.dylib).\r\n\r\nYour script seems to be using something that is trying to specifically use this version of the dylib. Best practise would be to instead use \r\n/usr/local/opt/openssl/lib/libssl.dylib which will always point to the latest version of openssl as it is not requesting a specific version. You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.\r\n\r\nDo you know if any packages you use have been updated recently?"
[v1.4.0] Release Tracker,Will 1.4.0 support Python 3.8?\r\n\r\n<hr>\r\n\r\nFrom https://github.com/pytorch/pytorch/issues/21741#issuecomment-541242504 it looks like it will.
Distributed Using Gloo on Multiple Nodes Does not Work,"I think that would create problems with the ranks. For worker_id 0, the ranks would be 0 and 1 and for worker_id 1 the ranks would be 1 and 2. Can you try with worker ids 0 and 2? Your ranks should be 0, 1, 2, 3 for world size 4."
The guidelines for loading a PyTorch model in C++ do not work on Windows,You'll need to copy the PDBs along with your executable.
Meta file not found or corrupted.,"Is this what you mean:\r\n```\r\nimport torchvision.datasets as datasets\r\ninput_transform = None\r\ndatasets.imagenet.ARCHIVE_DICT['devkit']['url'] = \""https://github.com/goodclass/PythonAI/raw/master/imagenet/ILSVRC2012_devkit_t12.tar.gz\""\r\nimagenet = datasets.ImageNet(\""/dataset-imagenet-ilsvrc2012/\"", split=\""val\"", transform=input_transform, target_transform=None, download=False)\r\n```"
Fix 1.3.1 branch submodule fbjni dependency,"Now that this has stayed unchanged for longer and `v1.5.0` is out, moving the tag seems less needed and less attractive (EDIT: and it can always still be done in the future). \r\n\r\nI propose to close this - will do so next week unless someone disagrees."
Connection closed by peer when using L-BFGS and distributed computing (gloo),"Excellent!

It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary."
"`torch.Size` is tranfered to`torch.Tensor`, values don't equal",`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.
"Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?","@peterzpy see [here](https://discuss.pytorch.org/t/out-of-memory-when-i-use-torch-cuda-empty-cache/57898/3) and [here](https://github.com/pytorch/pytorch/issues/25752#issuecomment-528866347) for a solution. Basically, you need to specify the gpu device, before calling to `empty_cache`."
Ruby Library,Also added a Homebrew formula so users can now do:\r\n\r\n```sh\r\nbrew install libtorch\r\n```\r\n\r\nhttps://github.com/Homebrew/homebrew-core/pull/47222
[jit] Spurious error when type comments are found in the body of a function.,Oh sorry I see it has been fixed.
[jit] Cannot create a `Tuple[List[T]]`,"This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length. In the second example you're creating a Tuple[List[T]], which has a fixed length so we can compile it."
RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED,Closed with #31724
failed to convert torch.jit.ScriptModule to ONNX (crash),"@linronmo, as I explained above, the change for flatten could only be done for opset 11.
So you can export your model in opset_version=11, for that, you should use the parameter opset_version=11 in the exporter api with PyTorch nighly: torch.onnx.export(..., opset_version=11)."
[Bug report] RuntimeError: backward_input can only be called in training mode,"@simon555 You only set training mode before the loop, but set eval mode in generate_predictions, which is called in the middle of the loop. So iterations after generate_predictions will fail."
[Caffe2] Windows build errors in generated file caffe2.pb.h,I remember I've seen this issue before.\r\n\r\nIt's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\r\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\r\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\r\n\r\nSo...the solution is to patch protobuf...\r\n\r\nBTW this issue is on protobuf 3.5.\r\n`master` branch of protobuf should not have this issue but it has something else...
[feature request] Untied biases for nn.Conv?d,"\r\n@lzamparo this should work for you for untied bias:\r\n\r\n```python\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.nn.modules.utils import _pair\r\n\r\nclass Conv2dUntiedBias(nn.Module):\r\n def __init__(self, height, width, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\r\n kernel_size = _pair(kernel_size)\r\n stride = _pair(stride)\r\n padding = _pair(padding)\r\n dilation = _pair(dilation)\r\n\r\n if in_channels % groups != 0:\r\n raise ValueError('in_channels must be divisible by groups')\r\n if out_channels % groups != 0:\r\n raise ValueError('out_channels must be divisible by groups')\r\n self.in_channels = in_channels\r\n self.out_channels = out_channels\r\n self.kernel_size = kernel_size\r\n self.stride = stride\r\n self.padding = padding\r\n self.dilation = dilation\r\n self.groups = groups\r\n self.weight = Parameter(torch.Tensor(\r\n out_channels, in_channels // groups, *kernel_size))\r\n self.bias = Parameter(torch.Tensor(out_channels, height, width))\r\n self.reset_parameters()\r\n\r\n def reset_parameters(self):\r\n n = self.in_channels\r\n for k in self.kernel_size:\r\n n *= k\r\n stdv = 1. / math.sqrt(n)\r\n self.weight.data.uniform_(-stdv, stdv)\r\n self.bias.data.uniform_(-stdv, stdv)\r\n\r\n\r\n def forward(self, input):\r\n output = F.conv2d(input, self.weight, None, self.stride,\r\n self.padding, self.dilation, self.groups)\r\n # add untied bias\r\n output += self.bias.unsqueeze(0).repeat(input.size(0), 1, 1, 1)\r\n```"
Segmentation fault on importing torch,"I assume `pip install git+https://github.com/mwydmuch/ViZDoom` compiles ViZDoom, correct? In that case, since your system compiler is GCC 4.84, it will be incompatible with PyTorch bindings, since we compile with GCC 4.9 and the two are ABI incompatible, which leads to segfaults.\r\n\r\nYou should either:\r\nA) Compile PyTorch from source, with your system compiler. Then both projects will have been compiled with the same compiler and the problem goes away.\r\nB) Install GCC 4.9 ([see instructions here](https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6)) and set the `CXX` environment variable before compiling VizDoom (`export CXX=g++-4.9`). Then VizDoom should be compiled with the same compiler as PyTorch and the problem should go away.\r\n\r\nEither way, you have to make sure both are compiled with the same compiler version."
[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times,"Why not using numpy manner? \r\n`x[..., None, None]` and `x[..., 0,0]` for your example"
np.repeat vs torch.repeat,"Easiest thing to do for now is to add a warning to the docs. One day we'll probably re-visit the all the differences between the Numpy API and pytorch API, but for now, just changing repeat might be disruptive to our users.

Feel free to submit a pull request @PetrochukM (otherwise, I can submit one)."
documentation for sparse tensor creation after 0.4.0 update,You want `torch.sparse_coo_tensor` -- I haven't checked if we've documented it properly.
[JIT]torch._C._infer_size throws an exception when traced,"Got it! I think this should work fine for most of our use cases, since this is only called when distribution instances are being constructed, after which the parameters are frozen and should have a fixed shape. One possible downside might be that we will need to pass tensors with the correct shape in the torch.trace annotation, and that can get tricky if it depends on the minibatch size. So eventually, it will be nice to have a more generic support for JIT. cc. @fritzo, @eb8680."
question: where (if) are the caffe2 libraries?,Caffe2 only has packages in conda at the moment. 'conda install -c caffe2 caffe2' will install the Caffe2 libraries into an Anaconda env; they will be linked against other libraries in the Anaconda env as well.
failed to move parameters to GPU,or make self.conv = nn.ModuleList()
"Inconsistent behavior of F.conv2d(...,padding) and F.pad",Got it. I think you mean `torch.backends.cudnn.deterministic=True`
"[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64","You'll need to do a clean build from scratch. Also, you'll still need a cuda.framework fix of some sort or another. Working on a more robust version of this fix."
ThnnConv2DBackward' object has no attribute 'padding',@kyuusaku you can trace a model accurately with pytorch and get the graph.\r\nFor example:\r\n\r\n```\r\ntraced_model = torch.jit.trace(input)(model)\r\nfwd = traced_model._get_method('forward')\r\nprint(fwd.graph)\r\n```
"[feature request] Clarify document to avoid \""Error Importing cuda extension\""","Once your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as this will resolve some symbols that the dynamic linker must see:"
RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70,is there something in your LD_LIBRARY_PATH that might override general RPATH? For example do you have cuda 9.2 in your LD_LIBRARY_PATH?
[feature request] [pytorch] Truncated SVD,"LOBPCG is expected to outperform randomized SVD (based on power iteration), see\r\nhttps://github.com/scikit-learn/scikit-learn/pull/12319\r\n\r\nLOBPCG is already available in CUDA and MAGMA, so may be not so difficult to add it to PyTorch."
[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack),"We have a helper `torch.distributions.utils.broadcast_all()` that should allow you to\r\n```py\r\ntorch.stack(broadcast_all(a, b))\r\n```"
Python interpreter died without Traceback when CPU Tensor divided by zero.,"As tricky as this may be to fix, it's still a bug in a very real sense. Allowing C errors to creep into Python code is the result of a leaky abstraction."
Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True," confirmed with @apaszke as well that this is expected behavior.

He says:

because create_graph=True, this is expected.
In the a+b case, basically the gradient of the gradient (wrt parameters) is all 0, which means it doesnâ€™t require grad, because it effectively never touched a single variable that does require grad."
AttributeError: module 'torch._C' has no attribute '_TensorBase',Can you try uninstall and reinstall? I'm assuming that you built from source. Can you also check if you have binary pytorch installed?
Feature request: Object detection model zoo,Torchvision now has models for object detection and semantic segmentation officially supported
"dynamically change tensor with requires_grad=False by \""+=\"" cause error but \""+\"" doesn't","`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place."
ASSERT FAILED at /pytorch/aten/src/ATen/core/jit_type.h:128 [bug],"> @sndnyang, please try `pip install tb-nightly` for now (it should get to the release channel soon) to get 1.14 and then try again. Let me know if that works for you.\r\n\r\n@orionr , Hi, the same question when I run ` dummy_input = (torch.zeros(2, 1, 28, 28),); writer.add_graph(MLP(None), dummy_input, True)`\r\n\r\n```\r\n~/project/other/DLCodeSnippet/venv/lib/python3.6/site-packages/torch/onnx/symbolic.py in batch_norm(g, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled)\r\n 994 weight_value = torch.tensor([1.] * input_sizes[1]).type(\r\n--> 995 'torch.' + input.type().scalarType() + 'Tensor')\r\n 996 weight = g.op(\""Constant\"", value_t=weight_value)\r\n\r\nRuntimeError: r ASSERT FAILED at /pytorch/aten/src/ATen/core/jit_type.h:142, please report a bug to PyTorch. (expect at /pytorch/aten/src/ATen/core/jit_type.h:142)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7ffaf01c9441 in /home/xyang2/project/other/DLCodeSnippet/venv/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7ffaf01c8d7a in \r\n```\r\n\r\nand what's more (I'm not sure if it's an issue) after the previous exception.\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNameError Traceback (most recent call last)\r\n<ipython-input-6-0645dc03db09> in <module>\r\n 1 dummy_input = (torch.zeros(2, 1, 28, 28),)\r\n----> 2 writer.add_graph(MLP(None), dummy_input, True)\r\n\r\n~/project/other/DLCodeSnippet/venv/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py in add_graph(self, model, input_to_model, verbose, **kwargs)\r\n 532 print('add_graph() only supports PyTorch v0.2.')\r\n 533 return\r\n--> 534 self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))\r\n 535 else:\r\n 536 # Caffe2 models do not have the 'forward' method\r\n\r\n~/project/other/DLCodeSnippet/venv/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py in graph(model, args, verbose, operator_export_type, omit_useless_nodes)\r\n 301 # and we don't want graph visualization to fail in this case. In this\r\n 302 # case we'll log the warning and display the non-optimized graph.\r\n--> 303 logging.warn(ImportError(e))\r\n 304 graph = trace.graph()\r\n 305 if verbose:\r\n\r\nNameError: name 'logging' is not defined\r\n```\r\nThank you so much!\r\n\r\nBy the way(again), for using the writer, I had to install two packages, future(for the missing of past) and pillow (for the PIL) maybe because of my python is python3..."
python setup.py install failed. undefined references,We had the same issue. Problem was the version of MKL which was not good. Just running:\r\n\r\n```\r\nconda remove mkl mkl-include\r\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\r\n```\r\n\r\nbefore the installation fixed the issue.
THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error,somehow restarting my machine fixed this issue. using torch v1.1.0
"[OSX] pip package does not ship MKL, RuntimeError: fft: ATen not compiled with MKL support","Here's a complete reproducer:\r\n```\r\nconda create python=3.7 -n pytorch-pip-macos --yes\r\nconda activate pytorch-pip-macos\r\nconda install pip numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing --yes\r\npip install torch\r\npython -c \""import torch; torch.fft(torch.randn(3,12,12,2), 2)\""\r\n```\r\n\r\nThat will result in:\r\n```\r\n...\r\nInstalling collected packages: torch\r\nSuccessfully installed torch-1.2.0\r\nTraceback (most recent call last):\r\n File \""<string>\"", line 1, in <module>\r\nRuntimeError: fft: ATen not compiled with MKL support\r\n```\r\n\r\nThe 1.2.0 wheel is linking against `Accelerate` rather than `MKL`:\r\n```\r\n$ otool -L ~/anaconda3/envs/pytorch-pip-macos/lib/python3.7/site-packages/torch/lib/libtorch.dylib \r\n/Users/rgommers/anaconda3/envs/pytorch-pip-macos/lib/python3.7/site-packages/torch/lib/libtorch.dylib:\r\n\t@rpath/libtorch.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t/System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate (compatibility version 1.0.0, current version 4.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.0.0)\r\n\t@rpath/libc10.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.0)\r\n```\r\n\r\nThe 1.2.0 conda package does link against `MKL`:\r\n```\r\npip uninstall torch\r\nconda install pytorch -c pytorch\r\npython -c \""import torch; torch.fft(torch.randn(3,12,12,2), 2)\"" # passes\r\n```\r\n```\r\n$ otool -L ~/anaconda3/envs/pytorch-pip-macos/lib/python3.7/site-packages/torch/lib/libtorch.dylib \r\n/Users/rgommers/anaconda3/envs/pytorch-pip-macos/lib/python3.7/site-packages/torch/lib/libtorch.dylib:\r\n\t@rpath/libtorch.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libmkl_intel_lp64.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libmkl_intel_thread.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libmkl_core.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libiomp5.dylib (compatibility version 5.0.0, current version 5.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.0.0)\r\n\t@rpath/libc10.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.0)\r\n```\r\n\r\nI'm assuming the use of `Accelerate` in the wheels is unwanted, it should be using `MKL` for everything?"
torchfile.T7ReaderException: unknown object type / typeidx: -1112529805,"Please try appending the parameter long_size like this `load_lua(....., long_size=8)`."
RuntimeError: CUDA error: unknown error,I had exactly the same issue. i got it fixed by installing the right pytorch version for my 10.1 CUDA with:\r\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\r\ninstead of installing it with the installtion snippet generated for me by pytorch website:\r\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\r\n
optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum,"I think you are correct, if you indent the second and third to last lines in the init it works."
Different deterministic behavior between CPU and CUDA for orthogonal initialization,"This is expected. We do not guarantee that the random sequences generated on different devices will look the same. For performance reasons they are likely using different pseudo random generators, so it's almost impossible to make them equal."
Loss explosion with DataParallel on WGAN models,
C++ torch::Tensor serialization,Cool!! I just checked that I can both save and load torch::Tensors and std::vector<torch::Tensor>s with the latest macOS libtorch! Closing this now :)
"Differentiable 1-D, 2-D covariance (numpy.cov) clone",It'd be insanely useful to have a PyTorch version of `numpy.cov`!
Compiling from master yields std::runtime_error,"@pietern Thanks for tracking this.

Yet, I've been able ""overcome"" this somehow by compiling PyTorch again with a git clean -fdx in between, thus I don't have the logs for the failed compilation.

I just tried again with the same command line I ran this morning and the error doesn't occur. I'll investigate a little bit more on my side, to see if I can reproduce this, and will follow up on this topic."
Improved documentation of distributed launch utility,Thanks for reporting! We are aware of this confusing doc issue and are actively working on remediating it. Please track our progress in https://github.com/pytorch/pytorch/issues/60754. Closing in favor of 60754.
[nnc] LoopNest::fuseLoops failed to fuse loops with reduce ops inside,"If I understand correctly, Mikhail's approach (only checking the indices that include 'i0') works for this special case, and also works for cases that can not be handled by extending the equivalent check. See the example below,\r\n\r\n```\r\n for (int i0 = 0; i0 < 64; i0++) { // L_C0\r\n for (int i1 = 0; i1 < 32; i1++) { // L_C1\r\n C[i0, i1] = float(0);\r\n for (int i0_1 = 0; i0_1 < 64; i0_1++) { // L_C01\r\n C[i0, i1] = ReduceOp((C[i0, i1]) + (A[i0, i0_1]) * (B[i0_1, i1]), reduce_args={i0});\r\n }\r\n }\r\n }\r\n for (int i0_2 = 0; i0_2 < 64; i0_2++) { // L_D0\r\n for (int i1_1 = 0; i1_1 < 64; i1_1++) { // L_D1\r\n D[i0_2, i1_1] = C[i0_2, i1_1];\r\n }\r\n }\r\n```\r\nI modified the stop value for ` L_C1`. With this modification, the stop value of `i1` in `C[i0, i1] ` is different with `i1_1` in `C[i0_2, i1_1]` and the equivalence check would fail but it is still valid to fuse the two loops."
test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS,"Created a conda env with Python=3.7, followed the same steps to install Pytorch and tried to reproduce the issue.\r\n\r\ntest_dataloader.py passes the test.\r\n\r\nSeems that it is an issue related to Python 3.8 on Mac"
[package] error in colab tutorial #60189,This error is intentionalâ€”you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.
User-defined preferred BLAS backend of choice not respected,"Yes, #59220 fixes my issue, thanks for the help!"
USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory,"I got the same error, and I fixed it by updating the submodules:\r\n```\r\ngit submodule sync\r\ngit submodule update --init --recursive\r\n```"
Tuning `SumKernel` for Float16 on CPU with AVX512 vectorization,"Yes, `ilp_factor` stands for instruction level parallelism. It's the number of vector accumulators that get summed in paralllel. So it's a performance optimization. In theory, the optimal value would be the add latency (clocks) divided by the inverse throughput of load + add (clocks per element). Which is roughly 4, but depends on the data type and exact CPU. I usually refer to the [intel intrinsics guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#!=undefined&text=_add_ps&techs=AVX,AVX_512&expand=139,124,3333,3336,139,136,139,136).\r\n\r\nThe summation in #59489 is so small that I can't see `ilp_factor` having any impact. More likely, the issue is that the final sum of the vector accumulator isn't done pairwise which for float16 on avx512 would be summing 32 elements. \r\n\r\n"
How do I avoid nan gradients for exponential parameters?,"[detect_anomaly](https://pytorch.org/docs/stable/autograd.html?highlight=detect_anomaly#torch.autograd.detect_anomaly) may help but anyway I think we need a minimal reproducible example to look into that. Also, questions are supposed to be posted to https://discuss.pytorch.org/ as it's more likely you will get help there."
Can I train AI If AI model is located in the another modelâ€™s forward?,You may want to add model2 to an attribute of model1 in \\_\\_init\\_\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\r\n\r\nfyi questions are supposed to be posted to https://discuss.pytorch.org/ as it's more likely you will get help there.
M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`,Will be fixed in next nightly build
Some Logic Error with New Group?,"> In order to keep up with my_rank in the world process group, here will convert the dst rank to the world one.\r\n\r\n`_get_group_rank` actually does the reverse, it converts the world rank to the local one and then the local rank is used in the gather call with the sub_pg.\r\n\r\nGiven that is the main concern here that we provide a sub_pg to gather but still need to provide the global rank as `dst`?"
Test profiler oncall alert,Works!
Libtorch JIT : Does enabling profiling mode increase CPU memory usage ? How to disable profiling mode properly ?,"@w1d2s could you please make a small repo? \r\n\r\n`torch::jit::getProfilingMode() = false;` should disable the profiling executor, so it's probably something else.\r\n\r\n1. Could you please profile memory allocations with https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html . Let's see if they come from aten ops or from somewhere else? \r\n2. @w1d2s is the memory usage **much lower** without using the JIT?\r\n\r\nThanks!\r\n\r\n@nikithamalgifb we could keep on the jit queue until we get more info.\r\n\r\n"
TestCommonCUDA.test_dtypes_matmul_cuda fails,"Read it as \""Skip this test when IS_WINDOWS is True,\"" or, equivalently, \""This skip is active if IS_WINDOWS is True, and it skips all tests instantiated from the TestCommon.test_dtypes template.\"""
it seems n_heads is not handled correctly in nn.MultiheadAttention,"As a simple workaround, couldn't you pass embed_dim * num_heads as the embed_dim? When it is divided amongst the heads, it should result in what you want."
"Nightly builds: CMake error \""Imported target \""torch\"" includes non-existent path\""","Ok, will do!"
`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky,@imaginary-person This test was failing consistently on master yesterday so I disabled them for XLA first. @bdhirsh and I will look into the root cause later today. Thanks for reporting!
Spurious failure reported by a GHA workflow,"This might actually be a false positive, we had noticed reports of artifacts carrying over from one build to another since our runners are atm not yet ephemeral (waiting on actions/runner#660)\r\n\r\nPerhaps a better cleanup step is in order so that we for sure have removed all of the previous build artifacts"
Does the NCCL operation use the default stream as other computations?,"The nccl implementation under the hood uses a custom stream pool and each time you launch collective operations, appropriate synchronizations are done automatically. So before the allreduce is launched, the nccl stream synchronizes with the default stream to capture computations appropriately. Also, after nccl allreduce is done, the default stream synchronizes with the nccl stream. As a result, you don't need to perform any synchronizations.\r\n\r\nIf you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option."
Add container for recurrent nets,"No, definitely not to `Module`. We can consider adding a base class for RNNs"
[docs] Docs missing not in-place Variable.scatter and some other methods,"Here is a working example for those looking for one\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nsize = [2, 2]\r\nn = 5\r\n\r\n# show case scatter method on tensors\r\ninput = torch.FloatTensor(*size + [n]).zero_()\r\nmask = torch.LongTensor(np.ones(size, dtype=np.int)).unsqueeze(dim=2).expand_as(input)\r\nprint(mask)\r\ninput.scatter_(-1, mask, n)\r\nprint(input)\r\n\r\n# Now test scatter method on variables\r\ninput = Variable(torch.FloatTensor(*size + [n]).zero_())\r\nmask = Variable(torch.LongTensor(np.ones(size, dtype=np.int)).unsqueeze(dim=2).expand_as(input))\r\nprint(mask)\r\ninput.scatter_(-1, mask, 5 * Variable(torch.ones(input.size())))\r\nprint(input)\r\n\r\n```"
Support for einsum notation,"Yeah, Pytorch lacks this. I can implement the feature"
GPU usage extremely in-balance for segmentation task,"The code hasn't been released yet. You can leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs"
How to select GPU programmatically in code,"```\r\nimport os\r\nos.environ[\""CUDA_DEVICE_ORDER\""]=\""PCI_BUS_ID\"" \r\nos.environ[\""CUDA_VISIBLE_DEVICES\""]=\""1\""\r\n```\r\n\r\nAre you looking for this?"
DataLoader raising inconsistent tensor sizes,"Ah, this comes down to `Scale` not scaling to a square but maintaining ratio, hence different sized input images will not fit when catting a tensor.."
Pooling throws an exception in Tegra TX1,"this is a weird error. Are you sure CuDNN is being detected at compile time?

You might have to set CUDNN_INCLUDE_DIR and CUDNN_LIB_DIR environment variables.
You can check cudnn's status with: print(torch.backends.cudnn.enabled)"
[Feature Request] Cyclical Learning Rates,"Hi. I am the puller of LR Scheduler. IMO, you should be able to easily implement this using class LambdaLR. @ajbrock

"
Get a single batch from DataLoader without iterating,`next(iter(data_loader))` ?
How to change the outputs of forward_hook?,This feature is now in master.
Different behavior of sum() in-place on CPU vs GPU,"passing the input as the output for a function that does resizes should be an error; these functions usually resize the outputs at the start of the call and it's likely they don't even do the calculations correctly since that also modifies the input.\r\n\r\nOr maybe we just check that output tensors aren't resized? (this is what numpy does). I believe @zdevito had a use case where he wanted to pass in a big buffer, or let it grow dynamically, and just wanted to use that for the output without worrying about the sizes himself. Do I have that right, Zach?"
Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid,"Thanks for reporting this @martinarjovsky.

@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?

Because of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)

According to the docstrings of binary_cross_entropy and binary_cross_entropy_with_logits the input and target should have the same shape. Shall we just add a check and raise a ValueError if they are not? Happy to send a PR for this"
[pylint] E1101:Module 'torch' has no 'squeeze' member,"On VS code:
Add ""python.linting.enabled"": false to the settings file. Worked for me."
[Feature Request] Layer Normalization,"I use this:\r\n\r\n```python\r\nclass LayerNorm(nn.Module):\r\n\r\n def __init__(self, features, eps=1e-6):\r\n super().__init__()\r\n self.gamma = nn.Parameter(torch.ones(features))\r\n self.beta = nn.Parameter(torch.zeros(features))\r\n self.eps = eps\r\n\r\n def forward(self, x):\r\n mean = x.mean(-1, keepdim=True)\r\n std = x.std(-1, keepdim=True)\r\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n```"
[feature request] Add YellowFin optimizer,draft PR seems fine.
Import fails after Conda install,"your `cudatoolkit` from anaconda is build 2, which had a bug. Anaconda team fixed this in build 3. Do this: `conda install cudatoolkit`, and that should install build 3."
ReduceOps are breaking Pyro test,"I found the bug, I'll send a patch soon."
Linux CPU build script fails as MKL header files not found,"If you are not using conda, pip install mkl-devel gets you the headers. I haven't looked at your CI script. @SsnL can we add this to the error message?

"
RuntimeError: value cannot be converted to type uint8_t without overflow: 10000,"yeah the tutorials needs to be updated. for now, you can try this:\r\n```\r\ncorrect += (predicted == labels).sum().item()\r\n```"
CUDNN_STATUS_NOT_INITIALIZED when built from source,After these updates:\r\n- Nvidia driver: 384.81-> 387.26 \r\n- CUDA: V9.0.176 -> V9.1.85\r\n- CuDNN: 7005 -> 7102\r\n\r\nThe installation completes without error. Thanks!
Core dump after torch.permute with negative index,"Ah okay, thank you for the tracebacks. The root cause should be the same: a std::vector is being indexed with negative numbers in the backward pass. Fix incoming."
I cannot initialize Tensor. (They will become torch.autograd.variable.Variable),See https://github.com/pytorch/pytorch/pull/5225
[feature request] Add underscore to nn.init functions,"That makes sense, but we need to keep backward compatibility, so we need the non-`_` aliases too."
How can I access the model's attribution created during forward pass when using dataparallel?,"Returning what you want from the top-level forward function should defiantly work. You could do this pretty easily with an additional wrapper. As such:\r\n\r\n```python\r\n\r\nclass NormalModel(nn.Module):\r\n pass\r\n\r\nclass ExtraOutputWrapper(nn.Module):\r\n def __init__(self, *args, **kw):\r\n self.wrapped = NormalModel(*args, **kw)\r\n\r\n def forward(self, input):\r\n normal_output = self.wrapped(input)\r\n extra_output = self.wrapped.layer_of_interest.property\r\n return normal_output, extra_output\r\n```\r\n\r\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest. This seems fairly clean to me. Is there an issue with this?"
Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object,The most recent version of cpuinfo should fix this. We have to update the submodule.
src/tcmalloc.cc:278] Attempt to free invalid pointer 0x7fc84f022d40,"I also came across this problem under the same condition except installing pytorch with anaconda.\r\nMy problem occurred when forwarding nn.LSTMCell with cuda(). The cpu mode can work fine.\r\n\r\nbefore this, I installed libtcmalloc-minimal4 to solve this problem #2314\r\n\r\nIs there any solution?"
fatal error: torch/torch.h: No such file or directory,"Right now you have to `build install` pytorch for that header to be installed correctly. There's three options:\r\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\r\n2. `python setup.py build install`\r\n3. `python run_test.py --exclude cpp_extensions`\r\n\r\n(1) is probably best"
pytorch installation error on macOS 10.13.3,^ resolved by reinstalling tbb
Pretrained Model Loading Error,"If you were to load the file using pickle alone, I'd suggest trying to load it using `encoding='latin1'` for example.\r\n```python\r\nwith open(myfile, 'rb') as f:\r\n data = pickle.load(f, encoding='latin1')\r\n```\r\nThis has solved issues for me when deserializing some pickle files saved on python2 and loaded on python3. But to make it work out-of-the-box on `torch.load` might require extending the API."
Error loading gzipped weights,"Try this (it will only work on master):\r\n```\r\nimport torch, shutil, gzip\r\nimport torchvision.models as models\r\nimport io\r\nresnet18 = models.resnet18()\r\ntorch.save(resnet18.state_dict(), 'test.pt')\r\nwith open('test.pt', 'rb') as f_in, gzip.open('test.pt.gz', 'wb') as f_out:\r\n shutil.copyfileobj(f_in, f_out)\r\n #f_out.write(f_in.read())\r\n \r\nwith gzip.open('test.pt.gz', 'rb') as f:\r\n # Use an intermediate buffer\r\n x = io.BytesIO(f.read())\r\n state_dict = torch.load(x)\r\n```\r\n\r\nI think the underlying problem is that `torch.load()` is bypassing gzip and directly reading the compressed file without uncompressing it. A workaround is to unzip the file into a new file and then load that file. Not sure how feasible it is to fix `torch.load` to support this behavior."
torch.nn.functional.cosine_similarity got unexpected behavier with torch.int8 dtype,"Indeed, converting back to float before calling the function does return the right value.\r\nSo something is wrong with the type promotion here."
Bazel - Pybind - Pytorch - Undefined Symbol,Resolved. Turns out you need to add 'linkshared=True' and compile it as a binary
Inplace ReLU incompatible with backward hook,"What I meant is that if you have access to the Tensor, you can use a Tensor hook:\r\n```python\r\n# This one will be called during the backward\r\ndef tensor_hook(grad): print(grad.size())\r\n\r\n# Register this wherever you have access to the Tensor for which you want to print the gradients\r\nyour_tensor.register_hook(tensor_hook)\r\n```\r\n\r\nIf you want to still use your Sequential Module and don't have access to the Tensor directly, you can get it via a forward hook:\r\n```python\r\n# This one will be called during the backward\r\ndef tensor_hook(grad): print(grad.size())\r\n\r\n# This one will be called during the forward\r\ndef fw_hook(mod, input, output):\r\n output.register_hook(tensor_hook)\r\n\r\n# Register this at the same place where you used to register the backward hook\r\nmod[2].register_forward_hook(fw_hook)\r\n```"
"TypeError: __init__() should return None, not 'int' in validation Dataset","Closing, as this does not look like a bug in PyTorch, but rather a mis-use of  Python data model.\r\nFor example, following code raises the same runtime error:\r\n```\r\nclass Bar:\r\n    def __init__(self, bar):\r\n        self.bar = bar\r\n        return bar\r\n\r\nif __name__ == \""__main__\"":\r\n    x = Bar(5)\r\n```\r\n```\r\n% python3 bar.py\r\nTraceback (most recent call last):\r\n  File \""bar.py\"", line 7, in <module>\r\n    x = Bar(5)\r\nTypeError: __init__() should return None, not 'int'\r\n```\r\nPlease do not hesitate to comment/update an example, if you believe that PyTorch behavior is incorrect in this case."
torch.linalg.cholesky fails for some PSD matrices,"Thanks for your response. Makes total sense - the context of the problem is trying to add a small amount of (tikhonov) regularization to make cholesky stable so aware of the stability problem just my fix was mixing precisions as you say. With double precision numpy==pytorch. Feel free to close.

"
Scribe stats reporting is broken in GHA due to secrets access from fork PRs,Verified that scribe proxy is working now
Maybe an error occurred in torch.matmul and torch.mm when pytorch == 1.9.0,"This is a duplicate of #61291, fixed on master in #61394."
"[CUDA 9.1] error: argument of type \""const void *\"" is incompatible with parameter of type \""const float *\""","Unfortunately I don't think we have the necessary resources now to debug issues with CUDA 9.1\r\n\r\nI'm going to close this issue since it's for a version we don't currently support (1.5), and a CUDA toolkit that we also do not support anymore (9.1), but if anyone has any input they can give feel free to leave comments below."
HTTP Error 403 for torch.hub ResNet,"As a workaround, can you please adding the following line before making any \""torch.hub\"" calls:\r\n```\r\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n```"
torch.permute missing in docs,"This has been fixed in master.\r\n\r\nThat being said, the docs are not well formatted, so I'll fix that."
DDP comm. hook:`torch.futures.Future.then()` callback returns singleton list,"Thanks for the proposal! Indeed the return type of a singleton list is not ideal for many cases as we only have SPMD mode.\r\n\r\nOne concern I have is that, a future can not only be returned by `allreduce`, but also by other communication primitives such as `allgather`, which naturally returns a list of output tensors. How are you going to handle this case by `then()` if it can only return a single tensor?"
"Conv2d triggers assertion in mkl-dnn when padding=(n, 3)","@sakaia , I verify that MKLDNN v0.21.1 can solve this problem, we have upgrade mkldnn to 0.21.1, you can test it your side by pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html or build the pytorch source code according to README.md. Thanks!"
PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES,More comment:\r\n\r\nUsing the following command instead\r\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`\r\nwould fix the problem.\r\n\r\nBut I am not sure if this is the expected behavior...
Freeze during validation with distributed training and model with batch normalization layers,"pytorch 1.1\r\nthe same problem here, the training freezes after 20 epochs. GPU usage is constantly 100%( in the normal situation, it would jump between 80% to 100%), the data loader also stops working."
Allow parallel sending to device in DataLoader,"No the push occurs inside the transform. It would be possible to design a custom collate_fn but that would be inefficient. The whole point of doing this is because pushing to GPU is so time consuming. Therefore, the best way is to send to GPU in a data transform."
distributed all_reduce deadlocks in v1.1,"Pytorch 1.1 uses nccl 2.4.2 which has a known issue of hanging with long running jobs that was fixed for 2.4.6. https://github.com/NVIDIA/nccl/commit/f40ce73e8987d2990e4b9ef6c75f4b3423acce78\r\nWorkaround is to export NCCL_LL_THRESHOLD=0. \r\ncc @pietern, @mrshenli to bump nccl submodule."
CUDA error: unknown error on Windows,"@colesbury ok!\r\n```\r\npip3 install https://download.pytorch.org/whl/nightly/cu100/torch_nightly-1.1.0.dev20190516-cp36-cp36m-win_amd64.whl\r\ngit clone https://github.com/kymatio/kymatio.git\r\ncd kymatio\r\npip3 install -e .\r\n```\r\nand execute the following file: https://github.com/Jonas1312/kymatio/blob/inverting-scatt-mse/examples/2d/invert_scattering.py\r\n\r\nyou should be able to comment most lines and imports except:\r\n```\r\n# Compute scattering coefficients\r\nscattering = Scattering2D(J=J, shape=(height, width), max_order=order)\r\nif device == \""cuda\"":\r\n scattering = scattering.cuda()\r\n```\r\n\r\nEdit: you can set `J=2, shape = (32,32) and max_order=1`\r\n\r\nNB:\r\n- nvidia drivers 430.64\r\n- cuda 10.1\r\n- numpy 1.16.3 mkl"
Error while installing PyTorch,Take a look at https://github.com/pandas-dev/pandas/issues/23424#issuecomment-435612148\r\n\r\nYou may need to upgrade your command-line tools.
Build error with MSVC (aten\\src\\ATen\\native\\quantized\\Copy.cpp),"move the sentence \""float* src_data = src.data<float>();\"" into the function of \""AT_DISPATCH_QINT_TYPES ...\"" solves my problem. It looks like:\r\n\r\n AT_DISPATCH_QINT_TYPES(self.scalar_type(), \""Copy\"", [&]() {\r\n float* src_data = src.data<float>();\r\n\tscalar_t* self_data = self.data<scalar_t>();\r\n for (int i = 0; i < self.numel(); ++i) {\r\n self_data[i] = quantize_val<scalar_t>(\r\n self.q_scale().to<float>(),\r\n self.q_zero_point().to<int32_t>(),\r\n src_data[i]);\r\n }\r\n });\r\n\r\n Hope that will help you."
RuntimeError: ONNX export failed: Couldn't export operator aten::softmax,I just checked the nightly version! There it works fine! Thanks! :)
Inconsistent behavior between jit loaded model and the one to be saved,"@suo fyi it was labeled jit 9 days ago, but not triaged"
random behavior is not able to be controlled,???
MultiheadAttention is not scriptable,"Thanks for bringing up the issue and we have fixed it. We also added two JIT unit tests to cover the applications of torch.nn.MultiheadAttention module (i.e. ""test_torchscript_multi_head_attn"" and ""test_scriptmodule_multi_head_attn_cuda"" in test_jit.py). Those two could possibly be good examples for JIT scriptable."
Provide a way to implement an infinite / streaming collection as a Dataset,"`return HPARAMS.batch_size` looks pretty wrong. you would want some large value (as you mentioned, something like `1<<30`).\r\nThe DataLoader only pre-loads one batch ahead of time.\r\nWhat you are running into is probably that if the size is VERY large, we actually generate indices ahead of time to preserve determinism: https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L144-L148\r\n\r\nAlso, the DataLoader is not a one-stop solution for all situations, something like `joblib` might serve better here. @SsnL has a WIP PR for IterableDataset, which might also serve this better: https://github.com/pytorch/pytorch/pull/14705\r\n"
Building libtorch-dependent project with CMake,"I don't know the answer to this, but I suggest using new cmake 'targets' feature, more like\r\n\r\n```\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_library(a)\r\ntarget_link_libraries(a PRIVATE Torch)\r\n```\r\ninstead of messing with TORCH_INCLUDE_DIRS and TORCH_LIBRARIES themselves."
Parameter not registering if .to(device) is used,"this is totally expected.\r\n\r\nParameters can only be leaf Tensors, not Tensors that are a result of an operation on another Tensor.\r\n\r\nWhat you want is:\r\n\r\n```\r\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\r\n```"
Windows CPU debug build fails at linking stage,"Hi,\r\nYes. With #17494 I can build without set BUILD_TEST=OFF\r\n\r\nThanks!"
Libtorch binaries compiled with flag _GLIBCXX_USE_CXX11_ABI = 1,???
Conda did not install cudnn for pytorch,"Hi,\r\neven if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`."
Tensor unfold backward is slow,"This was fixed by gh-36612, closing."
Makefile:140: recipe for target 'all' failed,I still got the same issue. The build.log is here: https://gist.github.com/jzenn/a97fd71775d7e572899ef7987e329d5d
Unused argument causes error in JIT graph to ONNX conversion,Thank you @spandantiwari!
cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer,CUDA 9 and RTX 2080 Ti simply aren't compatible and dont play well togethere.\r\nAn older CuDNN version working is likely a side-effect rather than expectation.\r\nUse CUDA10 and CUDA10 versions of CuDNN etc. for RTX 2080 which is Turing architecture
PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`,"Sorry I think I misunderstood your point above then, I though you were still seeing a problem even when .item() is used.
In that case it is (unfortunately) the expected behavior here.

It is easy to free any buffers encountered during the backward pass and raise a proper error if the user try to use them again.
But it is much harder to delete the graph structure itself while we're traversing it to compute the backward. Also the memory usage due to a single graph is small compared to the other objects in general and so should not be a problem (unless of course the graph keeps growing as in your case).

Another approach to solve this problem would be to use the `with torch.no_grad():` block (or the functions decorator equivalent) around any operations for which you won't need to compute backward. This way, it won't even build the graph and so will be faster and less memory hungry."
Is it possible to integrate jax into pytorch ?,"1.1, we already have with torch.jit\r\n1.2 seems interesting\r\n2. probably not relevant\r\n\r\n1.2 is tracked in https://github.com/pytorch/pytorch/issues/1642\r\n\r\nOverall, there isn't really the concept of \""jax in pytorch\"" or \""pytorch in jax\"" in the same sense that they're both frontends"
[jit] Support for modules that have hooks when compiling,Thank you @eellison ! :tada: :clap:
"ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5","Looks like it's not an issue with latest TRT6 and parser, so we can close it."
Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing,"@subho406 seems to work fine for me (with 1.1.0 and the latest `examples` repo), just fyi"
Windows Debug Build Failure,Please apply this patch: https://github.com/pytorch/pytorch/pull/17201
Why torch wheel is so huge (582MB)?,"The number of GPU architectures targeted by the binary is a large contributor to binary size. If you'd like a smaller GPU binary, you can change TORCH_CUDA_ARCH_LIST accordingly. For example, if you want to only support compute 5.0 w/forward compatibility, you can set TORCH_CUDA_ARCH_LIST=5.0+PTX as opposed to the more comprehensive list built by default by the PyTorch devs.\r\n\r\n(I'm not a PyTorch developer, just someone who has built smaller versions of the library.)"
[DISCUSSION] Expose Future() in Python API,"I would vote for `torch.Future` if it is really a single class.\r\nIf it requires extra functions to work with it, then`torch.futures.*` sounds good.\r\n\r\nAlso do we have a unified stable API for these?"
"We should not mark non-floating point Tensors as requirering gradients, ever","Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense."
Pickling of _VariableFunctions no longer works in 1.5,If any newcomers come here and wonder how to fix this. Upgrading to 1.5.1 fixed the issue. Downgrading to 1.4.1 may too.\r\n\r\nThat is\r\n`conda install pytorch==1.5.1 torchvision==0.6.1 cudatoolkit=10.1 -c pytorch` if you use vision\r\n`conda install pytorch==1.5.1 cudatoolkit=10.1 -c pytorch` if you use NLP.\r\n\r\ndepending on your cuda version.
Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions,include_dirs now supports both absolute and relative paths.
"Broken link from \""Contribution Guide\"" to \""Governance\""","Thanks for reporting, this looks like a bug. We'll accept a PR fixing it."
Empty GPU memory cache after Jupyter notebook interrupted,"jupyter notebook holds reference to the exception when interrupted (for things like %debug), which holds reference to the stack frames, which hold reference to the variables. so empty_cache won't work. this is not really a pytorch issue per se."
torch.utils.checkpoint.checkpoint + torch.cuda.amp fails,"Thanks for raising this issue!\r\nMy best guess is that `CheckpointFunction.backward` uses the stored mixed-precision tensors from its `forward`, but breaks the autocasting contract for the backward.\r\nIf you run `scaler.scale(loss).backward()` inside the `autocast` block, it should work for now as a workaround.\r\n\r\nCC @mcarilli"
"\""The specified procedure could not be found\"" importing `caffe2_detectron_ops_gpu.dll` via torch",@AsmaZbt The easiest solution is to patch `__init__.py` for PyTorch. Please use the modified version (for v1.5.0) [here](https://gist.github.com/peterjc123/bcbf4418ff63d88e11313d308cf1b427).
Windows 10 Libtorch installation issue.,"Okay. I resolved it myself. ðŸ‘ \r\nThe tutorial needs to add this line in CMakeLists.txt file\r\n``` set(CMAKE_PREFIX_PATH \""libtorch/share/cmake/Torch\"") ```\r\n\r\nwhich should point to where Torch is unzipped appropriately.\r\nPlease update the docs. This issue is quite prelevant\r\n"
torch.remainder gives a remainder larger than the divisor,"Thanks for the report!\r\n\r\nIt turns out that this problem is due to numerical precision issue when you subtract two larger numbers with only small difference, such as 100000000002 - 100000000001. Even the numpy single precision result of 1.024195 is not accurate (from c function `fmodf`). The accurate double precision result is 0.577396.\r\n\r\nThe current `torch.remainder` impl for both cpu and gpu have this problem, e.g. https://github.com/pytorch/pytorch/blob/253943d5a7bb90a420e4d94366101915823c7929/aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu#L80-L85\r\n\r\nI also checked the `torch.fmod` impl which are correct for both cpu and gpu. You can temporarily use `torch.fmod` as a substitute for `torch.remainder`. I will provide a fix to `torch.remainder` soon, and let it use the native c/cuda function of `fmod`."
PyTorch 1.5 conda package disallows OpenBLAS,"@soumith \r\nThis makes the conda package unsuitable for AMD CPUs, as Intel MKL uses a slow code path for AMD processors. For broader hardware compatibility and to avoid vendor lock-in, OpenBLAS should be used instead."
torch.cdist [Cuda out of memory Tried to allocate 108GB of memory],"I'm closing it as after building my env from scratch it now works. (I was already on torch 1.5, so I can't directly point on the problem but ...)"
C++ API for Transformer model in libtorch 1.5.0,This is available since 1.7
RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py),@AugF `LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.\r\nThere should be a `/lib` after `site-packages/torch/`
Programmatically Disambiguate When to Step `_LRScheduler`,"I do agree with explicit is better, and leaving the control in the hand of the users. Alright, let's go for a `steps_on` property that can be either \""batch\"" or \""epoch\"". If we need more later, we'd do \""batch_and_epoch\"". Thoughts?\r\n\r\nIf you like this, do you want to update #37770 for this?"
Better testing on CPUs without AVX capabilities,"In that case we can run cpp tests under qemu, which can be configured not to support any vecorized instructions."
[BatchNorm] Unexpected behaviour with track_running_stats,Closed by #38084
`torch.log10` with float32 produces different results on different CPU,"@mruberry \r\nYes, that makes sense. I wanted to leave a record of this finding and double check if there is something that we can work on.\r\n\r\nWhile filling out this issue I also found that `float64` produces consistent result, so we are going to use `float64` in torchaudio to resolve the issue we are having.\r\n\r\nThanks!"
torch.stack() gradient errors in 0.4.1,We will update and merge #9995
"Reuse the Variable with the same name with \""For loop\""","there is no problem with this loop, backward will be computed correctly and is independent of how you use `g_x_local`. In the future, post questions to https://discuss.pytorch.org"
DataLoader Multiprocessing Problems,???
The default value of parameter count_include_pad of torch.nn.Functional.avg_pool2d(),docs are updated now.
Falling to turn shape into Tensor,"Do\r\n```python\r\na = torch.ones(10, 10)\r\nb = torch.tensor(a.shape)\r\nprint(b)\r\n# tensor([10, 10])\r\n```\r\nAlso, questions like this are better suited for https://discuss.pytorch.org/"
Build fails in caffe2 git master with OpenCL ON,"I encountered the same issue...\r\nCurrently, have no solution."
[pytorch] [feature request] Entropy function,you can calculate this via `distributions.Categorical(probs=p).entropy()`?
Error Sharing CUDA Models between Processes using `torch.multiprocessing`,I can repro this on 0.4.1. Running bisect to find the offending commit.
[feature request] unique to work on nd tensors via dim arg,I have no plans to work on it @ptrblck and I don't think anyone else is planning on it (@alicanb?). \r\n\r\nThat and the [cuda kernel for unique](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Unique.cu) look like good places to start.
[pytorch] [docs] Mention that sparse COO tensors support duplicate indices,"This is a nice explanation of coalesced sparse tensors. A suggestion: add a link to this page from `torch.sparse_coo_tensor` doc, saying that details about duplicate indices are there?"
Torch handles assignment to all-False boolean masks incorrectly,probably fixed by @gchanan 's changes
mse_loss reduction='none' is ignored when required_grads is True,"Many thanks @li-roy, with 1.0.0.dev20181018 it works smoothly."
3x regression in JIT LSTM speeds,Yes the slowdown was due to memory leak
[JIT] Support torch.distributions.utils.broadcast_all(),I believe this is related to https://github.com/pytorch/pytorch/issues/8076
windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device,"The new binaries are updated for Windows.\r\n\r\nIf you have previously installed via anaconda, you have to do:\r\n\r\n```\r\nconda uninstall -y pytorch\r\nconda clean -t -y\r\n```\r\n\r\nand then reinstall pytorch.\r\n\r\n\r\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)"
[jit] Proposal: fuse add ops that expand tensors,"Me, @zdevito and @colesbury discussed that when merging the `TensorIterator` PR, and decided to settle for a temporary regression in the fuser to unblock it. I don't think we should insert unnecessary expands if the nodes won't get fused anyway, because that's just adding extra work, so I'd go with option 2."
Quadro m2000m not able to get pytorch working with gpu,"The new binaries are updated for Windows.\r\n\r\nIf you have previously installed via anaconda, you have to do:\r\n\r\n```\r\nconda uninstall -y pytorch\r\nconda clean -t -y\r\n```\r\n\r\nand then reinstall pytorch.\r\n\r\n\r\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)"
"Wrong Warning \""compiler (c++) may be ABI-incompatible with PyTorch!\""","@johnmarkwayve no, the warning is only raised if you're using a binary build of Pytorch, i.e. from pip. It's never raised if you compile from source."
[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.,"Yes @vadimkantorov , that was the solution!!
I enabled ""set_flush_denorm"" in case of CPU and now different checkpoints have the same inference time. Many thanks!"
CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++),"I just found out that it works, if I add \""filter\"" as external input for the \""train_net\"" like so:\r\n\r\n train_net.add_external_input (\""filter\"");\r\n\r\nbefore executing CreateNet(train_net)."
MSELoss wrongly sums instead of averages when reduction='elementwise_mean',"Yes but itâ€™s fixed on master and will be included in the next release.\n\nOn Wed, Sep 19, 2018 at 03:52 simama <notifications@github.com> wrote:\n\n> I am on 0.4.1 version and it seems this bug is still not fixed.\n>\n> â€”\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/10148#issuecomment-422696370>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZfuFXXaUvgqzfKfZO7_kPUMt0gsQks5ucfe-gaJpZM4VrZIT>\n> .\n>\n"
Support 5D tensors in PReLU,Thanks! I'll give it a go.
Backward engine computes unnecessary dependencies,Fixed in #752.
F.relu(inplace) followed by F.dropout(inplace) breaks backward pass,"Yeah, that is expected. You can't apply in-place operations to leaf Variables. Just remove the `inplace` flag and it should be good."
a simple compiling error with clang in macOS,hopefully #732 fixes this
"torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",I think we should make a `torch.arange` that is equivalent to `numpy.arange` and depreceate `torch.range` properly. It is used quite extensively all over the place.\r\nwdyt @colesbury @apaszke ?
Improve error readibility,add to https://github.com/pytorch/pytorch/issues/39
LSTM forget gate bias initialization,"Yes, the ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value."
Feature Request: Add BCELossWithLogits to replace BCELoss for numerical stability.,> @yzgao I am having trouble understanding how you came up with this formulation:\r\n> \r\n> 1. How is this equivalent to BCE?\r\n> 2. Where do the numerical instabilities arise in BCE and how does this formulation fix it?\r\n> \r\n> Thank you for your time!\r\n\r\nyou can see it at http://gameofdimension.com/2017/10/29/numerical-stability/
Error while saving my network,this is a syntax error. `torch.save` does not have the order of arguments as you've used.
Build fails on Ubuntu 14.04 + conda latest,CUDA Version 8.0.27 has this issue. it is a pre-release version. The stable CUDA8 version is 8.0.44
"cuda.XxxTensor.inverse, svd ... raise errors at even times","@ShigekiKarita @rdfong Do either of you know if this still happens? It's been a while and we've done many updates of magma version, and so the problem may have been fixed. I am unable to reproduce on the pytorch: 1.0.1-py3.6_cuda9.0.176_cudnn7.4.2_2 pytorch official conda release."
Visualizing Networks,Thanks Soumith. Found it.\r\n\r\nDiscussion for visualization of DNN can be found [here](https://discuss.pytorch.org/t/how-to-get-the-graph-of-your-dnn-drawn/184).
view() after transpose() raises non contiguous error,"Yes, this is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue"
Feature Request: Bitwise Operations,fixed via #1556
Errors import torch installed form source on macOS,Run python from a different directory than the repository root.
Add support for variable length sequences in cuDNN RNNs,@glample CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality
utils.data.DataLoader freezes,"What is the size of your shared memory segment (df -h | grep shm)? Pytorch uses shared memory to transfer data between processes, and it is quite demanding this way."
Maxout Layer,"For ones who need Maxout, I changed the above code to make it work. \r\n\r\n\r\n```python\r\nclass Maxout(nn.Module):\r\n\r\n def __init__(self, d_in, d_out, pool_size):\r\n super().__init__()\r\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\r\n self.lin = nn.Linear(d_in, d_out * pool_size)\r\n\r\n\r\n def forward(self, inputs):\r\n shape = list(inputs.size())\r\n shape[-1] = self.d_out\r\n shape.append(self.pool_size)\r\n max_dim = len(shape) - 1\r\n out = self.lin(inputs)\r\n m, i = out.view(*shape).max(max_dim)\r\n return m\r\n```\r\n"
dead loop when iterate through dataloader using customized datasetfolder,"Sorry, I don't know it, either. Multiprocessing is very complex when some interactive interpreters are involved like juypter or IPython. Anyway, Could you please close this PR?"
torch.autograd.gradcheck doesn't work for a function with one argument,fixed via https://github.com/pytorch/pytorch/pull/13543
Unefined reference to C10::Error::Error when linking against libTorch,"I think you're missing `\""-D_GLIBCXX_USE_CXX11_ABI=0\""` in your build flags. We provide this in our TorchConfig.cmake, which is why we recommend cmake as the easiest way to build LibTorch. That said, you can add that to your QtCreator config. See https://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in for the relevant file\r\n\r\nPlease let me know if that fixes the issue."
torch.dist with inf or -inf norm always returns 1,"I think `dist` exists for legacy reasons. The THC backend has a fused implementation that performs the subtraction and the norm in one kernel which will make it faster than `at::norm(src - dest, p)` in some cases."
Exception messages are getting swallowed in Script from python functions,@eellison This may be a regression related to adding exceptions and assertions. Can you look at it?
can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.,"`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor."
Support dist.barrier with NCCL backend,"@SsnL See my implementation, there are two scenarios that need to be considered and I think this should be good to go."
GPU Memery leak,i find a solution yesterday\r\n\r\nbatch_y_predlabel has to be **detach** so the GPU memory can be free.\r\n\r\nI don't know if it's normal that a GPU tensor transfer to the CPU in the compute graph still take GPU memory?\r\nfor gradient compute I imagine?
[Build error] libnccl.so: error adding symbols: File in wrong format,"it's very likely that you installed x64 libnccl, instead of ppc64 version"
Variable/Tensor Merge Proposal,"Most of the work here is done, the rest is cleanup, and making good on the devirtualization promise."
Can cuda10 use pytorch-0.4.1?,no
cosine_similarity function produces results more than 1.0,"Probably better to re-order the computations to improve numerical precision. Might want to look at SciPy:\r\n\r\nhttps://github.com/scipy/scipy/blob/453932337f4a67170e4e7fda3f808b273a787a41/scipy/spatial/distance.py#L717-L721\r\n\r\nI think the issue is that we're doing:\r\n\r\n```\r\nx / (sqrt(x) * sqrt(x)) # bad\r\n```\r\n\r\nvs.\r\n\r\n```\r\nx / sqrt(x * x) # good\r\n```\r\n\r\nI don't know enough about floating point arithmetic to argue why the second is more accurate, but it seems to be. (You can look at the min/max over a large random tensor)"
Port adaptive_avg_pool3d to ATen,@crcrpar It doesn't look like anyone is working on this yet. Paging @xmnlab and @skrah as heads up; can you two volunteer to review the PR when it comes up?
Torch Inverse C++ - Error in magma_getdevice_arch: MAGMA not initialized,"It looks like the C++ CUDA API is not calling the magma initialization function (it is done from Python only)\r\n\r\n```\r\nmacbook-pro-116:pytorch-tmp ezyang$ ag THCMagma_init\r\ntorch/csrc/cuda/Module.cpp\r\n382: THCMagma_init(state);\r\n\r\naten/src/THC/THCTensorMathMagma.cu\r\n21:void THCMagma_init(THCState *state)\r\n\r\naten/src/THC/THCGeneral.h.in\r\n71:THC_API void THCMagma_init(THCState *state);\r\n```\r\n\r\nCC @yf225, can you look into this, or delegate it accordingly?"
Is mkl-dnn enabled in the latest binary distribution v1.0.1?,Yes.
nn.LSTM gives nondeterministic results with dropout and multiple layers,Closed and fixed in cudnn_7.6.1 @ngimel
Add support for mixture models in torch.distributions,"`MixtureSameFamily` should be easy to implement. We don't use this in Pyro since we usually keep the mixture component id as an explicit variable and enumerate over that variable:\r\n```py\r\ncomponent = pyro.sample(\""component\"", dist.Categorical(probs),\r\n                        infer={\""enumerate\"": \""parallel\""})\r\nassert component.reshape(-1).shape == probs.shape[-1:]\r\nvalue = pyro.sample(\""mixture\"", MyDistribution(my_params[component]))\r\n```\r\ncc @martinjankowiak"
IndexError while trying to save torchscript,"Closing this since I can't reproduce and we fixed some similar errors recently, but feel free to re-open if you're still getting this error"
Strange issue moving torch::Tensors in libtorch/C++,"The fact that it doesn't happen when you are pure C++ makes me think it's a GIL problem. I have a guess: adjust your code to stop using `gil_scoped_release`; instead, manually release the GIL yourself inside the bodies of your functions using AutoNoGIL."
Descriptive error message for distribution constraint violation,"> Arguably, exceeding the normalization error tolerance should only trigger a warning, since it just reflects a limitation of finite precision arithmetic, not an error by the caller. Unlike e.g. passing negative / inf / NaN.\r\n\r\nI just checked the multinomial implementation and it does the normalization of the cumulative distribution to ensure that the last value sums to 1. So we can certainly relax the threshold. Note that you can disable the args checking by passing `validate_args=False` for the distribution instance. Raising an error is helpful during model debugging when figuring out where NaNs are coming from etc., but can safely be turned off for a mature model. Also, you could consider using logits directly which shouldn't have this issue."
[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED,Closing as this has been fixed (the repro in the initial post now works as intended)
[CPU] several inplace functions fail since 1.0.1 on certain hw,I can reproduce this with OMP_NUM_THREADS=2. I haven't seen it with OMP_NUM_THREADS=1.
module' object has no attribute '_dl',"it's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\r\n\r\nTry:\r\n\r\n```\r\npip uninstall torch\r\npip uninstall torch\r\npip uninstall torch\r\npip install torch\r\n```"
Cannot open include file: 'numa.h': No such file or directory,"\r\nActual problem:\r\nhttps://stackoverflow.com/questions/18435516/how-to-set-a-cmake-option-at-command-line. And actually, calling `tools\\build_libtorch.py` is a better way to build libtorch because it shares the same code path with `setup.py`."
CTCLoss with empty target doesn't work well,???
IndyLSTM & IndyGRU in PyTorch,"it's not widely used enough yet, to be pushed into core (feel free to reopen once it becomes more of a standard).\r\n\r\nAdditionally, we are working on a user handwritten RNNs being fast, rather than adding more fundamental multi-layer RNNs into core.\r\nSo, I'm closing the feature request."
Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight',your input is on gpu 1 but your net work is on gpu 0.
CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1,"I got the same issue with Pytorch 1.3.1 + CUDA 10.0. Finally, I got it resolved with:\r\n```\r\npip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\r\n```"
[jit][script] torch.jit.script does not support nn.ModuleDict,Going to close this as https://github.com/pytorch/pytorch/issues/16123 is already tracking. Thanks for the report!
error executing torch_shm_manager in cifar10_tutorial.py,"Experienced the same issue on OSX. Setting `num_workers=0` on the DataLoaders solved it, even though the tutorial only recommends it for Windows.\r\nIt should probably be better clarified/fixed though."
arange sometimes changes dimensionality of output tensor,???
"\""module has no attribute 'downsample'\"" when scripting torchvision's resnet",I believe you need to use a nightly version of torchvision (and thus a nightly version of PyTorch) for resnet to be scriptable.
"Improved Tensor subclassing support, preserving subclasses on function/method calls","I want to point out another use case of this functionality that surfaced in our conversations with OpenAI. What OpenAI wants to do is insert hooks at a per operator level, so they can inspect the tensors that are flowing through each operation (right now, they are hooking in at the module level, but sometimes there are more fine grained operations they need to hook into).\r\n\r\n`__torch_function__` is tantalizingly close to providing what you need for this, but:\r\n\r\n1. We need subclass preservation, so our hooks keep getting run (this issue)\r\n2. We need away to insert code that operates on both functions and methods uniformly (so we can write a single function that overrides all operators)\r\n\r\nLet's make sure we can hit this case too!\r\n\r\ncc @suo @orionr @NarineK who were present for this conversation."
`at::parallel_for` does not propagate thread-local variables to child threads in embedding_renorm_,"## Identification of the current problem\r\nSo the problem is that `parallel_for` does not propagate thread local states into the thread pool it uses.\r\nThe problematic states here are the `GradMode::enabled` and wether or not to unwrap variables during dispatch.\r\nIndeed, in the original function, we work with Tensors and GradMode is disabled. While in some of the worker threads, we now work with Variables and with GradMode enabled.\r\n\r\nBut after a quick look, others could be leading to unexpected behavior as well like\r\n- `in_parallel_region_` used to check whether we are already in a parallel region, worker threads could actually re-spawn more threads in some cases?\r\n- many jit stuff\r\n- cuda current streams\r\n\r\n## Main question\r\n\r\nIs `parallel_for` supposed to be usable with Tensors. And so running operations on Tensors in each worker thread should do the same thing?\r\nOr is `parallel_for` supposed to work only with pointers of raw data: Tensors should be unpacked before and only pointers should be passed to it?\r\n\r\n## Proposed answer\r\n\r\nAfter discussion, the main point is that making `parallel_for` work with Tensor operations is actually a big feature (as in a lot of work).\r\nSo for now we can fix this function to remove the use of Tensors and add a comment with `parallel_for` to let the user know that they should unpack all Tensors before doing operations.\r\nWe can open a new feature request for support of Tensors in `parallel_for`.\r\n\r\nAny thoughts @ifedan @ngimel @VitalyFedyunin @ezyang ?"
"Failed to load model on mobile for device type \""c10::DeviceType::CUDA\""",@ljk53 sorry late response. \r\nI confirmed the model converted to cpu() is able to load on the phone.
[Quantization] (Error): No function is registered for schema aten,"Yeah, please fuse batchnorm, see tutorial: https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"
No type hints for `Tensor.T`,"I hope to not do know, but as a rule of thumb, only the things generated from `native_functions.yaml` are autogenerated. `.T` is defined manually `python_variable.cpp` below, so the hints would be manual, too.\r\nhttps://github.com/pytorch/pytorch/blob/ee920b92c4d52c3481ce4fa1594893fde8ccdd93/torch/csrc/autograd/python_variable.cpp#L491"
Didn't find kernel to dispatch to for operator 'quantized::conv2d',"Hi @thiyagu145, the error message you've encountered says that you're trying to run the quantized Conv2d operator, but you're passing in an unquantized tensor.

Please take a look for where we use QuantStub in the tutorial and workflow documentation. Once this is present and the values are passed through them, the issue should be solved"
How does execute() in MKL-DNN call functions 'jit_*',Here's [relevant discussion](https://github.com/intel/mkl-dnn/issues/576) on DNNL Github.
Not able to reproduce the same results with the C++ API. I get different output result.,???
"Why t.arange(0,3) create an int type Tensor?","torch.arange returns an int64 tensor by defaut. It mimics numpy behavior. if you want different dtype, give dtype as argument, such as `t.arange(0, 3, dtype=torch.float32)`"
[Caffe2] Caffe2 built with protobuf-lite still require full protobuf library,"@gemfield, thanks for fixing libcaffe2_protos.a target installation!\r\n\r\nUnfortunately adding **option optimize_for = LITE_RUNTIME;** breaks compilation:\r\n> /home/user/pytorch/caffe2/contrib/script/compiler.cc:760:48: error: no member named 'DebugString' in 'caffe2::NetDef'\r\n return functions.at(functionName).net_def->DebugString();\r\n\r\nAnyway, in case this error can be easily fixed, I think these adjusments of proto files should be done somehow automatically, because right now these configuration flags make no sense and require additional knowledge of protobuf internals from user.\r\n\r\n"
Loading opencv image to pytorch tensor,"I found it at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#exhale-class-classat-1-1-tensor.\r\nHere is a demo:\r\n```C++\r\n tensor_image = tensor_image.permute({0, 3, 1, 2});\r\n```"
[libtorch] FindCUDA error when running cmake,"> > Oh I am sorry. I didn't realize that you required LibTorch. Maybe you could try installing LibTorch from source, as specified [here](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/libtorch.rst).\r\n> \r\n> Will try that. Thanks.\r\n\r\nDoes that work? I came across the same problem"
CTCLoss produces NaNs in some situations,"@soumith Accroding to the doc, I have add a log_softmax to the output from network ,but the nan is appeared again in later iters."
What happened to symbolic_override?,I think adding a symbolic function for the nn.Module should still work. Will create an example to explain it.
Bug in transferring model from pytorch --> caffe2,"I modified the tutorial and removed the `pixel_shuffle` part and composed the network only with ReLU and conv layers (just to see if `pytorch` and `caffe2` give the same output), and **now it works fine**.\r\n\r\nSince `pixel_shuffle` is only on the master branch, I assume that it's still buggy?\r\nAnyway, I'll just test if all the layers I personally need output the same thing between the two frameworks and proceed."
[Caffe2] Caffe2 for Android has no include files,hi @rivergold Maybe this can help you : https://github.com/wangnamu/AICamera_new
osx-64/pytorch pypi nightly build segfault,"`pytorch-nightly` on OSX is the CPU build. OSX doesn't ship GPU binaries. At some point we screwed up on this, but fixed it after -- that's the reason for the issue and the fix."
Assertion fails when using DataParallel with two nn.Embedding,"I encounter the same issue too. When the nn.Embedding initialized with \""max_norm=1\"", then it occurs."
Using net.cuda crashes the kernel,"If you look into the list of types in the `got (...)` part, you'll find a mix of CPU and CUDA tensors, with `input` and `output` being on CPU, while `weight` and `bias` is on the GPU.\r\n\r\nYou probably forgot to send the input to the GPU. Alternatively, keep in mind that `.cuda()` is an out of place operation i.e.\r\n```python\r\ninput.cuda()\r\nmodel(Variable(input))\r\n```\r\nwill fail. You need to overwrite the reference with a new CUDA tensor:\r\n```python\r\ninput = input.cuda()\r\nmodel(Variable(input))\r\n```"
Gumbel noise,"Here you go. Much more readable and no modules required:\r\n```python\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\ndef sampler(input, tau, temperature):\r\n noise = torch.rand(input.size())\r\n noise.add_(1e-9).log_().neg_()\r\n noise.add_(1e-9).log_().neg_()\r\n noise = Variable(noise)\r\n x = (input + noise) / tau + temperature\r\n x = F.softmax(x.view(input.size(0), -1))\r\n return x.view_as(input)\r\n```\r\nWe're using GitHub for bug reports only, if you have questions please post the on [our forums](https://discuss.pytorch.org)."
GOMP_4.0 not found,I fixed the problem by importing opencv before torch.
[install] Debian packaging for Pytorch,@timmeinhardt All packages were updated and uploaded.
DataParallel should support multiple inputs,"Yes, it expects the batch dimension to be first."
Feature Request: Local response normalization (LRN),@eriche2016 I have used the LRN in an AlexNet implementation ([link](https://github.com/jiecaoyu/pytorch_imagenet/blob/master/networks/model_list/alexnet.py)). Check [here](https://github.com/jiecaoyu/pytorch_imagenet) for more details.
Cannot install `torchvision 0.1.7` by using conda,"i see. in this case, just remove torchvision (conda uninstall torchvision) and then install it via pip.\r\n\r\npip install torchvision"
nn.Module not importing parameters contained in lists,This behaviour is expected.\r\nThere is a detailed discussion in https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219
Placeholder variable,"Yeah I know how Theano and TF do this, and it's completely different from our imperative backends. There's no way to defer the function execution right now, and as far as I know it's necessary for Keras, as it will be calling ops on Variables without the data. If you enable lazy evaluation, it won't actually compute a thing, util you touch the `.data` of some Variable, which is when the whole subgraph before it will get computed. Until then, the leaves can even contain empty tensors, as they won't actually be used."
libTH doesn't recognize Intel MKL in its default location,"I encounter the same error, and I solve it by running command 'conda install mkl' in my activated conda env.\r\n\r\n> File \""/usr/local/lib/python3.5/site-packages/torch/__init__.py\"", line 45, in <module>\r\n> from torch._C import *\r\n> ImportError: dlopen(/usr/local/lib/python3.5/site-packages/torch/_C.cpython-35m-darwin.so, 10): Library not loaded: @rpath/libmkl_intel_lp64.dylib\r\n> Referenced from: /usr/local/lib/python3.5/site-packages/torch/lib/libTH.1.dylib\r\n> Reason: image not found"
[build/nccl] failed to build libnccl on Debian unstable,"@apaszke Thanks, the fix is to to export the two environment variables:\r\n```\r\nexport CUDA_HOME=/usr\r\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\r\n```"
When will you release a Windows branch for Pytorch?,And there's an issue on that already - #494
Feature request: add (log_)gamma and (log_)beta functions,"@colesbury @soumith could you prioritize this? `digamma` (`lgamma` with grads) is blocking the following probability distributions: `Binomial`, `Multinomial`, `Gamma`, `Beta`, and `Dirichlet`."
Allow optimizers to skip nn.Parameters that have requires_grad=False,"So I don't really think that it makes sense to allow such parameters. If you don't want to optimize some tensors, they're not parameters - they're fixed. You probably don't want to count them in. And if you really need to then\r\n```pytorch\r\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\r\n```\r\nshould do the trick."
Low CPU utilization on ImageNet example,"Also, note that if HDD is a bottleneck, then increasing the number of workers will only make it worse - the disk is 100% busy, but they're competing for files scattered across it, so the reads get even more random and degrade the performance."
Using generators instead of iterators for sampling (DataLoader),"Yes, it is very expensive. Yes, I don't want the iterators to end. In fact, I am not using a single iterator, but multiple at the same time (from different sources), and have to have constant batch sizes, which is why if they reach the end it is troublesome.\r\n\r\n 1. Yes, I am doing something like that. However, it does feel \""unclean\"" from the code point of view.\r\n 2. I have inserted a numpy.random.seed() in the generator itself that I trigger at the sampling start, which does work, but again is rather hackish.\r\n\r\nFrom my point of view, I have it all working, although it does feel hackish and I wouldn't be surprised to see it break when the DataLoader internals slightly change. So if you deem this not an issue (or feature bloat), feel free to close the issue.\r\n\r\nEasiest solution would be to have a parameter to tell DataLoader it is a generator, not an iterator, and have it use for example ```__call__``` instead of ```__iter__``` in that case. That would just generate a new sample. On the random issue, either have all the processes automatically generate new random seeds, or add an option to give the worker threads a \""initialization\"" function, which would be run when they are created. This would possibly allow other usages, while also allowing the user to randomize the seeds."
cuda runtime error (8) : invalid device function - adding cuda tensors,"@adelsalehali1982 Your error is unrelated to this issue, and happens because the code you are running supposes that 4 GPUs are used (if gpus are enabled), and you probabmy have less than 4 GPUs in your machine. You can fix that by changing the line with DataParallel to only use 1 or 2 GPUs"
"\""Symbol not found\"" when \""import torch\"" on Mac OS","As a workaround, \r\n\r\n\r\n`pip3 install torchtext==0.4`\r\n\r\nsolved the issue."
How to use cudnn in pytorchï¼Ÿ,To verify that pytorch uses cudnn:\r\n```\r\n>>> torch.backends.cudnn.version()\r\n6021\r\n```
[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member,"On VS code: \r\nAdding `\""python.linting.enabled\"": false` also worked in this case."
ImportError: No module named 'tools.setup_helpers',"@soumith I expect people will just keep doing this until there's a working package on PyPI. That's just the first thing people are going to try for a Python project.\r\n\r\nMaybe you could upload an `0.2` wheel that just prints a more helpful error like:\r\n\r\n> Installation from PyPI not supported yet (see status at `https://github.com/pytorch/pytorch/issues/566`). For now, please uninstall this package (`pip uninstall pytorch`) and follow the instructions at `http://pytorch.org/` to install with miniconda."
Register THNN double and half backends,"Yeah, that needs to be fixed. Should be done this weekend. Thanks for the report!"
[feature request] Support soft target distribution in cross entropy loss,"Is performance the reason why soft target is not officially supported? If so, we can provide a separate method of loss with soft target. Currently I have to manually implement it like this:\r\n```python\r\n def softmax_cross_entropy_with_softtarget(input, target, reduction='mean'):\r\n \""\""\""\r\n :param input: (batch, *)\r\n :param target: (batch, *) same shape as input, each item must be a valid distribution: target[i, :].sum() == 1.\r\n \""\""\""\r\n logprobs = torch.nn.functional.log_softmax(input.view(input.shape[0], -1), dim=1)\r\n batchloss = - torch.sum(target.view(target.shape[0], -1) * logprobs, dim=1)\r\n if reduction == 'none':\r\n return batchloss\r\n elif reduction == 'mean':\r\n return torch.mean(batchloss)\r\n elif reduction == 'sum':\r\n return torch.sum(batchloss)\r\n else:\r\n raise NotImplementedError('Unsupported reduction mode.')\r\n```"
"from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",It is pydot.\r\n
[feature request]Please go back to the old theme,"actually, the background and foreground is so confused, and I feel hard to read in such a theme."
massive test failures with SIGILL on old P6200 CPU with pytorch-0.4.1 built from source,"@colesbury \r\nPR by @cpuhrsch (891a3f5eeb) indeed fixes the problem. Now most of test suite runs successfully, no SIGILLs at all. Although some other errors appeared, I guess it's a totally separate issue. Here is the test log: http://vyal.ru/pytorch_test.logs"
RuntimeError: CUDA error: unknown error,Somehow the problem can be fixed following this advice:\r\n[https://github.com/pjreddie/darknet/issues/98#issuecomment-348441285](https://github.com/pjreddie/darknet/issues/98#issuecomment-348441285)
ImportError: No module named future.utils,sudo pip install future\r\n or\r\neasy_install future
Raise correct error type when passing invalid covariance matrix to MultivariateNormal,"As mentioned in #12102, the issue is that arg checking [happens](https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L30) in `Distribution.__init__()`, but `MultivariateNormal.__init__()` performs some linear algebra before calling `super(...).__init__()`. I think the solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`."
ImportError: No module named 'torch',"Also if it works for you in the terminal, using the peterjc123's check script,\r\nbut not in jupyter notebook [ you still have the error message : no module named torch ],\r\nthen get sure jupyter is properly installed IN YOUR CONDA env too.\r\nIf you do not see jupyter in the output of that command line => conda list\r\nthen type :\r\nconda install jupyter"
BN gets dramatically slow when batch_size exceeds a threshold,Just a quick progress update: Now the bottleneck is (by far) the pointwise operation at the end. I see good success using a separate kernel (i.e. with different thread/block setup) with the configuration in the repro script (Thank you! It is so valuable to have a reproduction with the report.) .\r\nI'll spend a moment benchmarking with other configs ~~and thinking about whether I can use a joint kernel~~ (I don't think a joint kernel will work) and then we should have a PR for better batch norm after the weekend.
Install only caffe2,it is no longer possible to only install CAFFE2.
Model checkpointed using torch.save() unable to be loaded using torch.load(),"Sigh, I think I found the problem. It seems like `local_rank` is actually the ID _within_ a worker; multiple workers have a `local_rank` of `0`, so they're probably trampling each other's checkpoints."
On cuda 10: THCAtomics.cuh(100): error: cannot overload functions distinguished by return type alone,"You should definitely backport that.\r\nRegarding releases, [next week, we'll all know more.](https://pytorch.fbreg.com/schedule)\r\n"
Illegal instruction (core dumped) on Debug CPU build,Could you try `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code`?
Please enable the autocomplete feature,"From what I read, the ongoing work to solve #7318 is going to be fairly standard (.pyi header files) and not tied specifically to pycharm. It should work for other editors too."
Broadcasting fails for mse_loss when tensors have no grad,"Yes, this is a bug and happens because the second case dispatches to our `THNN` implementation, which doesn't support broadcasting and has much more strict requirements.\r\nThis should be fixed, probably by removing the dependency on THNN altogether.\r\nFor information, the same will happen with `l1_loss`."
Some issues about the new doc theme,Closing because it's a duplicate. Please @leomao feel free to comment on that thread.
OMP: Warning #190 because of fork not waiting for parallel region to end,"I am also having this issue. It persists across data (MNIST, CIFAR10) and various architectures. I do not get the warning if I set `pin_memory=False`. I think it might be due to having three Dataloaders in my script like @FunkyKoki (i.e. train, test, validation) and iterating over two of them (test, val) while inside the loop of the other (train)."
[JIT] cannot access to initialized attribute in script-annotated class __getitem__,"Ugh, this is because Python's` inspect` returns members in name order instead of definition order.\r\n\r\nThe only method for which definition order matters is `__init__`, since it initializes the class's attributes. As a temporary workaround, try defining a method that does not precede \""__init__\"" in token order. \r\n\r\nThe longer term fix is simple enoughâ€”we should special-case `__init__` to be compiled first."
Tensor To OpenCV - C++,"Something like this?\r\nI stripped cuda usages out.\r\n```C++\r\n // Convert a char/float mat to torch Tensor\r\n torch::Tensor matToTensor(const cv::Mat &image)\r\n {\r\n bool isChar = (image.type() & 0xF) < 2;\r\n std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\r\n return torch::from_blob(image.data, dims, isChar ? torch::kChar : torch::kFloat).to(torch::kFloat);\r\n }\r\n\r\n cv::Mat tensorToMat(const torch::Tensor &tensor)\r\n {\r\n auto sizes = tensor.sizes();\r\n return cv::Mat{sizes[0], sizes[1], CV_32FC(sizes[2]), tensor.data()};\r\n }\r\n```\r\nDon't see any bugs there. Should probably be on forum instead.\r\nNot sure why you are splitting the channels and joining them back together. You can always permute your tensor if you want to move the channel ordering.\r\nConverting to float should just be: `.to(torch::kFloat);`"
torch.jit.trace returns unwrapped C type,"@NeilWangziyu You could also consider trying building PyTorch from source, if nightly doesn't work for you. Saving should definitely work now. @driazati added it 2 months ago in this PR: #20386"
CosineAnnealingWarmRestarts documentation poor and not appearing,"Hi, @jcreinhold\r\nI also wondered why it did not appear. Sorry for the poor doc after deciding to change name form SGDR to CosineAnnealingWarmRestarts.\r\nAbout the usage `LR should be updated every batch, not epoch`, `step()` without argument is set to epoch update, you can give an argument for a batch update. For example, if you have 15 training examples and batch size is 4, so there would be 4 iterations in an epoch. You could call step(0 + 0 / 4) for the epoch 0 and first batch update, step(0 + 1 / 4) for the epoch 0 and second batch update.\r\nSo the code might be modified to \r\n```\r\n\""\""\""Step should be called after every time a batch is processed in addition to after each epoch,\r\n like in the following example:\r\n\r\n >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\r\n >>> iters = len(trainloader)\r\n >>> for epoch in range(N):\r\n >>> for i, (inputs, labels) in enumerate(trainloader):\r\n >>> scheduler.step(epoch + i / iters)\r\n >>> optimizer.zero_grad()\r\n >>> outputs = net(inputs)\r\n >>> loss = criterion(outputs, labels)\r\n >>> loss.backward()\r\n >>> optimizer.step()\r\n\r\n\""\""\""\r\n```\r\n"
"I can't import PyTorch, libomp.dylib can't be loaded.",`brew install libomp` solves the problem.
Slow distributed training,"After using OMP_NUM_THREADS=1, the speed is back to normal. Thanks, @VitalyFedyunin."
SegFault and other errors on instantiating subclass of torch.FloatTensor and torch.Tensor,This looks complete after https://github.com/pytorch/pytorch/pull/20283.
[JIT] LSTMCell still not speedup with 1.1.0,fwiw running this on a master build with some warmup iters gives me numbers like\r\n```\r\n0.0015532950055785477\r\n0.001382627000566572\r\n```
RuntimeError: _th_uniform_ not supported on CPUType for Long,"You may have been getting this error if you were using a function besides `zeroes`. I imagine many people ended up here for that reason, so just in case: Like @CoruNethron said, the `dtype` argument that you probably grabbed from the [60-Minute Blitz Tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) is meant for the `zeros` function specifically.\r\n\r\n`x = torch.zeros(5, 3, dtype=torch.long)`\r\n\r\nIf you try to use it with `rand` for instance, you'll get the following error:\r\n\r\n`RuntimeError: _th_uniform_ not supported on CPUType for Long`"
Unable to compile an older version of PyTorch,Try `git submodule sync` and then `git submodule update --init`?
torch.arange always generate constant result in tracing,"Hi @lara-hdr, \r\n\r\nDo you mind linking to where you set `traceable=true` exactly? I'm not too familiar with that part of the code base.\r\n\r\n@zou3519 do you know why this has to be implemented in the python arg parser? it's also a schematized aten op. \r\n\r\n\r\n\r\n"
How to add dynamically allocated strings to Pickler?,cc @driazati
Unable to import 1.1 when installing with pip,Probably duplicate of #20030\r\ntry `brew install libomp `
Is the `device=` parameter required in torch.FloatTensor and similar ones,"`torch.FloatTensor` is a legacy constructor and doesn't support all types.\r\nTo answer your question, i think `torch.cuda.FloatTensor` might be what you are looking for -- but please dont use either. Just use `torch.empty(..., device='cuda')`"
[FR] Warn if scheduler.step() is called but optim.step has not been called,Why did that change??
NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training,"I ran with n_procs=1 and suddenly a magic new error message appeared telling me to try adding `find_unused_parameters=True` to `DistributedDataParallel`. When I did that, it worked! Thank you for good error messages!!!"
[FR] make IncompatibleKeys print nicer when there is no error,@designnner This is not an error. It's just the `repr` that isn't ideal. Feel free to ignore.
GRUcell has a wrong formula,"either is fine, because it's all about semantics of z. that is, if z is considered an update gate, z * n makes sense, while (1-z)* n makes sense if we think of z as a leaky coefficient. in the end, they are equivalent."
TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above,I think it's a version problem.\r\njust run this:\r\n`pip install tensorboard==1.14.0`\r\n(not pip install tensorboard==1.14)
TripletMarginLoss example isn't clear,
Building from source error: command 'gcc' failed with exit status 1,I have the same problem (`error: expected ')' before 'PRId64'` etc.) with gcc 4.8.5 on Linux with the current head ( 02450fff3)\r\n\r\nWhat works for me is to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\r\n* torch/csrc/Storage.cpp\r\n* torch/csrc/Tensor.cpp\r\n* torch/csrc/cuda/Storage.cpp\r\n* torch/csrc/cuda/Tensor.cpp\r\n\r\n(see also https://github.com/pytorch/pytorch/compare/master...andreh7:2017-11-10-prid64-fix -- this can be turned into a pull request very easily)\r\n\r\nThis fix is the same as #3574 but for different files. \r\n\r\n 
,
RuntimeError when using DistributedDataParallel,"I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with ``CUDA_VISIBLE_DEVICES=1`` on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\r\n\r\nI solved the problem by deleting a ``nn.Linear`` that I assigned to an attribute during the ``__init__`` of a custom ``nn.Module`` but never used during ``forward``.\r\n\r\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\r\n\r\nThanks!"
gradients of index_select wrong on GPU,
GPU Memory Leak at Master Branch,"Closing since this issue is stale, please reopen with a repro script if you still see the memory leak."
0.3 release checklist,
Support view() on batch dimensions for non-contiguous tensors?,"There's a good reason for the `view` invariant - most of such reshapes are impossible to pull off using stride tricks if then tensor isn't contiguous. On the other hand, making it contiguous inside `view` would mean that sometimes the returned tensor shares storage with input, and sometimes doesn't. This is important for cases like these: `x.view(-1)[::x.size(1) + 1] += c` (add `c` to diagonal of matrix `x`). If you know/suspect that tensors might sometimes be non-contiguous just add `.contiguous()` before `.view()` it's a no-op if the tensor already is contiguous"
Slight memory leak for LSTM,"Thanks @ngimel !

I'll close this issue for now. If you see this issue, please upgrade to cudnn 7.1+, driver 384.69+.

@Evpok @jiesutd @bangbangjim See above."
"Bug in build scripts, outdated existing PyTorch headers are picked up",
Error in nll_loss - multi-target not supported,"CrossEntropyLoss takes a 1D tensor. If your `target` has size `(32, 1)`, you need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor."
softmax doesn't support negative dimensions,if it didn't support negative dims in 0.2.0 then i wont mark it as a release blocker for 0.3
torch.load() requires model module in the same folder,PyTorch internally uses pickle and it's a limitation of pickle. You can try meddling with `sys.path` to include the directory where `module.py` is. This is exactly why we recommend saving only the state dicts and not whole model objects.
Pytorch AssertionError: Torch not compiled with CUDA enabled,"you are running this on OSX. The OSX binary of PyTorch does not come with GPU support.\r\n\r\nThe code you linked to still has some GPU stuff remaining (as you see from the stack-trace.\r\n\r\nChange these two lines to get_iterator:\r\nhttps://github.com/eladhoffer/captionGen/blob/48552694775ef11f5fa68c100584f40d98e4b690/main.py#L96\r\n\r\n`get_iterator(..., pin_memory=False)`\r\n"
Sampled Softmax / Log-Uniform Sampler - Google Billion Word Language Model,
Variable methods which need to change before we combine Variable and Tensor,"We have additional work to do before we can combined Variable and Tensor, but all these methods are implemented."
"using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__","This confirms my worst fears, python allows for pathologic iterables that have a length but never end... The fact that the `reversed` trick works is even weirder from a logical point of view: Reversing an infinite iterable results in a finite iterable... :woozy_face:\r\n\r\nAfter digging a bit through the [python reference](https://docs.python.org/3/reference/datamodel.html#object.__getitem__), the intended way of handling this is by raising an `IndexError` from `__getitem__` for invalid indices. Upon encountering this, the for loop will stop automagically. Changing my above example like below makes the iteration work as expected\r\n\r\n```python\r\nfrom torch.utils.data import Dataset\r\n\r\nclass TestDataset(Dataset):\r\n def __init__(self):\r\n super().__init__()\r\n\r\n def __getitem__(self, i):\r\n if i < 0 or i >= len(self): # These two lines\r\n raise IndexError() # are new\r\n return 0\r\n\r\n def __len__(self):\r\n return 10\r\n\r\n\r\ndataset = TestDataset()\r\nfor i, data in enumerate(dataset):\r\n print(i)\r\n assert i < len(dataset)\r\n```\r\n\r\nI believe this issue can be closed, as it's not pytorch specific. (Of course, @minghuisvn you're free to object, as I kind of highjacked your issue thread :wink:)"
Windows source build fails with 'error LNK2019' at linking stage,"I found what causing the problem, and wanted to inform you since it might be useful. `BUILD_TEST=0` variable is causing this problem at linking stage. If that not being set it builds successfully."
Complex-valued symmetric eigendecomposition,Upvote for the eigen decomposition for Hermitian inputs. I could work on the backward part once forward is done.
Too few arguments to vulkanOptimizeForMobile(),The issue was fixed in https://github.com/pytorch/pytorch/pull/45052
CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0,"Can you share a few more details how you've built gcc-5.4.0?\r\nBy default, GCC is coupled with a version of `libstdc++`, but there is a way to configure the build to use version already available in the system path. \r\nOtherwise, it sort of expected behaviour: shared library/executable compiled against newer version of libstdc++ is not compatible with an old one.\r\n\r\nPyTorch binary is build using `devtoolset-7`, which can be installed using something like the following \r\nhttps://github.com/pytorch/builder/blob/589a615fc8a8ee24690a1037ba583d32f22bc3a3/manywheel/Dockerfile#L12-L14\r\n\r\nFollowing article describes process of installing newer toolchain on CentOS in a bit more detail: https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/"
nn.Module.script,PyText is replacing `ScriptVocab` with TorchText's `Vocab`. However we have to add this `to_ivalue` to our transforms in order to scriptify them because of this issue. It would be great if users like us don't need to add this additional function to all layers to keep our interface clean. Thanks! CC @hudeven
Add RMSE loss function,"Actually, there is probably no advantage in having RMSE as a loss function. It's more computationally expensive than MSE and I don't see how it can be helpful to compensate for that. What I actually wanted is ready-made RMSE metric, not a loss function. Not sure if having a library of metrics is in scope for pytorch, so feel free to close this if the issue is not useful."
Is there a typo in ReLU6 declaration ?,nope if you look at the documentation of `relu6` and `hardtanh` it's reasonable to do subclass. Closing since it's resolved.
DISABLED test_backward_ddp_inside (__main__.TensorPipeDdpUnderDistAutogradTestWithSpawn),Isn't this a duplicate of #45116?
DISABLED test_backward_ddp_outside (__main__.TensorPipeDdpUnderDistAutogradTestWithSpawn),Isn't this a duplicate of #45117?
Ellipsis support for view and reshape functions,"I'm afraid this could be an ill-defined operation in general (specially if mixed with `-1` reshaping).\r\n\r\nFor the most common cases of flattening only a subset of dimensions, we have `tensor.flatten(start, end)`, which IMO is a good compromise in veneral."
Static Quantized model accuracy varies greatly with Calibration data,"Using numeric suite I was able to debug and fix the issue.
Appreciate all your help and time."
"torch.svd has a RuntimeError \""copy_\"" not implemented for 'ComplexDouble' when the size of input matrix is larger than 60","Actually, there are two separate issue: incorrect invocation of LAPACK's `zgesdd` for `std::complex<double>` and indeed a missing dispatch for special copy op for transposed tensors"
test_linalg.py not run in CI,"Thanks for reporting this issue, @antocuni. I think you're right. I'll fix it."
test_nn.py returns inconsistent result in different test setup,"Here's a reliable reproduction\r\n```\r\nimport torch\r\n\r\ninput_channels = 3\r\noutput_channels = 3\r\nbatch_size = 2\r\ndepth=3\r\nheight = 5\r\nwidth = 5\r\nkernel = 1\r\nstride = 1\r\nwith torch.backends.mkldnn.flags(enabled=False):\r\n conv_op = torch.nn.Conv3d(\r\n input_channels,\r\n output_channels,\r\n kernel,\r\n bias=False, # No bias\r\n ).to(dtype=torch.double)\r\n input = torch.randn(batch_size, input_channels, depth, height, width, dtype=torch.double, requires_grad=True)\r\n out = conv_op(input)\r\n gO = torch.rand_like(out)\r\n out.backward(gO)\r\n print(conv_op.weight.grad)\r\n```\r\nThe issue is on cpu, not on cuda."
ONNX export of nn.interpolate scripted module is broken due to removal of aten::__interpolate op,This PR addressed the issue: #35744
Nightly builds for cp38 missing for non-windows targets,"Looks like we neglected to add them to `master`, submitted #34732 to remedy that"
Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor,"This is expected, as it's a float32 limitation.\r\nFrom the [Wikipedia article](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\r\n> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between âˆ’16777216 and 0)\r\nIntegers between `2**24=16777216` and `2**25=33554432` round to a multiple of 2 (even number)\r\nIntegers between `2**25` and `2**26` round to a multiple of 4\r\n..."
torch.stft does not accept complex input,We will gladly accept PR implementing Complex support.
Building wheel for torch (setup.py) ... error - While running pip install torch,"You could not do `pip install torch` because the Windows packages are not hosted on PyPI. Instead, please enter the commands in https://pytorch.org."
AT_DISPATCH_FLOATING_TYPES_AND2 fails with ScalarType::Byte,PyTorch devs: we might want to ensure that the AT_DISPATCH macros work with all the built-in scalar types
load_state_dict_from_url error with weights downloaded from Google Drive ?,"That URL is not a PyTorch file (.pth). It's a webpage with a link to a PyTorch file.\r\n\r\nYou need a direct download URL. Google drive doesn't support that directly, but you can use something like https://sites.google.com/site/gdocs2direct/"
Support arbitrary types in jit,"We have support for classes in TorchScript, is there something in particular that's missing that you're looking for? NamedTuples are also supported. We also have other things like enum.Enum and @dataclass on our roadmap but no concrete timeline yet. We're also introducing a way to bind C++ classes into TorchScript in our next release."
[C++ API] Support for CIFAR10 and CIFAR100 Datasets,"Not yet, will try to add one by this weekend as an example"
max / min doesn't work on tensors with 0 elements,"Agreed that this should work. gh-38238 does look like it's going in the right direction but seems to have stalled. That PR followed up on gh-35062, which also stalled.\r\n\r\nSome shape checks are still missing in that PR, e.g. the PyTorch equivalents of:\r\n```\r\nIn [10]: x = np.ones((0, 3, 4)) \r\n\r\nIn [11]: np.max(x, axis=1) \r\nOut[11]: array([], shape=(0, 4), dtype=float64)\r\n\r\nIn [12]: np.max(x, axis=2) \r\nOut[12]: array([], shape=(0, 3), dtype=float64)\r\n```\r\nto make sure the fix doesn't unconditionally return a shape `(0,)` sized tensor.\r\n\r\n@willwray this seems like a good issue to work on and get it over the finish line."
Cannot use setup.py install,> ModuleNotFoundError: No module named 'past'\r\n\r\nModule `past` is part of `python3-future` package on Ubuntu (or just `pip3 install future`)
[Tensorboard] `RuntimeWarning: invalid value encountered in multiply` when calling `add_embedding` with `label_img`.,Thanks for tracking this down. A PR fixing this would be very welcome.
Gradient update of a sparse matrix results in a memory leak,We should investigate into this and provide a fix.
[Master Plan] Merge c10::ivalue::Future and torch::utils::Future<T>,"We have completely deleted torch/csrc/utils/future.h and are using c10::ivalue::Future across the RPC codebase now, so closing this out."
[v1.5.0] Release Tracker,"https://github.com/pytorch/pytorch/pull/35340 may be a candidate. Autocasting completes Pytorch's native automatic mixed precision support, which I've been writing for over 6 months now targeting 1.5. It has thorough documentation and test coverage. It was approved and merged already, but reverted for minor fixes.\r\n\r\nUpdate: PR against master https://github.com/pytorch/pytorch/pull/35102 has been re-merged and stuck the landing so far. https://github.com/pytorch/pytorch/pull/35340 cherry-picks those diffs onto 1.5 (+3 lines of cosmetic docstring changes).\r\n\r\n----------\r\n\r\nFeature, sorry :(. It's in nightlies though."
`torch.cat` should do type promotion,Can we please do simple fix to Raise error (as we need it in 1.5) and discuss type promotion after.
torch.cat is moving Tensors across devices silently,"One case (hopefully supported in future) where moving between devices makes sense:
`x = torch.cat([torch.rand(3, device = 'cuda'), 1])` for appending python scalars to a tensor"
Compilation error on aarch64,"I ran into the same issue on aarch64. I am able to work around it by downloading sleef 3.5.1, install it to system and set the USE_SYSTEM_SLEEF=ON flag.

Hope it helps"
Migrate `_cat` from the TH to Aten (CUDA),"While in general I favor reusing TensorIterator whenever possible, cuda implementation of cat seems like a very tricky case for it. The approach taken by cpu porting of cat to use TensorIterator cannot be directly reused in cuda (it would result in launching as many kernels as there are tensors, and that kills performance), and existing cat kernels are clever and flexible, reasonably efficiently handling the tensors of wildly differing sizes. So my recommendation here would be to reuse THC kernels. For benchmarks, I'd suggest 2 main usecases: catting large number (100-1000) of 1d tensors of very different sizes, this is typically encountered when flattening parameters of the model, and catting relatively small number (5-50) of similarly sized tensors across different dimensions."
Migrate `addmm` and `addmm_` from the TH to Aten (CUDA),"@Baranowski Yes, it's likely that the cluster of the matrix multiply / BLAS operations are most conveniently done all in one go. Check on prior work first."
[feature request] Allow `torch.unsqueeze` to insert multiple new dims,"@tshadley \r\n\r\n> ```\r\n> import torch\r\n> t_b = t[...,(None,)*3]\r\n> print(t_b.shape)\r\n> ```\r\n\r\nTo unsqueeze at the end, you could try to use `t_b = t[(..., ) + (None, ) * 3]` (works for me on version 1.1.0)."
[feature request] add `torch.find` to find the indices of values,"A not very optimized version of this function can be obtained with a one-liner I believe (for 1d `values`)\r\n```python\r\ndef find(tensor, values):\r\n return torch.nonzero(tensor[..., None] == values)\r\n```"
Segmentation Fault using dist.broadcast() with openmpi,"Actually I tested a simple cuda program doing MPI_Bcast/MPI_allreduce and confirmed that it also segfaults there with openmpi 1.10, while it works fine on openmpi 2.1+. I'm more inclined to a openmpi issue in this case. I will make a PR to add this warning to the doc I think."
.topk() returns incorrect values + indeces on non-contiguous tensors (CUDA),The reason this fails is `topk` needs contiguous inputs and the outputs of Beta.sample aren't.\r\nCan you retitle the bug report to that?\r\nI'll submit a PR to check contiguous in topk.
OOM when using Adam optimizer compared to SGD when using same batch size.,"Adam is more stateful than SGD, so it is expected that it uses more memory (proportional to the total size of the optimized parameters)."
PyTorch ImportError,"The reason I've ""access is denied"" is due to McAfee VirusScan Enterprise applying access protection policy. I do not know which operation of \torch_C.cp36-win_amd64.pyd is blocked. From the process monitor, I could see \torch_C.cp36-win_amd64.pyd is loaded and some operations have succeeded."
log_prob returns positive values for small cov,"The values of the `pdf` can be arbitrarily large but never negative. This is completely acceptable, and I am sure this is not a bug.\r\n\r\nSimple example: normal distribution with standard deviation = 0.000001 and mean = 0. The `pdf` at `x = 0` is `1000 / (2 * pi)`, and the natural log of this is clearly greater than 0."
"\""ImportError: No module named tools.setup_helpers.env\"" when \""python setup.py egg_info\""","Does it work if you do ""FULL_CAFFE2=1 python setup.py install"" instead?"
[bug] Multiplication of tensor with numpy scalar does not always work,"The problem is that the left operand is the default operand to execute the `__mul__`.\r\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\r\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.\r\nStandard numpy scalars have a strongly negative priority, standard ndarrays have one of 0.\r\nSo in case 2:\r\n```\r\ntensor.__array_priority__ = 1000\r\nprint (scalar * tensor)\r\n```\r\nworks!\r\n\r\nI'm happy to send a PR.\r\n"
git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.,"Sorry, I did not realize you were trying to install detectron. It looks like detectron expects to find Caffe2 through find_package, which right now requires Caffe2 to be built from source. What's causing that error message is that a source build (through Caffe2's cmake) populates a ""Caffe2 target"" that tells other cmake projects (Detectron) where to find Caffe2. These extra cmake files aren't included in the pre-built packages because you shouldn't need cmake to install and use Caffe2.

When installing from source, do still read through https://caffe2.ai/docs/faq.html#why-do-i-get-import-errors-in-python-when-i-try-to-use-caffe2 and follow it's recommendations; it can save you from a lot of frustrating errors."
[docs] document torch.nn.functional.bilinear,"Oh, bilinear, I missed that last time I looked at it. I'll add a docstring."
[jit] support at::optional,cc @wanchaol \r\n\r\n@SsnL the nan is a temporary stopgap. I believe the end state is indeed to support `at::optional` or something similar.
Proposal: type promotion logic (torch.result_type),???
[distributions] dirichlet pathwise gradient does not work well with .expand,I think this should be closed now.
MarginRankingLoss with multiple examples per batch is broken,"Ok after reading the documentation more carefully I realized, that the target tensor has to be of the same shape as the inputs."
[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`,"After a lot of discussion, we decided not to move on with this proposal."
"RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True","maybe \""sudo python\"" can solve it"
CUDA 8 build failed on Windows,@cbecker Fixed in both master and v0.4.1. You can have a try.
Add python 3.7 to binary install page,"python 3.7 binaries are live on PyPI, conda and on https://pytorch.org"
Caffe2 Train your own image,"@aswin1980 Check out the \""CIFAR10_Part1\"" and \""CIFAR10_Part2\"" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. Part 1 specifically shows how to take a custom image dataset (in this case .png mirror of the CIFAR-10 dataset), format it, create image LMDBs, and train a model on them."
Conda Install: PackageNotFoundError,@pjh5 \r\n\r\nUpdating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda` fixed it! I'm able to properly install pytorch now by running `conda install pytorch torchvision -c pytorch`. \r\n\r\nThanks for the help.
"Always get error \""ConnectionResetError: [Errno 104] Connection reset by peer\""","for me, I found set thread number equal 0 will solve this problem, namely:\r\nnum_workers=0\r\n"
[Bug] Segmentation fault when importing fastText (with v0.4.0),"For the record, the problem was:\r\n - in a conda environment, I installed pytorch with `conda install`(as described on pytorch web site) and fastText with `pip install .` from their git clone.\r\n - that resulted in a segfault when doing `import fastText` and `import torch` \r\n\r\nReason:\r\n - pytorch is compiled with gcc 4.9.2\r\n - conda's default gcc is 4.8.5\r\n\r\nFix:\r\n - install gcc-4.9 in conda (e.g. `conda install -c serge-sans-paille gcc_49`)\r\n - install pytorch with `conda install` (in my case, `conda install pytorch torchvision cuda90 -c pytorch`)\r\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone\r\n\r\nThat's it! Thanks a lot @weiyangfb and @SsnL for your help!"
How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source,@Tianji95 this problem is outdated. The FULL_CAFFE2 flag no longer exists and is no longer needed.
"[feature request] torch.isinf, torch.isfinite","`torch.isinf` merged in, `torch.isfinite` not implemented yet"
cannot reload on CPU model saved on GPU,"When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`. If you don't, since you are loading GPU tensors, PyTorch will try to reconstruct GPU tensors, and fail."
LinearLayer precision problem,???
setting CUDA_VISIBLE_DEVICES just has no effect,You need to do that before import pytorch.
[feature request] nn.Identity,You can use nn.Sequential() to simulate identity
DataParallel model stucks with CUDA_LAUNCH_BLOCKING=1 sometime,@acgtyrant we asked some NVIDIA engineers. Apparently NCCL doesn't really like CUDA_LAUNCH_BLOCKING
Big drop in performance for larger batch size for otherwise same training script,"You should tune different hyparams (e.g., lr) to accommodate different batch size. Some say that larger batch size requires larger lr too."
torch.save() and nn.DataParallel(),I usually do:\r\n```\r\ntry:\r\n state_dict = model.module.state_dict()\r\nexcept AttributeError:\r\n state_dict = model.state_dict()\r\n```
torch.Tensor.__repr__ is slow,This has been fixed in master. Here are my timings: ...
"[feature request] Convert \""indices\"" variable in \""torch.utils.data.dataset.random_split\"" to list",I think we can just add a .tolist() after the code in here. Could you send a PR?
problem building with ROCm,"There's a \""hipify\"" step to replace all the CUDA references with HIP/ROCm references in-place: \""python tools/amd_build/build_pytorch_amd.py\"". Also, don't forget to set env var USE_ROCM to 1. After that, you should be able to build using the normal \""python setup.py\"" step."
RNN weights are not Xavier-initialized,"Also, adding to @Kaixhin 's point, if you really want Xavier initialization, you could try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`."
"Sequential does not allow muti-output modules such as RNN, LSTM","As I said in forum, this is not a bug."
Error fluctuations with well-mixed data,"ohhhh now i get it. yea your code sample's error reporting is the reason, thanks for letting me know :)"
Weights won't update during backpropogation,"your learning rate simply has to be increased, and as Issam pointed out SGD has quite a bit of variance in the gradients."
"error: identifier \""__half_as_ushort\"" is undefined","It tends out to be my own installation problem. cuda_fp16.h is not inside /usr/local/cuda/include but I've found it in /usr/local/cuda/targets/x86_64-linux/include. Maybe because I used to have an older version of cuda installed.\r\n\r\nAnyway, I completely removed cuda and reinstalled it. All the header files are now in place and built with no problem."
"When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.","I meet the same error on pytorch 1.0.0.dev20181115 with inner ctc loss, but did not encounter this situation at pytorch 0.4 with warpctc loss"
Memory leak from Function.save_for_backward() when looping over batch,the `@staticmethod` stuff is the right / official way. We'll change the tutorials right away.
Handle infinity in Variables without error,"It seems this is broken again on tags/v0.3.0\r\n``` python\r\ntorch.autograd.Variable(torch.Tensor([float('inf')])).sum()\r\n# Traceback (most recent call last):\r\n# File \""<stdin>\"", line 1, in <module>\r\n# RuntimeError: value cannot be converted to type double without overflow: inf\r\nprint(torch.__version__)\r\n# 0.3.0b0+af3964a\r\n```"
Numerical instability of core algorithms,Welford one-pass computation is pretty stable https://docs.google.com/document/d/1V3VEm9zQ17jsbWT9dxK_iS5qRZZenU6tdApj-VsbGk0/edit
"Segfault in neg, introduced in 3e6e81d",Fixed in #3433
`torch.utils.data.DataLoader`,this is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe: \r\nhttps://github.com/pandas-dev/pandas/issues/12236\r\nhttps://github.com/pandas-dev/pandas/issues/14692\r\n\r\nTo actually allow concurrent reads for a file you have to use SWMR feature of HDF5: https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html\r\n\r\n\r\n
inconsistent behavior of max,this was fixed in master. will be part of the next release.
Kaiming/Xavier initializer cannot deal with bias term,"Correct me if I am wrong. I think those methods are not supposed to deal with bias terms. In the paper, bias terms are initialized to zeros, which you can achieve by `layer.bias.data.zero_()`"
Implementation Discussion: Native CTC,"You would have to use 1.0rc, I wasn't quite finished in time for 0.4.1."
Can I plz has determinant function?,fixed on master via #3816
Recent bug in torch.cat() on Variables?,"Great, this is also fixed on the `v0.3.0` branch as of 9a67882"
cuda out of memory error when GPU0 memory is fully utilized,@TomHeaven did you set CUDA_VISIBLE_DEVICES outside the python process? if that's the case pytorch should not even have driver-level access to your GPU0.\nIdeally: `CUDA_VISIBLE_DEVICES=1 python foo.py`
RuntimeError: CUDA error (3): initialization error,"as mentioned in http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\r\n\r\ninsert this to the top of your script\r\n\r\n```\r\nimport torch\r\ntorch.multiprocessing.set_start_method(\""spawn\"")\r\n```"
RuntimeError: context has already been set(multiprocessing),"Hi @pancho111203 ,\r\n\r\nYou might have other files in your project which also have a `if __name__ == '__main__':`. One workaround is to call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`. This solved the issue for me.\r\n\r\nRegards\r\nNabarun"
Gradient Ascent Cross Entropy Loss,"@cdjhz `(-loss).backward(); optimizer.step()`
"
ParameterList and ModuleList with named modules or parameters,fixed via #3505
PyTorch Implementation of Michael Jordanâ€™s lab's Perturbed SGD?,"I think adding noise to gradients is simple. A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.\r\n\r\nI initially proposed adding Noisy SGD, but the proposal was rejected considering its triviality. Ref: https://github.com/pytorch/pytorch/pull/4332"
NameError: name 'logging' is not defined,"I think this has been fixed on master, because the point of error is non-existent on master."
Builing for a specific SM number,"Yes. By example:\r\n```\r\nTORCH_CUDA_ARCH_LIST=\""5.2;6.1;7.0\"" python setup.py install\r\n```"
max_pool2d_with_indices: internal error,???
[jit] torch.empty_like is different from eager mode,"I think I can fix this - it's probably not an issue with `empty_like` in particular, but with all optional arguments. https://github.com/pytorch/pytorch/pull/22055 fixes one case of this."
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0,"Try to use this command from the [website](https://pytorch.org/get-started/locally/):\r\n```\r\npip3 install torch torchvision\r\n```\r\n\r\nThis should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\r\nJust select whatever config matches your current setup and use the shown install command."
CI failure points to nonexistent code...,fixed via #23304 by @zou3519 :D
torch.onnx._export does not support tensor sum with multiple dims,"@yil8 - yes, you can install the nightly build of PyTorch to test this."
issue with ONNX and PyTorch,"@arijit17 - It is hard to say where exactly the issue is without looking at your model and export code. Could you please share the repro? \r\nFYI - if your model has input-dependent control flow, then ideally you will have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation. But once you do that the export process itself should not change.\r\nIf you can share a repro, we might be able to help better."
Wrong distribution sampled by torch.multinomial on CUDA,Closed by #22183
Inplace error if DistributedDataParallel module that contains a buffer is called twice,"The cause here is that by default DDP modules broadcast the contents of the root process module's buffers to all processes at every forward pass, and this broadcasting counts as an inplace operation for the purposes versioning. The fix is to disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor. Thanks @pietern for the help."
LR scheduler design bug !,It's in master: you need to compile from source for now.
Segmentation fault Autograd,"@shoukang by any chance you can try installing new Pytorch, we have done a lot in terms of threading recently. And your problem might be related."
Multiple (redundant?) return statements in test_jit.py,"On a side note I think we're not generating in-place kernels at the moment, but it wouldn't be too hard to adjust our codegen to take care of that. We might be able to automatically change them to out-of-place ops and fuse them at that point even now, but idk if we have merged this feature in the end (I worked on it some time ago)."
Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?,"Actually, in the current code, sigmoid is also deprecated in nn https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L1390. But not relu. Are you now saying that sigmoid is a general purpose mathematical function but relu isn't ?"
Building from source failed. Multiple errors in the printout,"You can find good answer here: https://github.com/torch/torch7/issues/1190#issuecomment-498934400 (credits to @talkenig)\r\n\r\nCopy pasting for future reference:\r\n\r\nHad the same problem and got to the bottom of it. My configuration is a Tesla T4 with 410.92 driver and CUDA 9.2, Ubuntu 18.10.\r\nThe thing is, the torch installer tries for some reason to use the highest compute capability supported by the device (or the driver - not sure which), but ignores the compute capability supported by the CUDA toolkit.\r\nSo, in my case the device supports compute capability 7.5, but CUDA 9.2 supports only 7.0 or 7.2 (not sure which one). You can guess I got the same nvcc fatal : Unsupported gpu architecture 'compute_75'\r\nThe solution is to force the nvcc compile options to use a lower compute capability. This can be achieved by setting the following environment variable:\r\n\r\n`export TORCH_CUDA_ARCH_LIST=\""7.0\""`\r\n\r\nJust before running ./install.sh from the torch directory.\r\nNote that the list may contain more than one compute capability, e.g. it can be \""6.0 6.2 7.0 7.2\"". This will be reflected in the CUDA_NVCC_FLAGS -gencode arch=compute_70,code=sm_70 and those will be outputted to the terminal during the build.\r\n"
[dataloader] Add a context= argument for multiprocessing,"This is expected, because thed spawned workers does not see the dataset def.\n I think the proper way to solve this is to add a `context=` argument to data loader, so that a global start method needs not be set."
Error when converting custom layer with output of tuple to onnx format in pytorch 1.1.0,Thanks for trying with 1.5.1 @kuramawzw3. I see you opened #41440 which is a duplicate of this item. It is better to track this in one place. Can we close #41440?
Sync Batchnorm running var update issue,"Yeah, I have submitted a [PR https://github.com/pytorch/pytorch/pull/22248](https://github.com/pytorch/pytorch/pull/22248), and a fail test looks like some internet issue."
"Torch crashes when calling torch.rand(2,3)",your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. We do not support processors that dont have these features in binaries.\r\nYour only choice is to install from source: https://github.com/pytorch/pytorch#from-source
No method to set the timeout for distributed Gloo backend,"@ejoebstl this would helps a lot, i used to solve this problem by changing the default timeout and recompile the whole pytorch o(â•¥ï¹â•¥)o"
[PyTorch] Build error (NCCL) on Ubuntu 16.04,"A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/nccl.py#L74\r\n\r\nAfter modifying this, I can compile PyTorch without error. \r\n"
Error when exiting tutorial script,I am aware of this and will have a fix.
"Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue","""It looks like you installed with setup_caffe2.py, as setuptools (the Python package behind setup.py scripts) is what installs .egg files.\r\n\r\nCould you try `pip uninstall caffe2` and `conda install -c caffe2 caffe2-cuda9.0-cudnn7` ? This way should be much faster too.\r\n\r\nIf the pip uninstall caffe2 doesn't seem to work, then you can manually uninstall by deleting every file and folder under /home/sam/anaconda3/envs/caffe36/ that has 'caffe' or 'caffe2' in the name."
"BatchNorm2d when batch size 1 works, what is it doing?",That is normalizing `[B x C x *]` over the dimensions `[*]`
[PyTorch] Dense embedding doesn't work with double backward,"The version on my branch solves the issue. You can follow the PR here. Also if you need the functionality now, it would be great if you could try the code on my branch and let me know if you face any issues.
Thank You."
Advanced indexing doesn't validate negative indexes (regression),Cuda version works through. I'll add a check to TH.
can't rebuild with NO_CUDA=1 after clean,Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\r\n`\r\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\r\n`
Where is the Caffe2 website?,https://github.com/caffe2/caffe2.github.io
Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed,"I had the same problem and renamed `THGeneral.h` from `conda/envs/<ENV_NAME>/` to something similar to what @rgreenblatt  said, and it was installed normally. Good luck to friends who are having the same problem.\r\n\r\n```\r\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\r\n```"
Add `torch.pi` like `numpy.pi` and possibly other constants,Aren't all of those constants in math?
"torch.irfft produces \""cuFFT error: CUFFT_ALLOC_FAILED\"" when called after torch.rfft","Me either, it seems to work now. Maybe you fixed something along the way :) Maybe this can be closed, having fewer temp tensors along the way also helps. Might be even better if complex product gets implemented some day.\nIt would be nice if the OOM exception was some standard PyTorch exception, not a CUDA one."
[feature request] Complex multiplication,One verbose way may be a kwarg `complex = True` to `torch.mul`. Some alternative ideas: `mul_complex` / `mulc`
cuda runtime error (48): no kernel image is available for execution on the device,"Hi, your problem is stated in this warning here\r\n```\r\n/home/azat/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py:97: UserWarning:\r\n Found GPU0 GeForce 820M which is of cuda capability 2.1.\r\n PyTorch no longer supports this GPU because it is too old.\r\n```\r\n\r\nYou can build from source to use some functionality, but there are many operations that require compute capability greater than SM_21 to perform."
Stop using undefined tensors to represent zero gradients in engine,going to close this because I believe the issues with undefined tensors in the engine have been addressed.
Proposal: rename upsample to resample,This function has been deprecated in favor of interpolate so I think this issue has been addressed.
RuntimeError for indexing with high dimensional tensor only when using cuda,"As for pytorch 0.4 (nightly build 2018.04.20, installed via conda) this problem does not exist anymore. And the following code works:\r\n```python\r\nimport torch\r\nx = torch.randn(2, 2, 2, 2, 2, 2).cuda()\r\ni = torch.cuda.LongTensor([0, 0, 0, 0, 0, 0])\r\nx[i,:]\r\n```\r\nClosing the issue."
RuntimeError: reduce failed to synchronize: unspecified launch failure,"I was having similar error. Make sure your layer has values that make sense to the BCELoss. If you for example output negative values and pass them to the logarithm, the training will fail.\r\n\r\nIn my case I was missing the last sigmoid activation to shrink the numbers between 0 and 1."
Zombie process when use GPU,"For anyone who still suffer from this issue, try the following command:\r\n`fuser -k /dev/nvidia*`\r\nor\r\n`kill $(lsof -t /dev/nvidia*)`"
torch.HalfTensor' object has no attribute 'mean',"we don't have math for CPU Half type (it would be very slow), convert it to cuda or CPU Float."
import torch; libcublas.so.9.0 error,"I had the exact same linking problem when trying to compile pytorch 0.3 with CUDA 9.1. I couldn't figure out how it manages to find and link to cublas.8. I gave up and installed the `pip` wheel package which contains CUDA, CuDNN everything inside."